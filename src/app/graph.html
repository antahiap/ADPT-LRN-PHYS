<html>
    <head>
        <meta charset="utf-8">
        
            <script src="lib/bindings/utils.js"></script>
            <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/vis-network/9.1.2/dist/dist/vis-network.min.css" integrity="sha512-WgxfT5LWjfszlPHXRmBWHkV2eceiWTOBvrKCNbdgDYTHrT2AeLCGbF4sZlZw3UMN3WtL0tGUoIAKsu8mllg/XA==" crossorigin="anonymous" referrerpolicy="no-referrer" />
            <script src="https://cdnjs.cloudflare.com/ajax/libs/vis-network/9.1.2/dist/vis-network.min.js" integrity="sha512-LnvoEWDFrqGHlHmDD2101OrLcbsfkrzoSpvtSQtxK3RMnRV0eOkhhBN2dXHKRrUU8p2DGRTk35n4O8nWSVe1mQ==" crossorigin="anonymous" referrerpolicy="no-referrer"></script>
            
        
<center>
<h1></h1>
</center>

<!-- <link rel="stylesheet" href="../node_modules/vis/dist/vis.min.css" type="text/css" />
<script type="text/javascript" src="../node_modules/vis/dist/vis.js"> </script>-->
        <link
          href="https://cdn.jsdelivr.net/npm/bootstrap@5.0.0-beta3/dist/css/bootstrap.min.css"
          rel="stylesheet"
          integrity="sha384-eOJMYsd53ii+scO/bJGFsiCZc+5NDVN2yr8+0RDqr0Ql0h+rP48ckxlpbzKgwra6"
          crossorigin="anonymous"
        />
        <script
          src="https://cdn.jsdelivr.net/npm/bootstrap@5.0.0-beta3/dist/js/bootstrap.bundle.min.js"
          integrity="sha384-JEW9xMcG8R+pH31jmWH6WWP0WintQrMb4s7ZOdauHnUtxwoG2vI5DkLtS3qm9Ekf"
          crossorigin="anonymous"
        ></script>


        <center>
          <h1></h1>
        </center>
        <style type="text/css">

             #mynetwork {
                 width: 500px;
                 height: 700px;
                 background-color: #ffffff;
                 border: 1px solid lightgray;
                 position: relative;
                 float: left;
             }

             

             

             
        </style>
    </head>


    <body>
        <div class="card" style="width: 100%">
            
            
            <div id="mynetwork" class="card-body"></div>
        </div>

        
        

        <script type="text/javascript">

              // initialize global variables.
              var edges;
              var nodes;
              var allNodes;
              var allEdges;
              var nodeColors;
              var originalNodes;
              var network;
              var container;
              var options, data;
              var filter = {
                  item : '',
                  property : '',
                  value : []
              };

              

              

              // This method is responsible for drawing the graph, returns the drawn network
              function drawGraph() {
                  var container = document.getElementById('mynetwork');

                  

                  // parsing and collecting nodes and edges from the python
                  nodes = new vis.DataSet([{"color": "#FF0000", "font": "25px arial black", "id": 0, "ids": "1", "label": "1", "shape": "dot", "size": 10, "text": "Recurrent neural networks, long short-term memory [ 13] and gated recurrent [ 7] neural networks\nin particular, have been firmly established as state of the art approaches in sequence modeling and\ntransduction problems such as language modeling and machine translation [ 35,2,5]. Numerous\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\narchitectures [38, 24, 15].\nRecurrent models typically factor computation along the symbol positions of the input and output\nsequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\nstates ht, as a function of the previous hidden state ht\u22121and the input for position t. This inherently\nsequential nature precludes parallelization within training examples, which becomes critical at longer\nsequence lengths, as memory constraints limit batching across examples. Recent work has achieved\nsignificant improvements in computational efficiency through factorization tricks [ 21] and conditional\ncomputation [ 32], while also improving model performance in case of the latter. The fundamental\nconstraint of sequential computation, however, remains.\nAttention mechanisms have become an integral part of compelling sequence modeling and transduc-\ntion models in various tasks, allowing modeling of dependencies without regard to their distance in\nthe input or output sequences [ 2,19]. In all but a few cases [ 27], however, such attention mechanisms\nare used in conjunction with a recurrent network.\nIn this work we propose the Transformer, a model architecture eschewing recurrence and instead\nrelying entirely on an attention mechanism to draw global dependencies between input and output.\nThe Transformer allows for significantly more parallelization and can reach a new state of the art in\ntranslation quality after being trained for as little as twelve hours on eight P100 GPUs.", "title": "Introduction"}, {"color": "#FF0000", "font": "25px arial black", "id": 1, "ids": "2", "label": "2", "shape": "dot", "size": 10, "text": "The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\n[16], ByteNet [ 18] and ConvS2S [ 9], all of which use convolutional neural networks as basic building\nblock, computing hidden representations in parallel for all input and output positions. In these models,\nthe number of operations required to relate signals from two arbitrary input or output positions grows\nin the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\nit more difficult to learn dependencies between distant positions [ 12]. In the Transformer this is\nreduced to a constant number of operations, albeit at the cost of reduced effective resolution due\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\ndescribed in section 3.2.\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions\nof a single sequence in order to compute a representation of the sequence. Self-attention has been\nused successfully in a variety of tasks including reading comprehension, abstractive summarization,\ntextual entailment and learning task-independent sentence representations [4, 27, 28, 22].\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\naligned recurrence and have been shown to perform well on simple-language question answering and\nlanguage modeling tasks [34].\nTo the best of our knowledge, however, the Transformer is the first transduction model relying\nentirely on self-attention to compute representations of its input and output without using sequence-\naligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\nself-attention and discuss its advantages over models such as [17, 18] and [9].", "title": "Background"}, {"color": "#FF0000", "font": "25px arial black", "id": 2, "ids": "3", "label": "3", "shape": "dot", "size": 10, "text": "Most competitive neural sequence transduction models have an encoder-decoder structure [ 5,2,35].\nHere, the encoder maps an input sequence of symbol representations (x1, ..., x n)to a sequence\nof continuous representations z= (z1, ..., z n). Given z, the decoder then generates an output\nsequence (y1, ..., y m)of symbols one element at a time. At each step the model is auto-regressive\n[10], consuming the previously generated symbols as additional input when generating the next.\nFigure 1: The Transformer - model architecture.\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\nrespectively.", "title": "Model Architecture"}, {"color": "#FF0000", "font": "25px arial black", "id": 20, "ids": "7", "label": "7", "shape": "dot", "size": 10, "text": "In this work, we presented the Transformer, the first sequence transduction model based entirely on\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\nmulti-headed self-attention.\nFor translation tasks, the Transformer can be trained significantly faster than architectures based\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\nmodel outperforms even all previously reported ensembles.\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We\nplan to extend the Transformer to problems involving input and output modalities other than text and\nto investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs\nsuch as images, audio and video. Making generation less sequential is another research goals of ours.\nThe code we used to train and evaluate our models is available at https://github.com/\ntensorflow/tensor2tensor .\nAcknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\ncomments, corrections and inspiration.\nReferences\n[1]Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\narXiv:1607.06450 , 2016.\n[2]Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\nlearning to align and translate. CoRR , abs/1409.0473, 2014.\n[3]Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V . Le. Massive exploration of neural\nmachine translation architectures. CoRR , abs/1703.03906, 2017.\n[4]Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\nreading. arXiv preprint arXiv:1601.06733 , 2016.\n[5]Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\nmachine translation. CoRR , abs/1406.1078, 2014.\n[6]Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\npreprint arXiv:1610.02357 , 2016.\n[7]Junyoung Chung, \u00c7aglar G\u00fcl\u00e7ehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\nof gated recurrent neural networks on sequence modeling. CoRR , abs/1412.3555, 2014.\n[8]Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, and Noah A. Smith. Recurrent neural\nnetwork grammars. In Proc. of NAACL , 2016.\n[9]Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2 , 2017.\n[10] Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint\narXiv:1308.0850 , 2013.\n[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\nRecognition , pages 770\u2013778, 2016.\n[12] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and J\u00fcrgen Schmidhuber. Gradient flow in\nrecurrent nets: the difficulty of learning long-term dependencies, 2001.\n[13] Sepp Hochreiter and J\u00fcrgen Schmidhuber. Long short-term memory. Neural computation ,\n9(8):1735\u20131780, 1997.\n[14] Zhongqiang Huang and Mary Harper. Self-training PCFG grammars with latent annotations\nacross languages. In Proceedings of the 2009 Conference on Empirical Methods in Natural\nLanguage Processing , pages 832\u2013841. ACL, August 2009.\n[15] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\nthe limits of language modeling. arXiv preprint arXiv:1602.02410 , 2016.\n[16] \u0141ukasz Kaiser and Samy Bengio. Can active memory replace attention? In Advances in Neural\nInformation Processing Systems, (NIPS) , 2016.\n[17] \u0141ukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\non Learning Representations (ICLR) , 2016.\n[18] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\nray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2 ,\n2017.\n[19] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\nInInternational Conference on Learning Representations , 2017.\n[20] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR , 2015.\n[21] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\narXiv:1703.10722 , 2017.\n[22] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\narXiv:1703.03130 , 2017.\n[23] Minh-Thang Luong, Quoc V . Le, Ilya Sutskever, Oriol Vinyals, and Lukasz Kaiser. Multi-task\nsequence to sequence learning. arXiv preprint arXiv:1511.06114 , 2015.\n[24] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\nbased neural machine translation. arXiv preprint arXiv:1508.04025 , 2015.\n[25] Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated\ncorpus of english: The penn treebank. Computational linguistics , 19(2):313\u2013330, 1993.\n[26] David McClosky, Eugene Charniak, and Mark Johnson. Effective self-training for parsing. In\nProceedings of the Human Language Technology Conference of the NAACL, Main Conference ,\npages 152\u2013159. ACL, June 2006.\n[27] Ankur Parikh, Oscar T\u00e4ckstr\u00f6m, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\nmodel. In Empirical Methods in Natural Language Processing , 2016.\n[28] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\nsummarization. arXiv preprint arXiv:1705.04304 , 2017.\n[29] Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. Learning accurate, compact,\nand interpretable tree annotation. In Proceedings of the 21st International Conference on\nComputational Linguistics and 44th Annual Meeting of the ACL , pages 433\u2013440. ACL, July\n2006.\n[30] Ofir Press and Lior Wolf. Using the output embedding to improve language models. arXiv\npreprint arXiv:1608.05859 , 2016.\n[31] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\nwith subword units. arXiv preprint arXiv:1508.07909 , 2015.\n[32] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\nlayer. arXiv preprint arXiv:1701.06538 , 2017.\n[33] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\nnov. Dropout: a simple way to prevent neural networks from overfitting. Journal of Machine\nLearning Research , 15(1):1929\u20131958, 2014.\n[34] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\nAdvances in Neural Information Processing Systems 28 , pages 2440\u20132448. Curran Associates,\nInc., 2015.\n[35] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\nnetworks. In Advances in Neural Information Processing Systems , pages 3104\u20133112, 2014.\n[36] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\nRethinking the inception architecture for computer vision. CoRR , abs/1512.00567, 2015.\n[37] Vinyals \u0026 Kaiser, Koo, Petrov, Sutskever, and Hinton. Grammar as a foreign language. In\nAdvances in Neural Information Processing Systems , 2015.\n[38] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google\u2019s neural machine\ntranslation system: Bridging the gap between human and machine translation. arXiv preprint\narXiv:1609.08144 , 2016.\n[39] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with\nfast-forward connections for neural machine translation. CoRR , abs/1606.04199, 2016.\n[40] Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang, and Jingbo Zhu. Fast and accurate\nshift-reduce constituent parsing. In Proceedings of the 51st Annual Meeting of the ACL (Volume\n1: Long Papers) , pages 434\u2013443. ACL, August 2013.\nAttention Visualizations\nin\nthis\nspirit\nthat\na\nmajority\nof\nAmerican\ngovernments\nhave\npassed\nnew\nlaws\nsince\n2009\nmaking\nthe\nregistration\nor\nvoting\nprocess\nmore\ndifficult\n.\n\u003cEOS\u003e\n\u003cpad\u003e\n\u003cpad\u003e\n\u003cpad\u003e\n\u003cpad\u003e\n\u003cpad\u003e\n\u003cpad\u003e\nin\nthis\nspirit\nthat\na\nof\nhave\nnew\nlaws\nsince\n2009\nthe\nor\nmore\ndifficult\n.\nInput-Input Layer5\nIt\nis\nin\nthis\nspirit\nthat\na\nmajority\nof\nAmerican\ngovernments\nhave\npassed\nnew\nlaws\nsince\n2009\nmaking\nthe\nregistration\nor\nvoting\nprocess\nmore\ndifficult\n.\n\u003cEOS\u003e\n\u003cpad\u003e\n\u003cpad\u003e\n\u003cpad\u003e\n\u003cpad\u003e\n\u003cpad\u003e\n\u003cpad\u003e\nIt\nis\nin\nthis\nspirit\nthat\na\nmajority\nof\nAmerican\ngovernments\nhave\npassed\nnew\nlaws\nsince\n2009\nmaking\nthe\nregistration\nor\nvoting\nprocess\nmore\ndifficult\n.\n\u003cEOS\u003e\n\u003cpad\u003e\n\u003cpad\u003e\n\u003cpad\u003e\n\u003cpad\u003e\n\u003cpad\u003e\n\u003cpad\u003e\nFigure 3: An example of the attention mechanism following long-distance dependencies in the\nencoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\nthe verb \u2018making\u2019, completing the phrase \u2018making...more difficult\u2019. Attentions here shown only for\nthe word \u2018making\u2019. Different colors represent different heads. Best viewed in color.\nwill\nnever\nbe\nperfect\n,\nbut\nits\napplication\nshould\nbe\njust\n-\nthis\nis\nwhat\nwe\nare\nmissing\n,\nin\nmy\nopinion\n.\n\u003cEOS\u003e\n\u003cpad\u003e\nwill\nnever\nbe\nperfect\n,\nbut\nits\napplication\nshould\nbe\njust\n-\nthis\nis\nwhat\nwe\nare\nmissing\n,\nin\nmy\nopinion\n.\n\u003cEOS\u003e\n\u003cpad\u003e\nwill\nbe\n,\nbut\nits\nbe\njust\n-\nthis\nis\nwhat\nwe\nare\n,\nin\nmy\n.\nFigure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top:\nFull attentions for head 5. Bottom: Isolated attentions from just the word \u2018its\u2019 for attention heads 5\nand 6. Note that the attentions are very sharp for this word.\nwill\nnever\nbe\nperfect\n,\nbut\nits\napplication\nshould\nbe\njust\n-\nthis\nis\nwhat\nwe\nare\nmissing\n,\nin\nmy\nopinion\n.\n\u003cEOS\u003e\n\u003cpad\u003e\nwill\nnever\nbe\nperfect\n,\nbut\nits\napplication\nshould\nbe\njust\n-\nthis\nis\nwhat\nwe\nare\nmissing\n,\nin\nmy\nopinion\n.\n\u003cEOS\u003e\n\u003cpad\u003e\nFigure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the\nsentence. We give two such examples above, from two different heads from the encoder self-attention\nat layer 5 of 6. The heads clearly learned to perform different tasks.", "title": "Conclusion"}, {"color": "#FF0000", "font": "25px arial black", "id": 21, "ids": "", "label": 21, "shape": "dot", "size": 10, "text": "The dominant sequence transduction models are based on complex recurrent or\nconvolutional neural networks that include an encoder and a decoder. The best\nperforming models also connect the encoder and decoder through an attention\nmechanism. We propose a new simple network architecture, the Transformer,\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\nentirely. Experiments on two machine translation tasks show these models to\nbe superior in quality while being more parallelizable and requiring significantly\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\nto-German translation task, improving over the existing best results, including\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\nour model establishes a new single-model state-of-the-art BLEU score of 41.8 after\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\nbest models from the literature. We show that the Transformer generalizes well to\nother tasks by applying it successfully to English constituency parsing both with\nlarge and limited training data.\n", "title": "Abstract"}, {"color": "#FF0000", "font": "25px arial black", "id": 7, "ids": "3.2.3", "label": "3.2.3", "shape": "dot", "size": 10, "text": "The Transformer uses multi-head attention in three different ways:\n\u2022In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\nand the memory keys and values come from the output of the encoder. This allows every\nposition in the decoder to attend over all positions in the input sequence. This mimics the\ntypical encoder-decoder attention mechanisms in sequence-to-sequence models such as\n[38, 2, 9].\n\u2022The encoder contains self-attention layers. In a self-attention layer all of the keys, values\nand queries come from the same place, in this case, the output of the previous layer in the\nencoder. Each position in the encoder can attend to all positions in the previous layer of the\nencoder.\n\u2022Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\nall positions in the decoder up to and including that position. We need to prevent leftward\ninformation flow in the decoder to preserve the auto-regressive property. We implement this\ninside of scaled dot-product attention by masking out (setting to \u2212\u221e) all values in the input\nof the softmax which correspond to illegal connections. See Figure 2.", "title": "Applications of Attention in our Model"}, {"color": "#FF0000", "font": "25px arial black", "id": 11, "ids": "4", "label": "4", "shape": "dot", "size": 10, "text": "In this section we compare various aspects of self-attention layers to the recurrent and convolu-\ntional layers commonly used for mapping one variable-length sequence of symbol representations\n(x1, ..., x n)to another sequence of equal length (z1, ..., z n), with xi, zi\u2208Rd, such as a hidden\nlayer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we\nconsider three desiderata.\nOne is the total computational complexity per layer. Another is the amount of computation that can\nbe parallelized, as measured by the minimum number of sequential operations required.\nThe third is the path length between long-range dependencies in the network. Learning long-range\ndependencies is a key challenge in many sequence transduction tasks. One key factor affecting the\nability to learn such dependencies is the length of the paths forward and backward signals have to\ntraverse in the network. The shorter these paths between any combination of positions in the input\nand output sequences, the easier it is to learn long-range dependencies [ 12]. Hence we also compare\nthe maximum path length between any two input and output positions in networks composed of the\ndifferent layer types.\nAs noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially\nexecuted operations, whereas a recurrent layer requires O(n)sequential operations. In terms of\ncomputational complexity, self-attention layers are faster than recurrent layers when the sequence\nlength nis smaller than the representation dimensionality d, which is most often the case with\nsentence representations used by state-of-the-art models in machine translations, such as word-piece\n[38] and byte-pair [ 31] representations. To improve computational performance for tasks involving\nvery long sequences, self-attention could be restricted to considering only a neighborhood of size rin\nthe input sequence centered around the respective output position. This would increase the maximum\npath length to O(n/r). We plan to investigate this approach further in future work.\nA single convolutional layer with kernel width k \u003c n does not connect all pairs of input and output\npositions. Doing so requires a stack of O(n/k)convolutional layers in the case of contiguous kernels,\norO(logk(n))in the case of dilated convolutions [ 18], increasing the length of the longest paths\nbetween any two positions in the network. Convolutional layers are generally more expensive than\nrecurrent layers, by a factor of k. Separable convolutions [ 6], however, decrease the complexity\nconsiderably, to O(k\u00b7n\u00b7d+n\u00b7d2). Even with k=n, however, the complexity of a separable\nconvolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer,\nthe approach we take in our model.\nAs side benefit, self-attention could yield more interpretable models. We inspect attention distributions\nfrom our models and present and discuss examples in the appendix. Not only do individual attention\nheads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic\nand semantic structure of the sentences.", "title": "Why Self-Attention"}, {"color": "#FF0000", "font": "25px arial black", "id": 3, "ids": "3.1", "label": "3.1", "shape": "dot", "size": 10, "text": "Encoder: The encoder is composed of a stack of N= 6 identical layers. Each layer has two\nsub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-\nwise fully connected feed-forward network. We employ a residual connection [ 11] around each of\nthe two sub-layers, followed by layer normalization [ 1]. That is, the output of each sub-layer is\nLayerNorm( x+ Sublayer( x)), where Sublayer( x)is the function implemented by the sub-layer\nitself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\nlayers, produce outputs of dimension dmodel = 512 .\nDecoder: The decoder is also composed of a stack of N= 6identical layers. In addition to the two\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\nattention over the output of the encoder stack. Similar to the encoder, we employ residual connections\naround each of the sub-layers, followed by layer normalization. We also modify the self-attention\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\nmasking, combined with fact that the output embeddings are offset by one position, ensures that the\npredictions for position ican depend only on the known outputs at positions less than i.", "title": "Encoder and Decoder Stacks"}, {"color": "#FF0000", "font": "25px arial black", "id": 8, "ids": "3.3", "label": "3.3", "shape": "dot", "size": 10, "text": "In addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\nconnected feed-forward network, which is applied to each position separately and identically. This\nconsists of two linear transformations with a ReLU activation in between.\nFFN( x) = max(0 , xW 1+b1)W2+b2 (2)\nWhile the linear transformations are the same across different positions, they use different parameters\nfrom layer to layer. Another way of describing this is as two convolutions with kernel size 1.\nThe dimensionality of input and output is dmodel = 512 , and the inner-layer has dimensionality\ndff= 2048 .", "title": "Position-wise Feed-Forward Networks"}, {"color": "#FF0000", "font": "25px arial black", "id": 4, "ids": "3.2", "label": "3.2", "shape": "dot", "size": 10, "text": "An attention function can be described as mapping a query and a set of key-value pairs to an output,\nwhere the query, keys, values, and output are all vectors. The output is computed as a weighted sum\nScaled Dot-Product Attention\n Multi-Head Attention\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\nattention layers running in parallel.\nof the values, where the weight assigned to each value is computed by a compatibility function of the\nquery with the corresponding key.", "title": "Attention"}, {"color": "#FF0000", "font": "25px arial black", "id": 5, "ids": "3.2.1", "label": "3.2.1", "shape": "dot", "size": 10, "text": "We call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of\nqueries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\nquery with all keys, divide each by\u221adk, and apply a softmax function to obtain the weights on the\nvalues.\nIn practice, we compute the attention function on a set of queries simultaneously, packed together\ninto a matrix Q. The keys and values are also packed together into matrices KandV. We compute\nthe matrix of outputs as:\nAttention( Q, K, V ) = softmax(QKT\n\u221adk)V (1)\nThe two most commonly used attention functions are additive attention [ 2], and dot-product (multi-\nplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\nof1\u221adk. Additive attention computes the compatibility function using a feed-forward network with\na single hidden layer. While the two are similar in theoretical complexity, dot-product attention is\nmuch faster and more space-efficient in practice, since it can be implemented using highly optimized\nmatrix multiplication code.\nWhile for small values of dkthe two mechanisms perform similarly, additive attention outperforms\ndot product attention without scaling for larger values of dk[3]. We suspect that for large values of\ndk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\nextremely small gradients4. To counteract this effect, we scale the dot products by1\u221adk.", "title": "Scaled Dot-Product Attention"}, {"color": "#FF0000", "font": "25px arial black", "id": 6, "ids": "3.2.2", "label": "3.2.2", "shape": "dot", "size": 10, "text": "Instead of performing a single attention function with dmodel-dimensional keys, values and queries,\nwe found it beneficial to linearly project the queries, keys and values htimes with different, learned\nlinear projections to dk,dkanddvdimensions, respectively. On each of these projected versions of\nqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\n4To illustrate why the dot products get large, assume that the components of qandkare independent random\nvariables with mean 0and variance 1. Then their dot product, q\u00b7k=Pdk\ni=1qiki, has mean 0and variance dk.\noutput values. These are concatenated and once again projected, resulting in the final values, as\ndepicted in Figure 2.\nMulti-head attention allows the model to jointly attend to information from different representation\nsubspaces at different positions. With a single attention head, averaging inhibits this.\nMultiHead( Q, K, V ) = Concat(head 1, ...,head h)WO\nwhere head i= Attention( QWQ\ni, KWK\ni, V WV\ni)\nWhere the projections are parameter matrices WQ\ni\u2208Rdmodel\u00d7dk,WK\ni\u2208Rdmodel\u00d7dk,WV\ni\u2208Rdmodel\u00d7dv\nandWO\u2208Rhdv\u00d7dmodel.\nIn this work we employ h= 8 parallel attention layers, or heads. For each of these we use\ndk=dv=dmodel/h= 64 . Due to the reduced dimension of each head, the total computational cost\nis similar to that of single-head attention with full dimensionality.", "title": "Multi-Head Attention"}, {"color": "#FF0000", "font": "25px arial black", "id": 16, "ids": "5.4", "label": "5.4", "shape": "dot", "size": 10, "text": "We employ three types of regularization during training:\nTable 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the\nEnglish-to-German and English-to-French newstest2014 tests at a fraction of the training cost.\nModelBLEU Training Cost (FLOPs)\nEN-DE EN-FR EN-DE EN-FR\nByteNet [18] 23.75\nDeep-Att + PosUnk [39] 39.2 1.0\u00b71020\nGNMT + RL [38] 24.6 39.92 2.3\u00b710191.4\u00b71020\nConvS2S [9] 25.16 40.46 9.6\u00b710181.5\u00b71020\nMoE [32] 26.03 40.56 2.0\u00b710191.2\u00b71020\nDeep-Att + PosUnk Ensemble [39] 40.4 8.0\u00b71020\nGNMT + RL Ensemble [38] 26.30 41.16 1.8\u00b710201.1\u00b71021\nConvS2S Ensemble [9] 26.36 41.29 7.7\u00b710191.2\u00b71021\nTransformer (base model) 27.3 38.1 3.3\u00b71018\nTransformer (big) 28.4 41.8 2.3\u00b71019\nResidual Dropout We apply dropout [ 33] to the output of each sub-layer, before it is added to the\nsub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the\npositional encodings in both the encoder and decoder stacks. For the base model, we use a rate of\nPdrop= 0.1.\nLabel Smoothing During training, we employed label smoothing of value \u03f5ls= 0.1[36]. This\nhurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.", "title": "Regularization"}, {"color": "#FF0000", "font": "25px arial black", "id": 17, "ids": "6.1", "label": "6.1", "shape": "dot", "size": 10, "text": "On the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)\nin Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0\nBLEU, establishing a new state-of-the-art BLEU score of 28.4. The configuration of this model is\nlisted in the bottom line of Table 3. Training took 3.5days on 8P100 GPUs. Even our base model\nsurpasses all previously published models and ensembles, at a fraction of the training cost of any of\nthe competitive models.\nOn the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0,\noutperforming all of the previously published single models, at less than 1/4the training cost of the\nprevious state-of-the-art model. The Transformer (big) model trained for English-to-French used\ndropout rate Pdrop= 0.1, instead of 0.3.\nFor the base models, we used a single model obtained by averaging the last 5 checkpoints, which\nwere written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We\nused beam search with a beam size of 4and length penalty \u03b1= 0.6[38]. These hyperparameters\nwere chosen after experimentation on the development set. We set the maximum output length during\ninference to input length + 50, but terminate early when possible [38].\nTable 2 summarizes our results and compares our translation quality and training costs to other model\narchitectures from the literature. We estimate the number of floating point operations used to train a\nmodel by multiplying the training time, the number of GPUs used, and an estimate of the sustained\nsingle-precision floating-point capacity of each GPU5.", "title": "Machine Translation"}, {"color": "#00FF00", "font": "25px arial black", "id": 22, "ids": "1", "label": "1", "shape": "dot", "size": 10, "text": "Recurrent neural networks (RNNs) are a rich class of dynamic models that have\nbeen used to generate sequences in domains as diverse as music [6, 4], text [30]\nand motion capture data [29]. RNNs can be trained for sequence generation by\nprocessing real data sequences one step at a time and predicting what comes\nnext. Assuming the predictions are probabilistic, novel sequences can be gener-\nated from a trained network by iteratively sampling from the network\u0027s output\ndistribution, then feeding in the sample as input at the next step. In other\nwords by making the network treat its inventions as if they were real, much like\na person dreaming. Although the network itself is deterministic, the stochas-\nticity injected by picking samples induces a distribution over sequences. This\ndistribution is conditional, since the internal state of the network, and hence its\npredictive distribution, depends on the previous inputs.\nRNNs are `fuzzy\u0027 in the sense that they do not use exact templates from\nthe training data to make predictions, but rather|like other neural networks|\nuse their internal representation to perform a high-dimensional interpolation\nbetween training examples. This distinguishes them from n-gram models and\ncompression algorithms such as Prediction by Partial Matching [5], whose pre-\ndictive distributions are determined by counting exact matches between the\nrecent history and the training set. The result|which is immediately appar-\nent from the samples in this paper|is that RNNs (unlike template-based al-\ngorithms) synthesise and reconstitute the training data in a complex way, and\nrarely generate the same thing twice. Furthermore, fuzzy predictions do not suf-\nfer from the curse of dimensionality, and are therefore much better at modelling\nreal-valued or multivariate data than exact matches.\nIn principle a large enough RNN should be su\u000ecient to generate sequences\nof arbitrary complexity. In practice however, standard RNNs are unable to\nstore information about past inputs for very long [15]. As well as diminishing\ntheir ability to model long-range structure, this `amnesia\u0027 makes them prone to\ninstability when generating sequences. The problem (common to all conditional\ngenerative models) is that if the network\u0027s predictions are only based on the last\nfew inputs, and these inputs were themselves predicted by the network, it has\nlittle opportunity to recover from past mistakes. Having a longer memory has\na stabilising e\u000bect, because even if the network cannot make sense of its recent\nhistory, it can look further back in the past to formulate its predictions. The\nproblem of instability is especially acute with real-valued data, where it is easy\nfor the predictions to stray from the manifold on which the training data lies.\nOne remedy that has been proposed for conditional models is to inject noise into\nthe predictions before feeding them back into the model [31], thereby increasing\nthe model\u0027s robustness to surprising inputs. However we believe that a better\nmemory is a more profound and e\u000bective solution.\nLong Short-term Memory (LSTM) [16] is an RNN architecture designed to\nbe better at storing and accessing information than standard RNNs. LSTM has\nrecently given state-of-the-art results in a variety of sequence processing tasks,\nincluding speech and handwriting recognition [10, 12]. The main goal of this\npaper is to demonstrate that LSTM can use its memory to generate complex,\nrealistic sequences containing long-range structure.\nSection 2 de\fnes a `deep\u0027 RNN composed of stacked LSTM layers, and ex-\nplains how it can be trained for next-step prediction and hence sequence gener-\nation. Section 3 applies the prediction network to text from the Penn Treebank\nand Hutter Prize Wikipedia datasets. The network\u0027s performance is compet-\nitive with state-of-the-art language models, and it works almost as well when\npredicting one character at a time as when predicting one word at a time. The\nhighlight of the section is a generated sample of Wikipedia text, which showcases\nthe network\u0027s ability to model long-range dependencies. Section 4 demonstrates\nhow the prediction network can be applied to real-valued data through the use\nof a mixture density output layer, and provides experimental results on the IAM\nOnline Handwriting Database. It also presents generated handwriting samples\nproving the network\u0027s ability to learn letters and short words direct from pen\ntraces, and to model global features of handwriting style. Section 5 introduces\nan extension to the prediction network that allows it to condition its outputs on\na short annotation sequence whose alignment with the predictions is unknown.\nThis makes it suitable for handwriting synthesis, where a human user inputs\na text and the algorithm generates a handwritten version of it. The synthesis\nnetwork is trained on the IAM database, then used to generate cursive hand-\nwriting samples, some of which cannot be distinguished from real data by the\nFigure 1: Deep recurrent neural network prediction architecture. The\ncircles represent network layers, the solid lines represent weighted connections\nand the dashed lines represent predictions.\nnaked eye. A method for biasing the samples towards higher probability (and\ngreater legibility) is described, along with a technique for `priming\u0027 the sam-\nples on real data and thereby mimicking a particular writer\u0027s style. Finally,\nconcluding remarks and directions for future work are given in Section 6.", "title": "Introduction"}, {"color": "#00FF00", "font": "25px arial black", "id": 38, "ids": "6", "label": "6", "shape": "dot", "size": 10, "text": "This paper has demonstrated the ability of Long Short-Term Memory recur-\nrent neural networks to generate both discrete and real-valued sequences with\ncomplex, long-range structure using next-step prediction. It has also introduced\na novel convolutional mechanism that allows a recurrent network to condition\nits predictions on an auxiliary annotation sequence, and used this approach to\nsynthesise diverse and realistic samples of online handwriting. Furthermore, it\nhas shown how these samples can be biased towards greater legibility, and how\nthey can be modelled on the style of a particular writer.\nSeveral directions for future work suggest themselves. One is the applica-\ntion of the network to speech synthesis, which is likely to be more challenging\nthan handwriting synthesis due to the greater dimensionality of the data points.\nAnother is to gain a better insight into the internal representation of the data,\nand to use this to manipulate the sample distribution directly. It would also\nbe interesting to develop a mechanism to automatically extract high-level an-\nnotations from sequence data. In the case of handwriting, this could allow for\nFigure 16: Samples biased towards higher probability. The probability\nbiasesbare shown at the left. As the bias increases the diversity decreases and\nthe samples tend towards a kind of `average handwriting\u0027 which is extremely\nregular and easy to read (easier, in fact, than most of the real handwriting in the\ntraining set). Note that even when the variance disappears, the same letter is\nnot written the same way at di\u000berent points in a sequence (for examples the `e\u0027s\nin \\exactly the same\", the `l\u0027s in \\until they all look\"), because the predictions\nare still in\ruenced by the previous outputs. If you look closely you can see that\nthe last three lines are not quite exactly the same.\nFigure 17: A slight bias. The top line in each block is real. The rest are\nsamples from the synthesis network with a probability bias of 0.15, which seems\nto give a good balance between diversity and legibility.\nFigure 18: Samples primed with real sequences. The priming sequences\n(drawn from the training set) are shown at the top of each block. None of the\nlines in the sampled text exist in the training set. The samples were selected\nfor legibility.\nFigure 19: Samples primed with real sequences (cotd).\nFigure 20: Samples primed with real sequences and biased towards\nhigher probability. The priming sequences are at the top of the blocks. The\nprobability bias was 1. None of the lines in the sampled text exist in the training\nset.\nFigure 21: Samples primed with real sequences and biased towards\nhigher probability (cotd)\nmore nuanced annotations than just text, for example stylistic features, di\u000berent\nforms of the same letter, information about stroke order and so on.\nAcknowledgements\nThanks to Yichuan Tang, Ilya Sutskever, Navdeep Jaitly, Geo\u000brey Hinton and\nother colleagues at the University of Toronto for numerous useful comments\nand suggestions. This work was supported by a Global Scholarship from the\nCanadian Institute for Advanced Research.\nReferences\n[1] Y. Bengio, P. Simard, and P. Frasconi. Learning long-term dependencies\nwith gradient descent is di\u000ecult. IEEE Transactions on Neural Networks ,\n5(2):157{166, March 1994.\n[2] C. Bishop. Mixture density networks. Technical report, 1994.\n[3] C. Bishop. Neural Networks for Pattern Recognition . Oxford University\nPress, Inc., 1995.\n[4] N. Boulanger-Lewandowski, Y. Bengio, and P. Vincent. Modeling tempo-\nral dependencies in high-dimensional sequences: Application to polyphonic\nmusic generation and transcription. In Proceedings of the Twenty-nine In-\nternational Conference on Machine Learning (ICML\u002712) , 2012.\n[5] J. G. Cleary, Ian, and I. H. Witten. Data compression using adaptive cod-\ning and partial string matching. IEEE Transactions on Communications ,\n32:396{402, 1984.\n[6] D. Eck and J. Schmidhuber. A \frst look at music composition using lstm\nrecurrent neural networks. Technical report, IDSIA USI-SUPSI Instituto\nDalle Molle.\n[7] F. Gers, N. Schraudolph, and J. Schmidhuber. Learning precise timing\nwith LSTM recurrent networks. Journal of Machine Learning Research ,\n3:115{143, 2002.\n[8] A. Graves. Practical variational inference for neural networks. In Advances\nin Neural Information Processing Systems , volume 24, pages 2348{2356.\n2011.\n[9] A. Graves. Sequence transduction with recurrent neural networks. In ICML\nRepresentation Learning Worksop , 2012.\n[10] A. Graves, A. Mohamed, and G. Hinton. Speech recognition with deep\nrecurrent neural networks. In Proc. ICASSP , 2013.\n[11] A. Graves and J. Schmidhuber. Framewise phoneme classi\fcation with bidi-\nrectional LSTM and other neural network architectures. Neural Networks ,\n18:602{610, 2005.\n[12] A. Graves and J. Schmidhuber. O\u000fine handwriting recognition with multi-\ndimensional recurrent neural networks. In Advances in Neural Information\nProcessing Systems , volume 21, 2008.\n[13] P. D. Gr\u007f unwald. The Minimum Description Length Principle (Adaptive\nComputation and Machine Learning) . The MIT Press, 2007.\n[14] G. Hinton. A Practical Guide to Training Restricted Boltzmann Machines.\nTechnical report, 2010.\n[15] S. Hochreiter, Y. Bengio, P. Frasconi, and J. Schmidhuber. Gradient Flow\nin Recurrent Nets: the Di\u000eculty of Learning Long-term Dependencies.\nIn S. C. Kremer and J. F. Kolen, editors, A Field Guide to Dynamical\nRecurrent Neural Networks . 2001.\n[16] S. Hochreiter and J. Schmidhuber. Long Short-Term Memory. Neural\nComputation , 9(8):1735{1780, 1997.\n[17] M. Hutter. The Human Knowledge Compression Contest, 2012.\n[18] K.-C. Jim, C. Giles, and B. Horne. An analysis of noise in recurrent neural\nnetworks: convergence and generalization. Neural Networks, IEEE Trans-\nactions on , 7(6):1424 {1438, 1996.\n[19] S. Johansson, R. Atwell, R. Garside, and G. Leech. The tagged LOB corpus\nuser\u0027s manual; Norwegian Computing Centre for the Humanities, 1986.\n[20] B. Knoll and N. de Freitas. A machine learning perspective on predictive\ncoding with paq. CoRR , abs/1108.3298, 2011.\n[21] M. Liwicki and H. Bunke. IAM-OnDB - an on-line English sentence\ndatabase acquired from handwritten text on a whiteboard. In Proc. 8th\nInt. Conf. on Document Analysis and Recognition , volume 2, pages 956{\n961, 2005.\n[22] M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz. Building a large\nannotated corpus of english: The penn treebank. COMPUTATIONAL\nLINGUISTICS , 19(2):313{330, 1993.\n[23] T. Mikolov. Statistical Language Models based on Neural Networks . PhD\nthesis, Brno University of Technology, 2012.\n[24] T. Mikolov, I. Sutskever, A. Deoras, H. Le, S. Kombrink, and J. Cernocky.\nSubword language modeling with neural networks. Technical report, Un-\npublished Manuscript, 2012.\n[25] A. Mnih and G. Hinton. A Scalable Hierarchical Distributed Language\nModel. In Advances in Neural Information Processing Systems , volume 21,\n2008.\n[26] A. Mnih and Y. W. Teh. A fast and simple algorithm for training neural\nprobabilistic language models. In Proceedings of the 29th International\nConference on Machine Learning , pages 1751{1758, 2012.\n[27] T. N. Sainath, A. Mohamed, B. Kingsbury, and B. Ramabhadran. Low-\nrank matrix factorization for deep neural network training with high-\ndimensional output targets. In Proc. ICASSP , 2013.\n[28] M. Schuster. Better generative models for sequential data problems: Bidi-\nrectional recurrent mixture density networks. pages 589{595. The MIT\nPress, 1999.\n[29] I. Sutskever, G. E. Hinton, and G. W. Taylor. The recurrent temporal\nrestricted boltzmann machine. pages 1601{1608, 2008.\n[30] I. Sutskever, J. Martens, and G. Hinton. Generating text with recurrent\nneural networks. In ICML , 2011.\n[31] G. W. Taylor and G. E. Hinton. Factored conditional restricted boltzmann\nmachines for modeling motion style. In Proc. 26th Annual International\nConference on Machine Learning , pages 1025{1032, 2009.\n[32] T. Tieleman and G. Hinton. Lecture 6.5 - rmsprop: Divide the gradient by\na running average of its recent magnitude, 2012.\n[33] R. Williams and D. Zipser. Gradient-based learning algorithms for recur-\nrent networks and their computational complexity. In Back-propagation:\nTheory, Architectures and Applications , pages 433{486. 1995.", "title": "Conclusions and Future Work"}, {"color": "#00FF00", "font": "25px arial black", "id": 39, "ids": "", "label": 39, "shape": "dot", "size": 10, "text": "This paper shows how Long Short-term Memory recurrent neural net-\nworks can be used to generate complex sequences with long-range struc-\nture, simply by predicting one data point at a time. The approach is\ndemonstrated for text (where the data are discrete) and online handwrit-\ning (where the data are real-valued). It is then extended to handwriting\nsynthesis by allowing the network to condition its predictions on a text\nsequence. The resulting system is able to generate highly realistic cursive\nhandwriting in a wide variety of styles.\n1 Introduction\nRecurrent neural networks (RNNs) are a rich class of dynamic models that have\nbeen used to generate sequences in domains as diverse as music ", "title": "Abstract"}, {"color": "#00FF00", "font": "25px arial black", "id": 35, "ids": "5.3", "label": "5.3", "shape": "dot", "size": 10, "text": "Given c, an unbiased sample can be picked from Pr( xjc) by iteratively drawing\nxt+1from Pr (xt+1jyt), just as for the prediction network. The only di\u000berence is\nthat we must also decide when the synthesis network has \fnished writing the text\nand should stop making any future decisions. To do this, we use the following\nheuristic: as soon as \u001e(t;U+ 1)\u003e\u001e(t;u)81\u0014u\u0014Uthe current input xtis\nde\fned as the end of the sequence and sampling ends. Examples of unbiased\nsynthesis samples are shown in Fig. 15. These and all subsequent \fgures were\ngenerated using the synthesis network retrained with adaptive weight noise.\nNotice how stylistic traits, such as character size, slant, cursiveness etc. vary\n2This was an oversight; however it led to the interesting result that when the text contains\na non-letter, the network must select a digits or punctuation mark to generate. Sometimes\nthe character can be be inferred from the context (e.g. the apostrophe in \\can\u0027t\"); otherwise\nit is chosen at random.\nwidely between the samples, but remain more-or-less consistent within them.\nThis suggests that the network identi\fes the traits early on in the sequence,\nthen remembers them until the end. By looking through enough samples for a\ngiven text, it appears to be possible to \fnd virtually any combination of stylistic\ntraits, which suggests that the network models them independently both from\neach other and from the text.\n`Blind taste tests\u0027 carried out by the author during presentations suggest\nthat at least some unbiased samples cannot be distinguished from real hand-\nwriting by the human eye. Nonetheless the network does make mistakes we\nwould not expect a human writer to make, often involving missing, confused\nor garbled letters3; this suggests that the network sometimes has trouble de-\ntermining the alignment between the characters and the trace. The number of\nmistakes increases markedly when less common words or phrases are included\nin the character sequence. Presumably this is because the network learns an\nimplicit character-level language model from the training set that gets confused\nwhen rare or unknown transitions occur.", "title": "Unbiased Sampling"}, {"color": "#00FF00", "font": "25px arial black", "id": 36, "ids": "5.4", "label": "5.4", "shape": "dot", "size": 10, "text": "One problem with unbiased samples is that they tend to be di\u000ecult to read\n(partly because real handwriting is di\u000ecult to read, and partly because the\nnetwork is an imperfect model). Intuitively, we would expect the network to\ngive higher probability to good handwriting because it tends to be smoother\nand more predictable than bad handwriting. If this is true, we should aim to\noutput more probable elements of Pr( xjc) if we want the samples to be easier to\nread. A principled search for high probability samples could lead to a di\u000ecult\ninference problem, as the probability of every output depends on all previous\noutputs. However a simple heuristic, where the sampler is biased towards more\nprobable predictions at each step independently, generally gives good results.\nDe\fne the probability bias bas a real number greater than or equal to zero.\nBefore drawing a sample from Pr( xt+1jyt), each standard deviation \u001bj\ntin the\nGaussian mixture is recalculated from Eq. (21) to\n\u001bj\nt= exp\u0010\n^\u001bj\nt\u0000b\u0011\n(61)\nand each mixture weight is recalculated from Eq. (19) to\n\u0019j\nt=exp\u0010\n^\u0019j\nt(1 +b)\u0011\nPM\nj0=1exp\u0010\n^\u0019j0\nt(1 +b)\u0011 (62)\nThis arti\fcially reduces the variance in both the choice of component from the\nmixture, and in the distribution of the component itself. When b= 0 unbiased\nsampling is recovered, and as b!1 the variance in the sampling disappears\n3We expect humans to make mistakes like misspelling `temperament\u0027 as `temperement\u0027, as\nthe second writer in Fig. 15 seems to have done.\nFigure 15: Real and generated handwriting . The top line in each block is\nreal, the rest are unbiased samples from the synthesis network. The two texts\nare from the validation set and were not seen during training.\nand the network always outputs the mode of the most probable component in\nthe mixture (which is not necessarily the mode of the mixture, but at least a\nreasonable approximation). Fig. 16 shows the e\u000bect of progressively increasing\nthe bias, and Fig. 17 shows samples generated with a low bias for the same texts\nas Fig. 15.", "title": "Biased Sampling"}, {"color": "#FF0000", "font": "25px arial black", "id": 9, "ids": "3.4", "label": "3.4", "shape": "dot", "size": 10, "text": "Similarly to other sequence transduction models, we use learned embeddings to convert the input\ntokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor-\nmation and softmax function to convert the decoder output to predicted next-token probabilities. In\nour model, we share the same weight matrix between the two embedding layers and the pre-softmax\nlinear transformation, similar to [ 30]. In the embedding layers, we multiply those weights by\u221admodel.\nTable 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations\nfor different layer types. nis the sequence length, dis the representation dimension, kis the kernel\nsize of convolutions and rthe size of the neighborhood in restricted self-attention.\nLayer Type Complexity per Layer Sequential Maximum Path Length\nOperations\nSelf-Attention O(n2\u00b7d) O(1) O(1)\nRecurrent O(n\u00b7d2) O(n) O(n)\nConvolutional O(k\u00b7n\u00b7d2) O(1) O(logk(n))\nSelf-Attention (restricted) O(r\u00b7n\u00b7d) O(1) O(n/r)", "title": "Embeddings and Softmax"}, {"color": "#FF0000", "font": "25px arial black", "id": 10, "ids": "3.5", "label": "3.5", "shape": "dot", "size": 10, "text": "Since our model contains no recurrence and no convolution, in order for the model to make use of the\norder of the sequence, we must inject some information about the relative or absolute position of the\ntokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the\nbottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel\nas the embeddings, so that the two can be summed. There are many choices of positional encodings,\nlearned and fixed [9].\nIn this work, we use sine and cosine functions of different frequencies:\nPE(pos,2i)=sin(pos/100002i/d model)\nPE(pos,2i+1)=cos(pos/100002i/d model)\nwhere posis the position and iis the dimension. That is, each dimension of the positional encoding\ncorresponds to a sinusoid. The wavelengths form a geometric progression from 2\u03c0to10000 \u00b72\u03c0. We\nchose this function because we hypothesized it would allow the model to easily learn to attend by\nrelative positions, since for any fixed offset k,PEpos+kcan be represented as a linear function of\nPEpos.\nWe also experimented with using learned positional embeddings [ 9] instead, and found that the two\nversions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version\nbecause it may allow the model to extrapolate to sequence lengths longer than the ones encountered\nduring training.", "title": "Positional Encoding"}, {"color": "#FF0000", "font": "25px arial black", "id": 12, "ids": "5", "label": "5", "shape": "dot", "size": 10, "text": "This section describes the training regime for our models.", "title": "Training"}, {"color": "#FF0000", "font": "25px arial black", "id": 13, "ids": "5.1", "label": "5.1", "shape": "dot", "size": 10, "text": "We trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million\nsentence pairs. Sentences were encoded using byte-pair encoding [ 3], which has a shared source-\ntarget vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT\n2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece\nvocabulary [ 38]. Sentence pairs were batched together by approximate sequence length. Each training\nbatch contained a set of sentence pairs containing approximately 25000 source tokens and 25000\ntarget tokens.", "title": "Training Data and Batching"}, {"color": "#FF0000", "font": "25px arial black", "id": 14, "ids": "5.2", "label": "5.2", "shape": "dot", "size": 10, "text": "We trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using\nthe hyperparameters described throughout the paper, each training step took about 0.4 seconds. We\ntrained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the\nbottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps\n(3.5 days).", "title": "Hardware and Schedule"}, {"color": "#FF0000", "font": "25px arial black", "id": 15, "ids": "5.3", "label": "5.3", "shape": "dot", "size": 10, "text": "We used the Adam optimizer [ 20] with \u03b21= 0.9,\u03b22= 0.98and\u03f5= 10\u22129. We varied the learning\nrate over the course of training, according to the formula:\nlrate =d\u22120.5\nmodel\u00b7min(step_num\u22120.5, step _num\u00b7warmup _steps\u22121.5) (3)\nThis corresponds to increasing the learning rate linearly for the first warmup _steps training steps,\nand decreasing it thereafter proportionally to the inverse square root of the step number. We used\nwarmup _steps = 4000 .", "title": "Optimizer"}, {"color": "#FF0000", "font": "25px arial black", "id": 18, "ids": "6.2", "label": "6.2", "shape": "dot", "size": 10, "text": "To evaluate the importance of different components of the Transformer, we varied our base model\nin different ways, measuring the change in performance on English-to-German translation on the\n5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.\nTable 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\nper-word perplexities.\nN d model dff h d k dvPdrop \u03f5lstrain PPL BLEU params\nsteps (dev) (dev) \u00d7106\nbase 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65\n(A)1 512 512 5.29 24.9\n4 128 128 5.00 25.5\n16 32 32 4.91 25.8\n32 16 16 5.01 25.4\n(B)16 5.16 25.1 58\n32 5.01 25.4 60\n(C)2 6.11 23.7 36\n4 5.19 25.3 50\n8 4.88 25.5 80\n256 32 32 5.75 24.5 28\n1024 128 128 4.66 26.0 168\n1024 5.12 25.4 53\n4096 4.75 26.2 90\n(D)0.0 5.77 24.6\n0.2 4.95 25.5\n0.0 4.67 25.3\n0.2 5.47 25.7\n(E) positional embedding instead of sinusoids 4.92 25.7\nbig 6 1024 4096 16 0.3 300K 4.33 26.4 213\ndevelopment set, newstest2013. We used beam search as described in the previous section, but no\ncheckpoint averaging. We present these results in Table 3.\nIn Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions,\nkeeping the amount of computation constant, as described in Section 3.2.2. While single-head\nattention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.\nIn Table 3 rows (B), we observe that reducing the attention key size dkhurts model quality. This\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\nfunction than dot product may be beneficial. We further observe in rows (C) and (D) that, as expected,\nbigger models are better, and dropout is very helpful in avoiding over-fitting. In row (E) we replace our\nsinusoidal positional encoding with learned positional embeddings [ 9], and observe nearly identical\nresults to the base model.", "title": "Model Variations"}, {"color": "#FF0000", "font": "25px arial black", "id": 19, "ids": "6.3", "label": "6.3", "shape": "dot", "size": 10, "text": "To evaluate if the Transformer can generalize to other tasks we performed experiments on English\nconstituency parsing. This task presents specific challenges: the output is subject to strong structural\nconstraints and is significantly longer than the input. Furthermore, RNN sequence-to-sequence\nmodels have not been able to attain state-of-the-art results in small-data regimes [37].\nWe trained a 4-layer transformer with dmodel = 1024 on the Wall Street Journal (WSJ) portion of the\nPenn Treebank [ 25], about 40K training sentences. We also trained it in a semi-supervised setting,\nusing the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences\n[37]. We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens\nfor the semi-supervised setting.\nWe performed only a small number of experiments to select the dropout, both attention and residual\n(section 5.4), learning rates and beam size on the Section 22 development set, all other parameters\nremained unchanged from the English-to-German base translation model. During inference, we\nTable 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23\nof WSJ)\nParser Training WSJ 23 F1\nVinyals \u0026 Kaiser el al. (2014) [37] WSJ only, discriminative 88.3\nPetrov et al. (2006) [29] WSJ only, discriminative 90.4\nZhu et al. (2013) [40] WSJ only, discriminative 90.4\nDyer et al. (2016) [8] WSJ only, discriminative 91.7\nTransformer (4 layers) WSJ only, discriminative 91.3\nZhu et al. (2013) [40] semi-supervised 91.3\nHuang \u0026 Harper (2009) [14] semi-supervised 91.3\nMcClosky et al. (2006) [26] semi-supervised 92.1\nVinyals \u0026 Kaiser el al. (2014) [37] semi-supervised 92.1\nTransformer (4 layers) semi-supervised 92.7\nLuong et al. (2015) [23] multi-task 93.0\nDyer et al. (2016) [8] generative 93.3\nincreased the maximum output length to input length + 300. We used a beam size of 21and\u03b1= 0.3\nfor both WSJ only and the semi-supervised setting.\nOur results in Table 4 show that despite the lack of task-specific tuning our model performs sur-\nprisingly well, yielding better results than all previously reported models with the exception of the\nRecurrent Neural Network Grammar [8].\nIn contrast to RNN sequence-to-sequence models [ 37], the Transformer outperforms the Berkeley-\nParser [29] even when training only on the WSJ training set of 40K sentences.", "title": "English Constituency Parsing"}, {"color": "#00FF00", "font": "25px arial black", "id": 23, "ids": "2", "label": "2", "shape": "dot", "size": 10, "text": "Fig. 1 illustrates the basic recurrent neural network prediction architecture used\nin this paper. An input vector sequence x= (x1;:::;x T) is passed through\nweighted connections to a stack of Nrecurrently connected hidden layers to\ncompute \frst the hidden vector sequences hn= (hn\n1;:::;hn\nT) and then the\noutput vector sequence y= (y1;:::;y T). Each output vector ytis used to\nparameterise a predictive distribution Pr( xt+1jyt) over the possible next inputs\nxt+1. The \frst element x1of every input sequence is always a null vector whose\nentries are all zero; the network therefore emits a prediction for x2, the \frst\nreal input, with no prior information. The network is `deep\u0027 in both space\nand time, in the sense that every piece of information passing either vertically\nor horizontally through the computation graph will be acted on by multiple\nsuccessive weight matrices and nonlinearities.\nNote the `skip connections\u0027 from the inputs to all hidden layers, and from\nall hidden layers to the outputs. These make it easier to train deep networks,\nby reducing the number of processing steps between the bottom of the network\nand the top, and thereby mitigating the `vanishing gradient\u0027 problem [1]. In\nthe special case that N= 1 the architecture reduces to an ordinary, single layer\nnext step prediction RNN.\nThe hidden layer activations are computed by iterating the following equa-\ntions fromt= 1 toTand fromn= 2 toN:\nh1\nt=H\u0000\nWih1xt+Wh1h1h1\nt\u00001+b1\nh\u0001\n(1)\nhn\nt=H\u0000\nWihnxt+Whn\u00001hnhn\u00001\nt+Whnhnhn\nt\u00001+bn\nh\u0001\n(2)\nwhere the Wterms denote weight matrices (e.g. Wihnis the weight matrix\nconnecting the inputs to the nthhidden layer, Wh1h1is the recurrent connection\nat the \frst hidden layer, and so on), the bterms denote bias vectors (e.g. byis\noutput bias vector) and His the hidden layer function.\nGiven the hidden sequences, the output sequence is computed as follows:\n^yt=by+NX\nn=1Whnyhn\nt (3)\nyt=Y(^yt) (4)\nwhereYis the output layer function. The complete network therefore de\fnes\na function, parameterised by the weight matrices, from input histories x1:tto\noutput vectors yt.\nThe output vectors ytare used to parameterise the predictive distribution\nPr(xt+1jyt) for the next input. The form of Pr( xt+1jyt) must be chosen carefully\nto match the input data. In particular, \fnding a good predictive distribution\nfor high-dimensional, real-valued data (usually referred to as density modelling ),\ncan be very challenging.\nThe probability given by the network to the input sequence xis\nPr(x) =TY\nt=1Pr(xt+1jyt) (5)\nand the sequence loss L(x) used to train the network is the negative logarithm\nof Pr( x):\nL(x) =\u0000TX\nt=1log Pr(xt+1jyt) (6)\nThe partial derivatives of the loss with respect to the network weights can be\ne\u000eciently calculated with backpropagation through time [33] applied to the\ncomputation graph shown in Fig. 1, and the network can then be trained with\ngradient descent.", "title": "Prediction Network"}, {"color": "#00FF00", "font": "25px arial black", "id": 24, "ids": "2.1", "label": "2.1", "shape": "dot", "size": 10, "text": "In most RNNs the hidden layer function His an elementwise application of a\nsigmoid function. However we have found that the Long Short-Term Memory\nFigure 2: Long Short-term Memory Cell\n(LSTM) architecture [16], which uses purpose-built memory cells to store infor-\nmation, is better at \fnding and exploiting long range dependencies in the data.\nFig. 2 illustrates a single LSTM memory cell. For the version of LSTM used in\nthis paper [7]His implemented by the following composite function:\nit=\u001b(Wxixt+Whiht\u00001+Wcict\u00001+bi) (7)\nft=\u001b(Wxfxt+Whfht\u00001+Wcfct\u00001+bf) (8)\nct=ftct\u00001+ittanh (Wxcxt+Whcht\u00001+bc) (9)\not=\u001b(Wxoxt+Whoht\u00001+Wcoct+bo) (10)\nht=ottanh(ct) (11)\nwhere\u001bis the logistic sigmoid function, and i,f,oandcare respectively the\ninput gate ,forget gate ,output gate ,celland cell input activation vectors, all of\nwhich are the same size as the hidden vector h. The weight matrix subscripts\nhave the obvious meaning, for example Whiis the hidden-input gate matrix,\nWxois the input-output gate matrix etc. The weight matrices from the cell\nto gate vectors (e.g. Wci) are diagonal, so element min each gate vector only\nreceives input from element mof the cell vector. The bias terms (which are\nadded toi,f,cando) have been omitted for clarity.\nThe original LSTM algorithm used a custom designed approximate gradi-\nent calculation that allowed the weights to be updated after every timestep [16].\nHowever the full gradient can instead be calculated with backpropagation through\ntime [11], the method used in this paper. One di\u000eculty when training LSTM\nwith the full gradient is that the derivatives sometimes become excessively large,\nleading to numerical problems. To prevent this, all the experiments in this pa-\nper clipped the derivative of the loss with respect to the network inputs to the\nLSTM layers (before the sigmoid and tanh functions are applied) to lie within\na prede\fned range1.", "title": "Long Short-Term Memory"}, {"color": "#00FF00", "font": "25px arial black", "id": 25, "ids": "3", "label": "3", "shape": "dot", "size": 10, "text": "Text data is discrete, and is typically presented to neural networks using `one-\nhot\u0027 input vectors. That is, if there are Ktext classes in total, and class kis fed\nin at timet, thenxtis a length Kvector whose entries are all zero except for\nthekth, which is one. Pr( xt+1jyt) is therefore a multinomial distribution, which\ncan be naturally parameterised by a softmax function at the output layer:\nPr(xt+1=kjyt) =yk\nt=exp\u0000\n^yk\nt\u0001\nPK\nk0=1exp\u0000\n^yk0\nt\u0001 (12)\nSubstituting into Eq. (6) we see that\nL(x) =\u0000TX\nt=1logyxt+1\nt (13)\n=)@L(x)\n@^yk\nt=yk\nt\u0000\u000ek;xt+1 (14)\nThe only thing that remains to be decided is which set of classes to use. In\nmost cases, text prediction (usually referred to as language modelling ) is per-\nformed at the word level. Kis therefore the number of words in the dictionary.\nThis can be problematic for realistic tasks, where the number of words (in-\ncluding variant conjugations, proper names, etc.) often exceeds 100,000. As\nwell as requiring many parameters to model, having so many classes demands a\nhuge amount of training data to adequately cover the possible contexts for the\nwords. In the case of softmax models, a further di\u000eculty is the high computa-\ntional cost of evaluating all the exponentials during training (although several\nmethods have been to devised make training large softmax layers more e\u000ecient,\nincluding tree-based models [25, 23], low rank approximations [27] and stochas-\ntic derivatives [26]). Furthermore, word-level models are not applicable to text\ndata containing non-word strings, such as multi-digit numbers or web addresses.\nCharacter-level language modelling with neural networks has recently been\nconsidered [30, 24], and found to give slightly worse performance than equiv-\nalent word-level models. Nonetheless, predicting one character at a time is\nmore interesting from the perspective of sequence generation, because it allows\nthe network to invent novel words and strings. In general, the experiments in\nthis paper aim to predict at the \fnest granularity found in the data, so as to\nmaximise the generative \rexibility of the network.\n1In fact this technique was used in all my previous papers on LSTM, and in my publicly\navailable LSTM code, but I forgot to mention it anywhere| mea culpa .", "title": "Text Prediction"}, {"color": "#00FF00", "font": "25px arial black", "id": 26, "ids": "3.1", "label": "3.1", "shape": "dot", "size": 10, "text": "The \frst set of text prediction experiments focused on the Penn Treebank por-\ntion of the Wall Street Journal corpus [22]. This was a preliminary study whose\nmain purpose was to gauge the predictive power of the network, rather than to\ngenerate interesting sequences.\nAlthough a relatively small text corpus (a little over a million words in total),\nthe Penn Treebank data is widely used as a language modelling benchmark. The\ntraining set contains 930,000 words, the validation set contains 74,000 words and\nthe test set contains 82,000 words. The vocabulary is limited to 10,000 words,\nwith all other words mapped to a special `unknown word\u0027 token. The end-of-\nsentence token was included in the input sequences, and was counted in the\nsequence loss. The start-of-sentence marker was ignored, because its role is\nalready ful\flled by the null vectors that begin the sequences (c.f. Section 2).\nThe experiments compared the performance of word and character-level\nLSTM predictors on the Penn corpus. In both cases, the network architecture\nwas a single hidden layer with 1000 LSTM units. For the character-level network\nthe input and output layers were size 49, giving approximately 4.3M weights in\ntotal, while the word-level network had 10,000 inputs and outputs and around\n54M weights. The comparison is therefore somewhat unfair, as the word-level\nnetwork had many more parameters. However, as the dataset is small, both net-\nworks were easily able to over\ft the training data, and it is not clear whether the\ncharacter-level network would have bene\fted from more weights. All networks\nwere trained with stochastic gradient descent, using a learn rate of 0.0001 and a\nmomentum of 0.99. The LSTM derivates were clipped in the range [ \u00001;1] (c.f.\nSection 2.1).\nNeural networks are usually evaluated on test data with \fxed weights. For\nprediction problems however, where the inputs arethe targets, it is legitimate\nto allow the network to adapt its weights as it is being evaluated (so long as\nit only sees the test data once). Mikolov refers to this as dynamic evaluation .\nDynamic evaluation allows for a fairer comparison with compression algorithms,\nfor which there is no division between training and test sets, as all data is only\npredicted once.\nSince both networks over\ft the training data, we also experiment with two\ntypes of regularisation: weight noise [18] with a std. deviation of 0.075 applied\nto the network weights at the start of each training sequence, and adaptive\nweight noise [8], where the variance of the noise is learned along with the weights\nusing a Minimum description Length (or equivalently, variational inference) loss\nfunction. When weight noise was used, the network was initialised with the\n\fnal weights of the unregularised network. Similarly, when adaptive weight\nnoise was used, the weights were initialised with those of the network trained\nwith weight noise. We have found that retraining with iteratively increased\nregularisation is considerably faster than training from random weights with\nregularisation. Adaptive weight noise was found to be prohibitively slow for\nthe word-level network, so it was regularised with \fxed-variance weight noise\nonly. One advantage of adaptive weight is that early stopping is not needed\nTable 1: Penn Treebank Test Set Results. `BPC\u0027 is bits-per-character.\n`Error\u0027 is next-step classi\fcation error rate, for either characters or words.\nInput Regularisation Dynamic BPC Perplexity Error (%) Epochs\nChar none no 1.32 167 28.5 9\nchar none yes 1.29 148 28.0 9\nchar weight noise no 1.27 140 27.4 25\nchar weight noise yes 1.24 124 26.9 25\nchar adapt. wt. noise no 1.26 133 27.4 26\nchar adapt. wt. noise yes 1.24 122 26.9 26\nword none no 1.27 138 77.8 11\nword none yes 1.25 126 76.9 11\nword weight noise no 1.25 126 76.9 14\nword weight noise yes 1.23 117 76.2 14\n(the network can safely be stopped at the point of minimum total `description\nlength\u0027 on the training data). However, to keep the comparison fair, the same\ntraining, validation and test sets were used for all experiments.\nThe results are presented with two equivalent metrics: bits-per-character\n(BPC), which is the average value of \u0000log2Pr(xt+1jyt) over the whole test set;\nandperplexity which is two to the power of the average number of bits per word\n(the average word length on the test set is about 5.6 characters, so perplexity \u0019\n25:6BPC). Perplexity is the usual performance measure for language modelling.\nTable 1 shows that the word-level RNN performed better than the character-\nlevel network, but the gap appeared to close when regularisation is used. Overall\nthe results compare favourably with those collected in Tomas Mikolov\u0027s the-\nsis [23]. For example, he records a perplexity of 141 for a 5-gram with Keyser-\nNey smoothing, 141.8 for a word level feedforward neural network, 131.1 for the\nstate-of-the-art compression algorithm PAQ8 and 123.2 for a dynamically eval-\nuated word-level RNN. However by combining multiple RNNs, a 5-gram and a\ncache model in an ensemble, he was able to achieve a perplexity of 89.4. Inter-\nestingly, the bene\ft of dynamic evaluation was far more pronounced here than\nin Mikolov\u0027s thesis (he records a perplexity improvement from 124.7 to 123.2\nwith word-level RNNs). This suggests that LSTM is better at rapidly adapting\nto new data than ordinary RNNs.", "title": "Penn Treebank Experiments"}, {"color": "#00FF00", "font": "25px arial black", "id": 27, "ids": "3.2", "label": "3.2", "shape": "dot", "size": 10, "text": "In 2006 Marcus Hutter, Jim Bowery and Matt Mahoney organised the following\nchallenge, commonly known as Hutter prize [17]: to compress the \frst 100\nmillion bytes of the complete English Wikipedia data (as it was at a certain\ntime on March 3rd 2006) to as small a \fle as possible. The \fle had to include\nnot only the compressed data, but also the code implementing the compression\nalgorithm. Its size can therefore be considered a measure of the minimum\ndescription length [13] of the data using a two part coding scheme.\nWikipedia data is interesting from a sequence generation perspective because\nit contains not only a huge range of dictionary words, but also many character\nsequences that would not be included in text corpora traditionally used for\nlanguage modelling. For example foreign words (including letters from non-\nLatin alphabets such as Arabic and Chinese), indented XML tags used to de\fne\nmeta-data, website addresses, and markup used to indicate page formatting such\nas headings, bullet points etc. An extract from the Hutter prize dataset is shown\nin Figs. 3 and 4.\nThe \frst 96M bytes in the data were evenly split into sequences of 100 bytes\nand used to train the network, with the remaining 4M were used for validation.\nThe data contains a total of 205 one-byte unicode symbols. The total number\nofcharacters is much higher, since many characters (especially those from non-\nLatin languages) are de\fned as multi-symbol sequences. In keeping with the\nprinciple of modelling the smallest meaningful units in the data, the network\npredicted a single byte at a time, and therefore had size 205 input and output\nlayers.\nWikipedia contains long-range regularities, such as the topic of an article,\nwhich can span many thousand words. To make it possible for the network to\ncapture these, its internal state (that is, the output activations htof the hidden\nlayers, and the activations ctof the LSTM cells within the layers) were only reset\nevery 100 sequences. Furthermore the order of the sequences was not shu\u000fed\nduring training, as it usually is for neural networks. The network was therefore\nable to access information from up to 10K characters in the past when making\npredictions. The error terms were only backpropagated to the start of each 100\nbyte sequence, meaning that the gradient calculation was approximate. This\nform of truncated backpropagation has been considered before for RNN lan-\nguage modelling [23], and found to speed up training (by reducing the sequence\nlength and hence increasing the frequency of stochastic weight updates) without\na\u000becting the network\u0027s ability to learn long-range dependencies.\nA much larger network was used for this data than the Penn data (re\recting\nthe greater size and complexity of the training set) with seven hidden layers of\n700 LSTM cells, giving approximately 21.3M weights. The network was trained\nwith stochastic gradient descent, using a learn rate of 0.0001 and a momentum\nof 0.9. It took four training epochs to converge. The LSTM derivates were\nclipped in the range [ \u00001;1].\nAs with the Penn data, we tested the network on the validation data with\nand without dynamic evaluation (where the weights are updated as the data\nis predicted). As can be seen from Table 2 performance was much better with\ndynamic evaluation. This is probably because of the long range coherence of\nWikipedia data; for example, certain words are much more frequent in some\narticles than others, and being able to adapt to this during evaluation is ad-\nvantageous. It may seem surprising that the dynamic results on the validation\nset were substantially better than on the training set. However this is easily\nexplained by two factors: \frstly, the network under\ft the training data, and\nsecondly some portions of the data are much more di\u000ecult than others (for\nexample, plain text is harder to predict than XML tags).\nTo put the results in context, the current winner of the Hutter Prize (a\nTable 2: Wikipedia Results (bits-per-character)\nTrain Validation (static) Validation (dynamic)\n1.42 1.67 1.33\nvariant of the PAQ-8 compression algorithm [20]) achieves 1.28 BPC on the same\ndata (including the code required to implement the algorithm), mainstream\ncompressors such as zip generally get more than 2, and a character level RNN\napplied to a text-only version of the data (i.e. with all the XML, markup tags\netc. removed) achieved 1.54 on held-out data, which improved to 1.47 when the\nRNN was combined with a maximum entropy model [24].\nA four page sample generated by the prediction network is shown in Figs. 5\nto 8. The sample shows that the network has learned a lot of structure from\nthe data, at a wide range of di\u000berent scales. Most obviously, it has learned a\nlarge vocabulary of dictionary words, along with a subword model that enables\nit to invent feasible-looking words and names: for example \\Lochroom River\",\n\\Mughal Ralvaldens\", \\submandration\", \\swalloped\". It has also learned basic\npunctuation, with commas, full stops and paragraph breaks occurring at roughly\nthe right rhythm in the text blocks.\nBeing able to correctly open and close quotation marks and parentheses is\na clear indicator of a language model\u0027s memory, because the closure cannot be\npredicted from the intervening text, and hence cannot be modelled with short-\nrange context [30]. The sample shows that the network is able to balance not\nonly parentheses and quotes, but also formatting marks such as the equals signs\nused to denote headings, and even nested XML tags and indentation.\nThe network generates non-Latin characters such as Cyrillic, Chinese and\nArabic, and seems to have learned a rudimentary model for languages other\nthan English (e.g. it generates \\es:Geotnia slago\" for the Spanish `version\u0027 of an\narticle, and \\nl:Rodenbaueri\" for the Dutch one) It also generates convincing\nlooking internet addresses (none of which appear to be real).\nThe network generates distinct, large-scale regions, such as XML headers,\nbullet-point lists and article text. Comparison with Figs. 3 and 4 suggests that\nthese regions are a fairly accurate re\rection of the constitution of the real data\n(although the generated versions tend to be somewhat shorter and more jumbled\ntogether). This is signi\fcant because each region may span hundreds or even\nthousands of timesteps. The fact that the network is able to remain coherent\nover such large intervals (even putting the regions in an approximately correct\norder, such as having headers at the start of articles and bullet-pointed `see also\u0027\nlists at the end) is testament to its long-range memory.\nAs with all text generated by language models, the sample does not make\nsense beyond the level of short phrases. The realism could perhaps be improved\nwith a larger network and/or more data. However, it seems futile to expect\nmeaningful language from a machine that has never been exposed to the sensory\nworld to which language refers.\nLastly, the network\u0027s adaptation to recent sequences during training (which\nallows it to bene\ft from dynamic evaluation) can be clearly observed in the\nextract. The last complete article before the end of the training set (at which\npoint the weights were stored) was on intercontinental ballistic missiles. The\nin\ruence of this article on the network\u0027s language model can be seen from the\nprofusion of missile-related terms. Other recent topics include `Individual An-\narchism\u0027, the Italian writer Italo Calvino and the International Organization\nfor Standardization (ISO), all of which make themselves felt in the network\u0027s\nvocabulary.\n_(moon)|Enceladus]], a moon oFigure 3: Real Wikipedia data\nFigure 4: Real Wikipedia data (cotd.)\nFigure 5: Generated Wikipedia data.\n\u0101Figure 6: Generated Wikipedia data (cotd.)\n\u3925\u0005\u35c1\u372a\u048d\u0377\r\u152b]]                                                                 \u0d07\u3646\u081b\u3b99\u09bc \u0a2c\u0a4c]]                                                                 \u041c\u043b\u043a\u0440\u0430\u043a\u044f\u043d\u0473\u0435\u043b\u043e\u043b\u05db\u0443\u0446\u0438\u044f\u043d\u0441\u044c\u043d\u0438\u044f \u0430\u0433\u043c\u043e\u0440\u0435\u043b\u0438\u0430]]                                       \u014d]]                                                 \u0011\t\f\u0002\b\u0017\u0007]]\u003c/text\u003e                                                           Figure 7: Generated Wikipedia data (cotd.)\n\u0012 submarine\u0026quot; is [[building|corruptable]],Figure 8: Generated Wikipedia data (cotd.)", "title": "Wikipedia Experiments"}, {"color": "#00FF00", "font": "25px arial black", "id": 28, "ids": "4", "label": "4", "shape": "dot", "size": 10, "text": "To test whether the prediction network could also be used to generate convincing\nreal-valued sequences, we applied it to online handwriting data ( online in this\ncontext means that the writing is recorded as a sequence of pen-tip locations,\nas opposed to o\u000fine handwriting, where only the page images are available).\nOnline handwriting is an attractive choice for sequence generation due to its\nlow dimensionality (two real numbers per data point) and ease of visualisation.\nAll the data used for this paper were taken from the IAM online handwriting\ndatabase (IAM-OnDB) [21]. IAM-OnDB consists of handwritten lines collected\nfrom 221 di\u000berent writers using a `smart whiteboard\u0027. The writers were asked to\nwrite forms from the Lancaster-Oslo-Bergen text corpus [19], and the position\nof their pen was tracked using an infra-red device in the corner of the board.\nSamples from the training data are shown in Fig. 9. The original input data\nconsists of the xandypen co-ordinates and the points in the sequence when\nthe pen is lifted o\u000b the whiteboard. Recording errors in the x;ydata was\ncorrected by interpolating to \fll in for missing readings, and removing steps\nwhose length exceeded a certain threshold. Beyond that, no preprocessing was\nused and the network was trained to predict the x;yco-ordinates and the end-\nof-stroke markers one point at a time. This contrasts with most approaches to\nhandwriting recognition and synthesis, which rely on sophisticated preprocessing\nand feature-extraction techniques. We eschewed such techniques because they\ntend to reduce the variation in the data (e.g. by normalising the character size,\nslant, skew and so-on) which we wanted the network to model. Predicting the\npen traces one point at a time gives the network maximum \rexibility to invent\nnovel handwriting, but also requires a lot of memory, with the average letter\noccupying more than 25 timesteps and the average line occupying around 700.\nPredicting delayed strokes (such as dots for `i\u0027s or crosses for `t\u0027s that are added\nafter the rest of the word has been written) is especially demanding.\nIAM-OnDB is divided into a training set, two validation sets and a test\nset, containing respectively 5364, 1438, 1518 and 3859 handwritten lines taken\nfrom 775, 192, 216 and 544 forms. For our experiments, each line was treated\nas a separate sequence (meaning that possible dependencies between successive\nlines were ignored). In order to maximise the amount of training data, we used\nthe training set, test set and the larger of the validation sets for training and\nthe smaller validation set for early-stopping. The lack of independent test set\nmeans that the recorded results may be somewhat over\ft on the validation set;\nhowever the validation results are of secondary importance, since no benchmark\nresults exist and the main goal was to generate convincing-looking handwriting.\nThe principal challenge in applying the prediction network to online hand-\nwriting data was determining a predictive distribution suitable for real-valued\ninputs. The following section describes how this was done.\nFigure 9: Training samples from the IAM online handwriting database.\nNotice the wide range of writing styles, the variation in line angle and character\nsizes, and the writing and recording errors, such as the scribbled out letters in\nthe \frst line and the repeated word in the \fnal line.", "title": "Handwriting Prediction"}, {"color": "#00FF00", "font": "25px arial black", "id": 29, "ids": "4.1", "label": "4.1", "shape": "dot", "size": 10, "text": "The idea of mixture density networks [2, 3] is to use the outputs of a neural\nnetwork to parameterise a mixture distribution. A subset of the outputs are\nused to de\fne the mixture weights, while the remaining outputs are used to\nparameterise the individual mixture components. The mixture weight outputs\nare normalised with a softmax function to ensure they form a valid discrete dis-\ntribution, and the other outputs are passed through suitable functions to keep\ntheir values within meaningful range (for example the exponential function is\ntypically applied to outputs used as scale parameters, which must be positive).\nMixture density network are trained by maximising the log probability den-\nsity of the targets under the induced distributions. Note that the densities are\nnormalised (up to a \fxed constant) and are therefore straightforward to di\u000ber-\nentiate and pick unbiased sample from, in contrast with restricted Boltzmann\nmachines [14] and other undirected models.\nMixture density outputs can also be used with recurrent neural networks [28].\nIn this case the output distribution is conditioned not only on the current input,\nbut on the history of previous inputs. Intuitively, the number of components is\nthe number of choices the network has for the next output given the inputs so\nfar.\nFor the handwriting experiments in this paper, the basic RNN architecture\nand update equations remain unchanged from Section 2. Each input vector xt\nconsists of a real-valued pair x1;x2that de\fnes the pen o\u000bset from the previous\ninput, along with a binary x3that has value 1 if the vector ends a stroke (that\nis, if the pen was lifted o\u000b the board before the next vector was recorded) and\nvalue 0 otherwise. A mixture of bivariate Gaussians was used to predict x1\nandx2, while a Bernoulli distribution was used for x3. Each output vector yt\ntherefore consists of the end of stroke probability e, along with a set of means\n\u0016j, standard deviations \u001bj, correlations \u001ajand mixture weights \u0019jfor theM\nmixture components. That is\nxt2R\u0002R\u0002f0;1g (15)\nyt=\u0010\net;f\u0019j\nt;\u0016j\nt;\u001bj\nt;\u001aj\ntgM\nj=1\u0011\n(16)\nNote that the mean and standard deviation are two dimensional vectors, whereas\nthe component weight, correlation and end-of-stroke probability are scalar. The\nvectorsytare obtained from the network outputs ^ yt, where\n^yt=\u0010\n^et;f^wj\nt;^\u0016j\nt;^\u001bj\nt;^\u001aj\ntgM\nj=1\u0011\n=by+NX\nn=1Whnyhn\nt (17)\nas follows:\net=1\n1 + exp (^et)=)et2(0;1) (18)\n\u0019j\nt=exp\u0010\n^\u0019j\nt\u0011\nPM\nj0=1exp\u0010\n^\u0019j0\nt\u0011 =)\u0019j\nt2(0;1);X\nj\u0019j\nt= 1 (19)\n\u0016j\nt= ^\u0016j\nt =)\u0016j\nt2R (20)\n\u001bj\nt= exp\u0010\n^\u001bj\nt\u0011\n=)\u001bj\nt\u003e0 (21)\n\u001aj\nt=tanh(^\u001aj\nt) = )\u001aj\nt2(\u00001;1) (22)\nThe probability density Pr( xt+1jyt) of the next input xt+1given the output\nvectorytis de\fned as follows:\nPr(xt+1jyt) =MX\nj=1\u0019j\ntN(xt+1j\u0016j\nt;\u001bj\nt;\u001aj\nt)(\net if (xt+1)3= 1\n1\u0000etotherwise(23)\nwhere\nN(xj\u0016;\u001b;\u001a ) =1\n2\u0019\u001b1\u001b2p\n1\u0000\u001a2exp\u0014\u0000Z\n2(1\u0000\u001a2)\u0015\n(24)\nwith\nZ=(x1\u0000\u00161)2\n\u001b2\n1+(x2\u0000\u00162)2\n\u001b2\n2\u00002\u001a(x1\u0000\u00161)(x2\u0000\u00162)\n\u001b1\u001b2(25)\nThis can be substituted into Eq. (6) to determine the sequence loss (up to\na constant that depends only on the quantisation of the data and does not\nin\ruence network training):\nL(x) =TX\nt=1\u0000log0\n@X\nj\u0019j\ntN(xt+1j\u0016j\nt;\u001bj\nt;\u001aj\nt)1\nA\u0000(\nloget if (xt+1)3= 1\nlog(1\u0000et) otherwise\n(26)\nThe derivative of the loss with respect to the end-of-stroke outputs is straight-\nforward:\n@L(x)\n@^et= (xt+1)3\u0000et (27)\nThe derivatives with respect to the mixture density outputs can be found by\n\frst de\fning the component responsibilities \rj\nt:\n^\rj\nt=\u0019j\ntN(xt+1j\u0016j\nt;\u001bj\nt;\u001aj\nt) (28)\n\rj\nt=^\rj\ntPM\nj0=1^\rj0\nt(29)\nThen observing that\n@L(x)\n@^\u0019j\nt=\u0019j\nt\u0000\rj\nt (30)\n@L(x)\n@(^\u0016j\nt;^\u001bj\nt;^\u001aj\nt)=\u0000\rj\nt@logN(xt+1j\u0016j\nt;\u001bj\nt;\u001aj\nt)\n@(^\u0016j\nt;^\u001bj\nt;^\u001aj\nt)(31)\nwhere\n@logN(xj\u0016;\u001b;\u001a )\n@^\u00161=C\n\u001b1\u0012x1\u0000\u00161\n\u001b1\u0000\u001a(x2\u0000\u00162)\n\u001b2\u0013\n(32)\n@logN(xj\u0016;\u001b;\u001a )\n@^\u00162=C\n\u001b2\u0012x2\u0000\u00162\n\u001b2\u0000\u001a(x1\u0000\u00161)\n\u001b1\u0013\n(33)\n@logN(xj\u0016;\u001b;\u001a )\n@^\u001b1=C(x1\u0000\u00161)\n\u001b1\u0012x1\u0000\u00161\n\u001b1\u0000\u001a(x2\u0000\u00162)\n\u001b2\u0013\n\u00001 (34)\n@logN(xj\u0016;\u001b;\u001a )\n@^\u001b2=C(x2\u0000\u00162)\n\u001b2\u0012x2\u0000\u00162\n\u001b2\u0000\u001a(x1\u0000\u00161)\n\u001b1\u0013\n\u00001 (35)\n@logN(xj\u0016;\u001b;\u001a )\n@^\u001a=(x1\u0000\u00161)(x2\u0000\u00162)\n\u001b1\u001b2+\u001a(1\u0000CZ) (36)\nwithZde\fned as in Eq. (25) and\nC=1\n1\u0000\u001a2(37)\nFig. 10 illustrates the operation of a mixture density output layer applied to\nonline handwriting prediction.\nOutput Density\nFigure 10: Mixture density outputs for handwriting prediction. The\ntop heatmap shows the sequence of probability distributions for the predicted\npen locations as the word `under\u0027 is written. The densities for successive\npredictions are added together, giving high values where the distributions\noverlap.\nTwo types of prediction are visible from the density map: the small\nblobs that spell out the letters are the predictions as the strokes are being\nwritten, the three large blobs are the predictions at the ends of the strokes for\nthe \frst point in the next stroke. The end-of-stroke predictions have much\nhigher variance because the pen position was not recorded when it was o\u000b the\nwhiteboard, and hence there may be a large distance between the end of one\nstroke and the start of the next.\nThe bottom heatmap shows the mixture component weights during the\nsame sequence. The stroke ends are also visible here, with the most active\ncomponents switching o\u000b in three places, and other components switching on:\nevidently end-of-stroke predictions use a di\u000berent set of mixture components\nfrom in-stroke predictions.", "title": "Mixture Density Outputs"}, {"color": "#00FF00", "font": "25px arial black", "id": 30, "ids": "4.2", "label": "4.2", "shape": "dot", "size": 10, "text": "Each point in the data sequences consisted of three numbers: the xandyo\u000bset\nfrom the previous point, and the binary end-of-stroke feature. The network\ninput layer was therefore size 3. The co-ordinate o\u000bsets were normalised to\nmean 0, std. dev. 1 over the training set. 20 mixture components were used\nto model the o\u000bsets, giving a total of 120 mixture parameters per timestep\n(20 weights, 40 means, 40 standard deviations and 20 correlations). A further\nparameter was used to model the end-of-stroke probability, giving an output\nlayer of size 121. Two network architectures were compared for the hidden\nlayers: one with three hidden layers, each consisting of 400 LSTM cells, and one\nwith a single hidden layer of 900 LSTM cells. Both networks had around 3.4M\nweights. The three layer network was retrained with adaptive weight noise [8],\nwith all std. devs. initialised to 0.075. Training with \fxed variance weight noise\nproved ine\u000bective, probably because it prevented the mixture density layer from\nusing precisely speci\fed weights.\nThe networks were trained with rmsprop , a form of stochastic gradient de-\nscent where the gradients are divided by a running average of their recent mag-\nnitude [32]. De\fne \u000fi=@L(x)\n@wiwherewiis network weight i. The weight update\nequations were:\nni=@ni+ (1\u0000@)\u000f2\ni (38)\ngi=@gi+ (1\u0000@)\u000fi (39)\n\u0001i=i\u0001i\u0000j\u000fip\nni\u0000g2\ni+k(40)\nwi=wi+ \u0001 i (41)\nwith the following parameters:\n@= 0:95 (42)\ni= 0:9 (43)\nj= 0:0001 (44)\nk= 0:0001 (45)\nThe output derivatives@L(x)\n@^ytwere clipped in the range [ \u0000100;100], and the\nLSTM derivates were clipped in the range [ \u000010;10]. Clipping the output gradi-\nents proved vital for numerical stability; even so, the networks sometimes had\nnumerical problems late on in training, after they had started over\ftting on the\ntraining data.\nTable 3 shows that the three layer network had an average per-sequence loss\n15.3 nats lower than the one layer net. However the sum-squared-error was\nslightly lower for the single layer network. the use of adaptive weight noise\nreduced the loss by another 16.7 nats relative to the unregularised three layer\nnetwork, but did not signi\fcantly change the sum-squared error. The adaptive\nweight noise network appeared to generate the best samples.\nTable 3: Handwriting Prediction Results. All results recorded on the val-\nidation set. `Log-Loss\u0027 is the mean value of L(x) (in nats). `SSE\u0027 is the mean\nsum-squared-error per data point.\nNetwork Regularisation Log-Loss SSE\n1 layer none -1025.7 0.40\n3 layer none -1041.0 0.41\n3 layer adaptive weight noise -1057.7 0.41", "title": "Experiments"}, {"color": "#00FF00", "font": "25px arial black", "id": 31, "ids": "4.3", "label": "4.3", "shape": "dot", "size": 10, "text": "Fig. 11 shows handwriting samples generated by the prediction network. The\nnetwork has clearly learned to model strokes, letters and even short words (es-\npecially common ones such as `of\u0027 and `the\u0027). It also appears to have learned a\nbasic character level language models, since the words it invents (`eald\u0027, `bryoes\u0027,\n`lenrest\u0027) look somewhat plausible in English. Given that the average character\noccupies more than 25 timesteps, this again demonstrates the network\u0027s ability\nto generate coherent long-range structures.", "title": "Samples"}, {"color": "#00FF00", "font": "25px arial black", "id": 32, "ids": "5", "label": "5", "shape": "dot", "size": 10, "text": "Handwriting synthesis is the generation of handwriting for a given text. Clearly\nthe prediction networks we have described so far are unable to do this, since\nthere is no way to constrain which letters the network writes. This section de-\nscribes an augmentation that allows a prediction network to generate data se-\nquences conditioned on some high-level annotation sequence (a character string,\nin the case of handwriting synthesis). The resulting sequences are su\u000eciently\nconvincing that they often cannot be distinguished from real handwriting. Fur-\nthermore, this realism is achieved without sacri\fcing the diversity in writing\nstyle demonstrated in the previous section.\nThe main challenge in conditioning the predictions on the text is that the two\nsequences are of very di\u000berent lengths (the pen trace being on average twenty\n\fve times as long as the text), and the alignment between them is unknown until\nthe data is generated. This is because the number of co-ordinates used to write\neach character varies greatly according to style, size, pen speed etc. One neural\nnetwork model able to make sequential predictions based on two sequences of\ndi\u000berent length and unknown alignment is the RNN transducer [9]. However\npreliminary experiments on handwriting synthesis with RNN transducers were\nnot encouraging. A possible explanation is that the transducer uses two sepa-\nrate RNNs to process the two sequences, then combines their outputs to make\ndecisions, when it is usually more desirable to make all the information avail-\nable to single network. This work proposes an alternative model, where a `soft\nwindow\u0027 is convolved with the text string and fed in as an extra input to the\nprediction network. The parameters of the window are output by the network\nFigure 11: Online handwriting samples generated by the prediction\nnetwork. All samples are 700 timesteps long.\nat the same time as it makes the predictions, so that it dynamically determines\nan alignment between the text and the pen locations. Put simply, it learns to\ndecide which character to write next.", "title": "Handwriting Synthesis"}, {"color": "#00FF00", "font": "25px arial black", "id": 33, "ids": "5.1", "label": "5.1", "shape": "dot", "size": 10, "text": "Fig. 12 illustrates the network architecture used for handwriting synthesis. As\nwith the prediction network, the hidden layers are stacked on top of each other,\neach feeding up to the layer above, and there are skip connections from the\ninputs to all hidden layers and from all hidden layers to the outputs. The\ndi\u000berence is the added input from the character sequence, mediated by the\nwindow layer.\nGiven a length Ucharacter sequence cand a length Tdata sequence x, the\nsoft window wtintocat timestep t(1\u0014t\u0014T) is de\fned by the following\ndiscrete convolution with a mixture of KGaussian functions\n\u001e(t;u) =KX\nk=1\u000bk\ntexp\u0010\n\u0000\fk\nt\u0000\n\u0014k\nt\u0000u\u00012\u0011\n(46)\nwt=UX\nu=1\u001e(t;u)cu (47)\nwhere\u001e(t;u) is the window weight ofcuat timestep t. Intuitively, the \u0014tparam-\neters control the location of the window, the \ftparameters control the width of\nthe window and the \u000btparameters control the importance of the window within\nthe mixture. The size of the soft window vectors is the same as the size of the\ncharacter vectors cu(assuming a one-hot encoding, this will be the number of\ncharacters in the alphabet). Note that the window mixture is not normalised\nand hence does not determine a probability distribution; however the window\nweight\u001e(t;u) can be loosely interpreted as the network\u0027s belief that it is writ-\ning character cuat timet. Fig. 13 shows the alignment implied by the window\nweights during a training sequence.\nThe size 3Kvectorpof window parameters is determined as follows by the\noutputs of the \frst hidden layer of the network:\n(^\u000bt;^\ft;^\u0014t) =Wh1ph1\nt+bp (48)\n\u000bt= exp (^\u000bt) (49)\n\ft= exp\u0010\n^\ft\u0011\n(50)\n\u0014t=\u0014t\u00001+ exp (^\u0014t) (51)\nNote that the location parameters \u0014tare de\fned as o\u000bsets from the previous\nlocationsct\u00001, and that the size of the o\u000bset is constrained to be greater than\nzero. Intuitively, this means that network learns how far to slide each window\nat each step, rather than an absolute location. Using o\u000bsets was essential to\ngetting the network to align the text with the pen trace.\nFigure 12: Synthesis Network Architecture Circles represent layers, solid\nlines represent connections and dashed lines represent predictions. The topology\nis similar to the prediction network in Fig. 1, except that extra input from the\ncharacter sequence c, is presented to the hidden layers via the window layer\n(with a delay in the connection to the \frst hidden layer to avoid a cycle in the\ngraph).\nFigure 13: Window weights during a handwriting synthesis sequence\nEach point on the map shows the value of \u001e(t;u), wheretindexes the pen trace\nalong the horizontal axis and uindexes the text character along the vertical axis.\nThe bright line is the alignment chosen by the network between the characters\nand the writing. Notice that the line spreads out at the boundaries between\ncharacters; this means the network receives information about next and previous\nletters as it makes transitions, which helps guide its predictions.\nThewtvectors are passed to the second and third hidden layers at time t,\nand the \frst hidden layer at time t+1 (to avoid creating a cycle in the processing\ngraph). The update equations for the hidden layers are\nh1\nt=H\u0000\nWih1xt+Wh1h1h1\nt\u00001+Wwh1wt\u00001+b1\nh\u0001\n(52)\nhn\nt=H\u0000\nWihnxt+Whn\u00001hnhn\u00001\nt+Whnhnhn\nt\u00001+Wwhnwt+bn\nh\u0001\n(53)\nThe equations for the output layer remain unchanged from Eqs. (17) to (22).\nThe sequence loss is\nL(x) =\u0000log Pr( xjc) (54)\nwhere\nPr(xjc) =TY\nt=1Pr (xt+1jyt) (55)\nNote thatytis now a function of cas well as x1:t.\nThe loss derivatives with respect to the outputs ^ et;^\u0019t;^\u0016t;^\u001bt;^\u001atremain un-\nchanged from Eqs. (27), (30) and (31). Given the loss derivative@L(x)\n@wtwith\nrespect to the size Wwindow vector wt, obtained by backpropagating the out-\nput derivatives through the computation graph in Fig. 12, the derivatives with\nrespect to the window parameters are as follows:\n\u000f(k;t;u )def=\u000bk\ntexp\u0010\n\u0000\fk\nt\u0000\n\u0014k\nt\u0000u\u00012\u0011WX\nj=1@L(x)\n@wj\ntcj\nu (56)\n@L(x)\n@^\u000bk\nt=UX\nu=1\u000f(k;t;u ) (57)\n@L(x)\n@^\fk\nt=\u0000\fk\ntUX\nu=1\u000f(k;t;u )(\u0014k\nt\u0000u)2(58)\n@L(x)\n@\u0014k\nt=@L(x)\n@\u0014k\nt+1+ 2\fk\ntUX\nu=1\u000f(k;t;u )(u\u0000\u0014k\nt) (59)\n@L(x)\n@^\u0014k\nt= exp\u0000\n^\u0014k\nt\u0001@L(x)\n@\u0014k\nt(60)\nFig. 14 illustrates the operation of a mixture density output layer applied to\nhandwriting synthesis.", "title": "Synthesis Network"}, {"color": "#00FF00", "font": "25px arial black", "id": 34, "ids": "5.2", "label": "5.2", "shape": "dot", "size": 10, "text": "The synthesis network was applied to the same input data as the handwriting\nprediction network in the previous section. The character-level transcriptions\nfrom the IAM-OnDB were now used to de\fne the character sequences c. The full\ntranscriptions contain 80 distinct characters (capital letters, lower case letters,\ndigits, and punctuation). However we used only a subset of 57, with all the\nSynthesis Output Density\nFigure 14: Mixture density outputs for handwriting synthesis. The top\nheatmap shows the predictive distributions for the pen locations, the bottom\nheatmap shows the mixture component weights. Comparison with Fig. 10 indi-\ncates that the synthesis network makes more precise predictions (with smaller\ndensity blobs) than the prediction-only network, especially at the ends of strokes,\nwhere the synthesis network has the advantage of knowing which letter comes\nnext.\nTable 4: Handwriting Synthesis Results. All results recorded on the val-\nidation set. `Log-Loss\u0027 is the mean value of L(x) in nats. `SSE\u0027 is the mean\nsum-squared-error per data point.\nRegularisation Log-Loss SSE\nnone -1096.9 0.23\nadaptive weight noise -1128.2 0.23\ndigits and most of the punctuation characters replaced with a generic `non-\nletter\u0027 label2.\nThe network architecture was as similar as possible to the best prediction\nnetwork: three hidden layers of 400 LSTM cells each, 20 bivariate Gaussian\nmixture components at the output layer and a size 3 input layer. The character\nsequence was encoded with one-hot vectors, and hence the window vectors were\nsize 57. A mixture of 10 Gaussian functions was used for the window parameters,\nrequiring a size 30 parameter vector. The total number of weights was increased\nto approximately 3.7M.\nThe network was trained with rmsprop, using the same parameters as in\nthe previous section. The network was retrained with adaptive weight noise,\ninitial standard deviation 0.075, and the output and LSTM gradients were again\nclipped in the range [ \u0000100;100] and [\u000010;10] respectively.\nTable 4 shows that adaptive weight noise gave a considerable improvement\nin log-loss (around 31.3 nats) but no signi\fcant change in sum-squared error.\nThe regularised network appears to generate slightly more realistic sequences,\nalthough the di\u000berence is hard to discern by eye. Both networks performed\nconsiderably better than the best prediction network. In particular the sum-\nsquared-error was reduced by 44%. This is likely due in large part to the im-\nproved predictions at the ends of strokes, where the error is largest.", "title": "Experiments"}, {"color": "#00FF00", "font": "25px arial black", "id": 37, "ids": "5.5", "label": "5.5", "shape": "dot", "size": 10, "text": "Another reason to constrain the sampling would be to generate handwriting\nin the style of a particular writer (rather than in a randomly selected style).\nThe easiest way to do this would be to retrain it on that writer only. But\neven without retraining, it is possible to mimic a particular style by `priming\u0027\nthe network with a real sequence, then generating an extension with the real\nsequence still in the network\u0027s memory. This can be achieved for a real x,cand\na synthesis character string sby setting the character sequence to c0=c+s\nand clamping the data inputs to xfor the \frst Ttimesteps, then sampling\nas usual until the sequence ends. Examples of primed samples are shown in\nFigs. 18 and 19. The fact that priming works proves that the network is able to\nremember stylistic features identi\fed earlier on in the sequence. This technique\nappears to work better for sequences in the training data than those the network\nhas never seen.\nPrimed sampling and reduced variance sampling can also be combined. As\nshown in Figs. 20 and 21 this tends to produce samples in a `cleaned up\u0027 version\nof the priming style, with overall stylistic traits such as slant and cursiveness\nretained, but the strokes appearing smoother and more regular. A possible\napplication would be the arti\fcial enhancement of poor handwriting.", "title": "Primed Sampling"}]);
                  edges = new vis.DataSet([{"from": 0, "to": 1, "width": 1}, {"from": 0, "to": 2, "width": 1}, {"from": 0, "to": 20, "width": 1}, {"from": 0, "to": 21, "width": 1}, {"from": 1, "to": 7, "width": 1}, {"from": 1, "to": 11, "width": 1}, {"from": 1, "to": 20, "width": 1}, {"from": 2, "to": 21, "width": 1}, {"from": 3, "to": 8, "width": 1}, {"from": 4, "to": 5, "width": 1}, {"from": 5, "to": 6, "width": 1}, {"from": 16, "to": 17, "width": 1}, {"from": 17, "to": 21, "width": 1}, {"from": 20, "to": 21, "width": 1}, {"from": 22, "to": 38, "width": 1}, {"from": 22, "to": 39, "width": 1}, {"from": 35, "to": 36, "width": 1}, {"from": 38, "to": 39, "width": 1}]);

                  nodeColors = {};
                  allNodes = nodes.get({ returnType: "Object" });
                  for (nodeId in allNodes) {
                    nodeColors[nodeId] = allNodes[nodeId].color;
                  }
                  allEdges = edges.get({ returnType: "Object" });
                  // adding nodes and edges to the graph
                  data = {nodes: nodes, edges: edges};

                  var options = {
    "configure": {
        "enabled": false
    },
    "edges": {
        "color": {
            "inherit": true
        },
        "smooth": {
            "enabled": true,
            "type": "dynamic"
        }
    },
    "interaction": {
        "dragNodes": true,
        "hideEdgesOnDrag": false,
        "hideNodesOnDrag": false
    },
    "physics": {
        "enabled": true,
        "stabilization": {
            "enabled": true,
            "fit": true,
            "iterations": 1000,
            "onlyDynamicEdges": false,
            "updateInterval": 50
        }
    }
};

                  


                  

                  network = new vis.Network(container, data, options);

                  

                  

                  


                  

                  return network;

              }
              drawGraph();
        </script>
    </body>
</html>