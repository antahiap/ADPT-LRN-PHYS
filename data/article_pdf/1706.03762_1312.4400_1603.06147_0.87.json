{
    "(0, 1)": {
        "diff": "paper\t importance weight\t the important aspect\n0\t1\tComplex recurrent and convolutional neural networks are used in the dominant sequence transduction models.\n0\t1\tTransformer is a new network architecture based solely on attention mechanisms and without recurrence and convolutions.\n0\t1\tTransformer models are superior in quality, more parallelizable, and require less time to train compared to other models.\n0\t0.5\tTransformer achieves 28.4 BLEU on the WMT 2014 English-to-German translation task and 41.8 BLEU on the English-to-French task.\n1\t1\tRecurrent neural networks (RNNs) and LSTM models are commonly used in sequence modeling and transduction problems.\n1\t1\tRNNs factor computation along the symbol positions of input and output sequences and generate hidden states based on previous ones.\n1\t1\tAttention mechanisms are used in conjunction with recurrent networks in most cases.\n1\t0.5\tThe Transformer architecture relies entirely on an attention mechanism and not on recurrence.\n1\t0.5\tThe Transformer allows for more parallelization and can achieve state of the art translation quality after being trained for as little as twelve hours on eight GPUs.",
        "sim": "dependent paper\tsource paper\timportance weight\tcontent of the relation\n0\t1\t1\tencoder-decoder architectures\n0\t1\t1\tmodel architecture\n0\t1\t1\tattention mechanisms\n0\t1\t1\tglobal dependencies\n0\t1\t0.5\tsequence transduction models"
    },
    "(0, 2)": {
        "diff": "paper\timportance weight\tthe important aspect\n0\t1\tencoder-decoder through attention mechanism\n0\t1\tmore parallelizable and requires less time to train\n0\t1\tachieves high BLEU scores on translation tasks\n0\t1\tgeneralizes well to other tasks like English constituency parsing\n2\t1\treduces sequential computation\n2\t1\tuses convolutional neural networks as basic building blocks\n2\t1\tintroduces self-attention mechanism\n2\t1\tadvantages over models using RNNs or convolution",
        "sim": "dependent paper\tsource paper\timportance weight\tcontent of the relation\n2\t0\t1\tSimilar network architecture based on attention mechanisms\n2\t0\t0.7\tSuperior quality and parallelizability compared to recurrent and convolutional models\n2\t0\t0.6\tImproved BLEU score on machine translation tasks\n2\t0\t0.8\tLess training time and cost compared to existing models\n2\t0\t0.9\tGeneralizes well to other tasks including constituency parsing"
    },
    "(0, 11)": {
        "diff": "paper\t importance weight\t the important aspect\n0\t1\tattention mechanisms, no recurrence or convolutions\n0\t0.9\tachieving superior quality and parallelizability\n0\t0.8\timproved BLEU scores on machine translation tasks\n0\t0.7\tgeneralizability to other tasks like English constituency parsing\n11\t1\tauto-regressive model architecture\n11\t0.9\tstacked self-attention and point-wise, fully connected layers\n11\t0.8\tencoder-decoder structure\n11\t0.7\tdetailed model architecture in Figure 1",
        "sim": "dependent paper\tsource paper\timportance weight\tcontent of the relation\n11\t0\t0.8\tbased on\tcomplex recurrent or convolutional neural networks\n11\t0\t0.7\tbest performing models\tconnect the encoder and decoder through an attention mechanism\n11\t0\t0.6\tproposed\ta new simple network architecture, the Transformer\n11\t0\t0.8\tbased solely on\tattention mechanisms\n11\t0\t0.7\trequire\tsignificantly less time to train"
    },
    "(0, 16)": {
        "diff": "paper\t importance weight\t the important aspect\n0\t1\tTansformer architecture based on attention mechanisms\n0\t1\tBetter quality and parallelizability compared to existing models\n0\t1\tSuperior performance on machine translation tasks\n0\t1\tGeneralization to other tasks, such as English constituency parsing\n16\t0.5\tRegularization techniques used during training\n16\t0.5\tComparison of BLEU scores with previous state-of-the-art models\n16\t0.5\tDropout and label smoothing techniques applied\n16\t0.5\tImproved accuracy and BLEU score with label smoothing",
        "sim": " dependent paper\t source paper\t importance weight\t content of the relation\n0\t16\t0.7\tTransformer achieves better BLEU scores\n0\t16\t0.6\tTransformer training cost is lower\n0\t16\t0.8\tRegularization techniques used during training\n0\t16\t0.7\tDropout applied to sub-layers and embeddings\n0\t16\t0.8\tLabel smoothing improves accuracy and BLEU score"
    },
    "(0, 18)": {
        "diff": "paper\timportance weight\tthe important aspect\n0\t1\tThe Transformer model architecture\n0\t0.9\tNo recurrence or convolutions\n0\t0.7\tParallelizable and faster training\n0\t0.6\tSuperior quality in machine translation tasks, achieving higher BLEU scores\n18\t1\tThe big Transformer model outperforms previous models in translation tasks\n18\t0.9\tSurpasses all previously published models and ensembles\n18\t0.8\tFraction of the training cost compared to competitive models\n18\t0.7\tDifferent hyperparameters used in training, such as dropout rate and beam size",
        "sim": "dependent paper\tsource paper\timportance weight\tcontent of the relation\n0\t18\t0.85\tTransformers outperforms previous models\n18\t0\t0.85\tTransformers achieve superior performance\n0\t18\t0.8\tTransformer has parallelizable architecture\n18\t0\t0.8\tTransformer requires less training time\n0\t18\t0.75\tTransformer achieves high BLEU score"
    },
    "(0, 19)": {
        "diff": "paper\timportance weight\tthe important aspect\n0\t0.5\tsimple network architecture, based on attention mechanisms, no recurrence or convolutions\n0\t0.7\tachieves higher quality, more parallelizable, requires less training time\n0\t0.9\tachieves higher BLEU score on English-to-German and English-to-French translation tasks\n0\t1.0\tgeneralizes well to other tasks, successful on English constituency parsing tasks\n\n19\t0.5\tvaried base model to evaluate importance of different components of Transformer\n19\t0.7\tvariations in attention heads and key dimensions affect model quality\n19\t0.9\treducing attention key size hurts model quality, suggests need for sophisticated compatibility function\n19\t1.0\tbigger models and dropout are beneficial for improved performance, learned positional embeddings have similar results to sinusoidal positional encoding",
        "sim": "dependent paper\tsource paper\timportance weight\tcontent of the relation\n19\t0\t1\tDescription of the Transformer architecture\n19\t0\t1\tComparison of different models variations\n19\t0\t1\tEvaluating the importance of Transformer components\n19\t0\t1\tEffect of varying attention parameters on model quality\n19\t0\t1\tImportance of dropout in avoiding overfitting"
    },
    "(0, 20)": {
        "diff": "paper\timportance weight\tthe important aspect\n0\t1\tthe Transformer architecture\n0\t0.8\tsuperior quality in machine translation\n0\t0.9\tparallelizable and less training time\n0\t0.7\tuse in English constituency parsing\n20\t1\tperformance in English constituency parsing\n20\t0.8\ttraining on the Wall Street Journal dataset\n20\t0.9\ttraining in a semi-supervised setting\n20\t0.7\toutperforming RNN models and Berkeley-Parser",
        "sim": "dependent paper\tsource paper\timportance weight\tcontent of the relation\n20\t0\t0.8\tbased on attention mechanisms\t\n20\t0\t0.7\tsuperior in quality and parallelizable\t\n20\t0\t0.6\trequires significantly less time to train\t\n20\t0\t0.7\tachieves higher BLEU scores\t\n20\t0\t0.5\tgeneralizes well to other tasks"
    },
    "(0, 21)": {
        "diff": "paper\timportance weight\tthe important aspect\n0\t1\tThe Transformer architecture based on attention mechanisms\n0\t1\tThe superior quality and parallelizability of the model\n0\t0.8\t28.4 BLEU score on the WMT 2014 English-to-German translation task\n0\t0.7\t41.8 BLEU score on the WMT 2014 English-to-French translation task\n21\t1\tReplacing recurrent layers with multi-headed self-attention in the Transformer\n21\t1\tSignificantly faster training compared to recurrent or convolutional architectures\n21\t0.8\tOutperforming previously reported ensembles in translation tasks\n21\t0.7\tPlans to extend the Transformer to handle inputs and outputs other than text",
        "sim": "dependent paper\tsource paper\timportance weight\tcontent of the relation\n21\t0\t1.0\tproposal of attention-based models\n21\t0\t0.9\timprovement in translation tasks\n21\t0\t0.8\treplacement of recurrent layers\n21\t0\t0.7\tfaster training compared to other architectures\n21\t0\t0.6\tfuture plans for extension and investigation"
    },
    "(1, 2)": {
        "diff": "paper\timportance weight\tthe important aspect\n1\t0.5\trecurrent neural networks, long short-term memory\n1\t0.75\tsequential nature precludes parallelization within training examples\n1\t0.65\tattention mechanisms used in conjunction with a recurrent network\n1\t0.8\tproposing the Transformer as a model architecture eschewing recurrence and relying entirely on an attention mechanism\n2\t0.75\treducing sequential computation as the foundation of Extended Neural GPU, ByteNet, and ConvS2S\n2\t0.6\tself-attention as an attention mechanism relating different positions of a single sequence\n2\t0.9\tthe Transformer is the first transduction model relying entirely on self-attention to compute representations without using sequence-aligned RNNs or convolution\n2\t1\treduced effective resolution due to averaging attention-weighted positions, countered with Multi-Head Attention",
        "sim": "1\t2\t0.7\trecurrent neural networks, long short-term memory, gated recurrent neural networks\n1\t2\t0.8\tsequence modeling and transduction problems\n1\t2\t0.9\trecurrent language models and encoder-decoder architectures\n1\t2\t0.6\tattention mechanisms and recurrent networks\n1\t2\t0.75\tthe Transformer architecture and its advantages"
    },
    "(1, 11)": {
        "diff": "paper\timportance weight\tthe important aspect\n1\t0.9\tRecurrent neural networks\n1\t0.8\tLong short-term memory\n1\t0.7\tGated recurrent neural networks\n1\t0.6\tEncoder-decoder architectures\n11\t0.9\tEncoder-decoder structure\n11\t0.8\tContinuous representations\n11\t0.7\tAuto-regressive model\n11\t0.6\tStacked self-attention",
        "sim": "dependent paper\tsource paper\timportance weight\tcontent of the relation\n1\t11\t0.7\tshare architecture with\t\n1\t11\t0.6\tuse attention mechanism\t\n1\t11\t0.8\tboth involve sequence modeling\t\n1\t11\t0.7\tuse encoder-decoder structure\t\n1\t11\t0.5\tcan reach state of the art translation quality"
    },
    "(1, 12)": {
        "diff": "paper\timportance weight\tthe important aspect\n1\t0.8\tRecurrent neural networks and gated recurrent neural networks are state of the art approaches in sequence modeling and transduction problems.\n1\t0.6\tRecurrent models factor computation along the symbol positions of the input and output sequences.\n1\t0.7\tRecent work has achieved improvements in computational efficiency through factorization tricks and conditional computation.\n1\t0.9\tThe Transformer model architecture relies entirely on an attention mechanism for global dependencies between input and output.\n\n12\t0.8\tSelf-attention layers have a total computational complexity per layer that is faster than recurrent layers.\n12\t0.7\tSelf-attention layers connect all positions with a constant number of sequentially executed operations.\n12\t0.9\tConvolutional layers require a stack of O(n/k)convolutional layers to connect all pairs of input and output positions.\n12\t0.8\tSelf-attention could yield more interpretable models by inspecting attention distributions.",
        "sim": " dependent paper\tsource paper\timportance weight\tcontent of the relation\n1\t12\t1\tThe Transformer compares self-attention layers to recurrent and convolutional layers\n1\t12\t0.8\tThe Transformer uses an attention mechanism instead of recurrence to draw global dependencies\n1\t12\t0.7\tThe Transformer allows for significantly more parallelization than recurrent and convolutional layers\n1\t12\t0.6\tThe Transformer can reach a new state of the art in translation quality\n1\t12\t0.5\tThe Transformer achieves significant improvements in computational efficiency"
    },
    "(1, 21)": {
        "diff": "paper\timportance weight\tthe important aspect\n1\t1\trecurrent neural networks\n1\t1\tgated recurrent neural networks\n1\t1\tsequence modeling and transduction problems\n1\t0.5\tfactorization tricks and conditional computation\n21\t1\ttransformer\n21\t1\tmulti-headed self-attention\n21\t1\tfaster training than recurrent or convolutional layers\n21\t1\tstate of the art in translation quality",
        "sim": "dependent paper\tsource paper\timportance weight\tcontent of the relation\n1\t21\t1\tboth papers are about the Transformer model\n1\t21\t0.7\tTransformer model uses attention mechanism instead of recurrence\n1\t21\t0.6\tTransformer model achieves state-of-the-art performance in translation tasks\n1\t21\t0.5\tTransformer model can be trained faster than recurrent or convolutional architectures\n1\t21\t0.5\tTransformer model plans to extend to other input and output modalities"
    },
    "(1, 39)": {
        "diff": "paper\timportance weight\tthe important aspect\n1\t0.9\tRecurrent neural networks, long short-term memory and gated recurrent neural networks\n1\t0.8\tSignificant improvements in computational efficiency through factorization tricks and conditional computation\n1\t0.7\tAttention mechanisms used in conjunction with a recurrent network\n1\t0.6\tModel architecture relying entirely on an attention mechanism to draw global dependencies between input and output\n\n39\t0.9\tNeural machine translation as an end-to-end neural network\n39\t0.8\tNeural translation model using a bidirectional recurrent neural network\n39\t0.7\tDecoder network computing the conditional distribution over all possible translations\n39\t0.6\tSoft-alignment mechanism weighting each vector in the context set according to its relevance",
        "sim": "dependent paper\tsource paper\timportance weight\tcontent of the relation\n1\t39\t0.5\tproposes the transformer\tmodel architecture eschewing recurrence\tand reliance on attention mechanism\t\n1\t39\t0.7\tTransformer allows for more parallelization\tand can reach a new state of the art in translation quality\tafter being trained for little as twelve hours on eight P100 GPUs\t\n1\t39\t0.6\tTransformer is dependent on attention mechanism\tto draw global dependencies between input and output\t\n1\t39\t0.8\tTransformer model reaches a new state of the art in translation quality\tafter being trained for little as twelve hours on eight P100 GPUs\t\n1\t39\t0.7\tTransformer model architecture allows for more parallelization\tand significantly improves computational efficiency"
    },
    "(1, 43)": {
        "diff": "paper\timportance weight\tthe important aspect\n1\t1\tRecurrent neural networks, long short-term memory and gated recurrent neural networks\n1\t0.5\tsequential nature precludes parallelization within training examples\n1\t0.7\tTransformer model architecture relies entirely on an attention mechanism\n1\t0.8\tSignificantly more parallelization with the Transformer model\n43\t1\tBuilding a neural network for nonlinear mapping from spelling to meaning\n43\t0.7\tDecoder neural network needs to summarize translated content\n43\t0.6\tCharacter-level modeling on the target side is challenging\n43\t0.9\tGating units and one-step processing in bi-scale recurrent neural network",
        "sim": "dependent paper\tsource paper\timportance weight\tcontent of the relation\n1\t43\t0.8\tbuilds a neural network\tfor sentence meaning mapping\n1\t43\t0.7\tneeds to summarize\ttranslated content\n1\t43\t0.6\tchallenges the decoder network\tgenerate a long coherent sequence\n1\t43\t0.5\tframes as Gating units\tovercome challenges in target side\n1\t43\t0.5\tfocuses on challenges\ton the target side"
    },
    "(2, 7)": {
        "diff": "paper\t importance weight\t the important aspect\n2\t 0.8\t reducing sequential computation\n2\t 0.7\t using convolutional neural networks\n2\t 0.9\t learning dependencies between distant positions\n2\t 1.0\t relying entirely on self-attention\n\n7\t 0.7\t encoder-decoder attention mechanisms\n7\t 0.8\t self-attention layers in the encoder\n7\t 1.0\t self-attention layers in the decoder\n7\t 0.9\t preventing leftward information flow",
        "sim": "dependent paper\tsource paper\timportance weight\tcontent of the relation\n2\t7\t0.7\tself-attention used in both papers\n2\t7\t0.6\tTransformer and decoder use self-attention\n2\t7\t0.5\tattention mechanism in sequence-to-sequence models\n2\t7\t0.8\tencoder-decoder attention in Transformer\n2\t7\t0.6\tself-attention in encoder and decoder of Transformer"
    },
    "(2, 11)": {
        "diff": "paper\timportance weight\tthe important aspect\n2\t0.5\tReducing sequential computation\n2\t0.6\tUsing convolutional neural networks\n2\t0.7\tLearning dependencies between distant positions\n2\t0.8\tUse of self-attention mechanism\n11\t0.5\tEncoder-decoder structure\n11\t0.6\tAuto-regressive model\n11\t0.7\tStacked self-attention and point-wise, fully connected layers\n11\t0.8\tModel architecture",
        "sim": "dependent paper\tsource paper\timportance weight\tcontent of the relation\n2\t11\t0.9\tuses encoder-decoder structure\n2\t11\t0.8\tboth use stacked self-attention\n2\t11\t0.7\tboth use point-wise, fully connected layers\n2\t11\t0.6\tboth generate output sequence one element at a time\n2\t11\t0.5\tboth follow auto-regressive model"
    },
    "(2, 12)": {
        "diff": "paper\t importance weight\t the important aspect\n2\t1\tThe transformer is the first model relying entirely on self-attention.\n2\t0.8\tReduced effective resolution due to averaging attention-weighted positions.\n2\t0.7\tUses convolutional neural networks as basic building blocks.\n2\t0.6\tThe number of operations required to relate signals grows with distance between positions.\n\n12\t1\tSelf-attention layers have a constant number of sequentially executed operations.\n12\t0.8\tSelf-attention layers are faster than recurrent layers for smaller sequence lengths.\n12\t0.7\tA single convolutional layer does not connect all input and output positions.\n12\t0.6\tSelf-attention could yield more interpretable models.",
        "sim": "2    12     1             reducing sequential computation\n2    12     1             use convolutional neural networks\n2    12     0.75           learn dependencies between distant positions\n2    12     0.8           compute representations of its input and output\n2    12     0.7           total computational complexity per layer"
    },
    "(2, 21)": {
        "diff": "paper\timportance weight\tthe important aspect\n2\t1\tReducing sequential computation\n2\t0.75\tLogarithmically growing operations between positions\n2\t0.9\tMulti-Head Attention to counteract reduced resolution\n2\t0.8\tSelf-attention used for a variety of tasks\n21\t1\tSequence transduction model based entirely on attention\n21\t0.8\tFaster training for translation tasks\n21\t0.9\tState of the art performance in translation tasks\n21\t0.7\tPlan to apply attention-based models to other tasks",
        "sim": "2\t21\t1\tThe Transformer is the first transduction model relying on self-attention.\n2\t21\t0.8\tThe Transformer is based entirely on attention.\n2\t21\t0.7\tThe Transformer replaces recurrent layers with multi-headed self-attention.\n2\t21\t0.6\tThe Transformer achieves a new state of the art in translation tasks.\n2\t21\t0.5\tThe Transformer plans to apply attention-based models to other tasks."
    },
    "(11, 7)": {
        "diff": "paper\t  importance weight\t  the important aspect\n11\t  1\t  encoder-decoder structure\n11\t  1\t  auto-regressive model\n11\t  0.75\t  self-attention and point-wise, fully connected layers\n11\t  0.5\t  transformer model architecture\n7\t  1\t  encoder-decoder attention\n7\t  0.75\t  self-attention layers in encoder and decoder\n7\t  0.75\t  preventing leftward information flow\n7\t  0.5\t  scaled dot-product attention",
        "sim": "dependent paper\tsource paper\timportance weight\tcontent of the relation\n11\t7\t0.8\tencoder-decoder attention\n11\t7\t0.7\tself-attention layers\n11\t7\t0.6\tsequence transduction models\n11\t7\t0.6\ttransformer - model architecture\n11\t7\t0.5\tscaled dot-product attention"
    },
    "(11, 21)": {
        "diff": "paper\timportance weight\tthe important aspect\n11\t1\tThe encoder-decoder structure\n11\t1\tThe use of stacked self-attention and point-wise, fully connected layers\n11\t0.8\tThe auto-regressive model\n11\t0.7\tThe Transformer model architecture\n21\t1\tThe use of attention instead of recurrent layers\n21\t0.9\tFaster training compared to recurrent or convolutional layers\n21\t0.8\tAchieved new state of the art in translation tasks\n21\t0.7\tPlan to apply attention-based models to other tasks",
        "sim": "dependent paper\tsource paper\timportance weight\tcontent of the relation\n11\t21\t0.6\tencoder-decoder architecture, attention-based models\n11\t21\t0.7\tsequence transduction models, Transformer\n11\t21\t0.8\tstate of the art, recurrent layers\n11\t21\t0.9\tself-attention, point-wise, fully connected layers\n11\t21\t1\ttranslation tasks, new state of the art"
    },
    "(16, 18)": {
        "diff": "paper\timportance weight\tthe important aspect\n16\t0.5\tregularization during training\n16\t0.7\tResidual Dropout\n16\t0.8\tLabel Smoothing\n18\t0.6\testablishing a new state-of-the-art BLEU score\n18\t0.8\ttraining took 3.5 days on 8P100 GPUs\n18\t0.9\tbase model surpasses all previously published models and ensembles\n18\t1.0\tbeam search with a beam size of 4 and length penalty \u03b1= 0.6",
        "sim": "dependent paper\tsource paper\timportance weight\tcontent of the relation\n16\t18\t0.8\tachieves better BLEU scores\n16\t18\t0.9\toutperforms the best previously reported models\n16\t18\t0.7\testablishing a new state-of-the-art BLEU score\n16\t18\t0.6\tsurpasses all previously published models and ensembles\n16\t18\t0.5\tat a fraction of the training cost"
    },
    "(16, 19)": {
        "diff": "paper\timportance weight\tthe important aspect\n16\t1\tRegularization types during training\n16\t1\tTransformer achieves better BLEU scores than previous models\n16\t1\tDropout applied to output of each sub-layer\n16\t1\tLabel smoothing employed during training\n19\t1\tVariations on the Transformer architecture\n19\t1\tDifferent number of attention heads and dimensions\n19\t1\tEffect of reducing attention key size on model quality\n19\t1\tBigger models are better\n19\t1\tReplacing sinusoidal positional encoding with learned embeddings",
        "sim": "Here are the similarities between nodes 16 and 19:\n\n1. Node 16 discusses the use of regularization during training, while node 19 evaluates the importance of different components of the Transformer. (relation: compare)\n2. Both nodes mention the base model of the Transformer. Node 16 mentions the \"Transformer (base model)\" and node 19 refers to the \"base\" model. (relation: mention)\n3. Node 16 mentions the use of label smoothing during training, which improves accuracy and BLEU score. Node 19 also mentions a change in performance on English-to-German translation. (relation: discuss)\n4. Both nodes mention variations or modifications to the Transformer architecture. Node 16 mentions the \"Transformer (big)\" and node 19 includes variations labeled as A, B, C, D, and E. (relation: mention)\n5. Node 16 mentions the use of dropout to avoid overfitting, while node 19 discusses the helpfulness of dropout in avoiding overfitting. (relation: discuss)"
    },
    "(18, 19)": {
        "diff": "paper\timportance weight\tthe important aspect\n18\t1\tTransformer (big) outperforms previously reported models by more than 2.0 BLEU on WMT 2014 English-to-German translation task.\n18\t1\tThe big model achieves a BLEU score of 41.0 on WMT 2014 English-to-French translation task.\n18\t0.5\tThe configuration of the Transformer (big) model is listed in the bottom line of Table 3.\n18\t0.5\tThe base model surpasses previously published models and ensembles at a fraction of the training cost.\n19\t1\tWhile single-head attention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.\n19\t1\tReducing the attention key size hurts model quality.\n19\t0.5\tBigger models are better for translation performance.\n19\t0.5\tReplacing sinusoidal positional encoding with learned positional embeddings yields nearly identical results.",
        "sim": "Source Node, Target Node, Importance Weight, Content of the Relation\n18, 19, 1, compare performance on English-to-German translation\n18, 19, 0.8, evaluate importance of different components\n18, 19, 0.7, measure change in performance on English-to-German translation\n18, 19, 0.6, vary base model in different ways\n18, 19, 0.5, observe impact of varying attention key size"
    },
    "(18, 20)": {
        "diff": "paper\timportance weight\tthe important aspect\n18\t1\tOn the WMT 2014 English-to-German translation task\n20\t1\tOn English constituency parsing\n18\t1\tThe big transformer model outperforms previously reported models by more than 2.0 BLEU\n20\t1\tIn contrast to RNN sequence-to-sequence models, the Transformer outperforms the Berkeley-Parser\n18\t0.8\tState-of-the-art BLEU score of 28.4\n20\t0.8\tModel performs surprisingly well, yielding better results than all previously reported models (except RNN Grammar)\n18\t0.7\tTraining took 3.5 days on 8P100 GPUs\n20\t0.7\tTrained a 4-layer transformer with dmodel = 1024",
        "sim": "dependent paper\tsource paper\timportance weight\tcontent of the relation\n18\t20\t0.7\tperforms well in constituency parsing\n18\t20\t0.6\toutperforms Berkeley-Parser in constituency parsing\n18\t20\t0.6\toutperforms previously reported models in constituency parsing\n18\t20\t0.5\tachieves better results than all previous models in constituency parsing\n18\t20\t0.5\tyields better results than most previous models in constituency parsing"
    },
    "(21, 7)": {
        "diff": "paper\timportance weight\tthe important aspect\n21\t0.5\tTransformer is based on attention\n21\t0.5\tTransformer achieves state-of-the-art results on translation tasks\n21\t1\tTransformer plans to extend to other modalities (images, audio, video)\n21\t1\tTransformer aims to make generation less sequential\n7\t1\tTransformer uses multi-head attention in encoder-decoder layers\n7\t1\tTransformer uses self-attention in encoder and decoder layers\n7\t0.5\tEncoder-decoder attention allows decoder to attend over all positions in input sequence\n7\t0.5\tDecoder prevents leftward information flow to preserve auto-regressive property",
        "sim": "dependent paper\tsource paper\timportance weight\tcontent of the relation\n21\t7\t0.8\tuses multi-head attention in three different ways: \n21\t7\t0.7\tqueries come from previous decoder layer\n21\t7\t0.7\tmemory keys and values come from output of encoder\n21\t7\t0.6\tallows every position in decoder to attend over all positions in input sequence\n21\t7\t0.6\timplementation inside scaled dot-product attention by masking out illegal connections"
    },
    "(12, 9)": {
        "diff": "paper\t importance weight\t the important aspect\n12\t1\tComparison of self-attention, recurrent, and convolutional layers for sequence mapping\n12\t0.75\tComputational complexity of self-attention and recurrent layers\n12\t0.8\tParallelizability of self-attention and recurrent layers\n12\t0.6\tShorter paths for learning long-range dependencies in self-attention layers\n9\t0.5\tUse of learned embeddings and linear transformation in sequence transduction models\n9\t0.7\tSharing weights between embedding layers and pre-softmax linear transformation\n9\t0.65\tMultiplication of weights by square root of dmodel in embedding layers\n9\t0.55\tConversion of decoder output to predicted next-token probabilities using softmax",
        "sim": "dependent paper\tsource paper\timportance weight\tcontent of the relation\n12\t9\t1\tThe models use learned embeddings\n12\t9\t0.9\tBoth models share weight matrix\n12\t9\t0.8\tThe models convert tokens to vectors\n12\t9\t0.7\tThe models have linear transformations\n12\t9\t0.6\tThe models predict next-token probabilities"
    },
    "(39, 38)": {
        "diff": "paper\timportance weight\tthe important aspect\n39\t1\tneural machine translation\n39\t0.8\tend-to-end neural network\n39\t0.7\tbidirectional recurrent neural network\n39\t0.6\tsoft-alignment mechanism\n38\t1\tcharacter-level modelling\n38\t0.8\tsequence of subwords\n38\t0.7\trecurrent neural network\n38\t0.6\tbetter handles multiple timescales",
        "sim": "dependent paper\tsource paper\timportance weight\tcontent of the relation\n39\t38\t1\tneural machine translation can be done directly on a sequence of characters\n39\t38\t1\tneural machine translation models with a character-level decoder outperformed the ones with a subword-level decoder\n39\t38\t1\tthe models with a character-level decoder achieved comparable results on En-Ru\n39\t38\t1\tneural machine translation can benefit from translating at the character-level\n39\t38\t1\tthe models with a character-level decoder achieved better results on En-Cs, En-De, and En-Fi"
    },
    "(43, 38)": {
        "diff": "paper\timportance weight\tthe important aspect\n43\t1\tThe challenges on the target side include character-level modeling and generating long, coherent sequences of characters.\n43\t0.8\tThe size of the state space grows exponentially with the number of symbols in the target side.\n43\t0.7\tExamining if current recurrent neural networks can address the challenges on the target side.\n38\t1\tExisting machine translation systems rely on word-level modeling with explicit segmentation due to data sparsity and the belief that words are basic units of meaning.\n38\t0.8\tNeural machine translation suffers from the issues specific to word-level modeling, such as the increased computational complexity from a large target vocabulary.\n38\t0.6\tInvestigating if neural machine translation can be done directly on a sequence of characters without explicit word segmentation.\n38\t0.5\tCharacter-level decoder outperforms subword-level decoder in neural machine translation models for four language pairs.",
        "sim": "dependent paper\tsource paper\timportance weight\tcontent of the relation\n43\t38\t1\tThe source network must learn a nonlinear mapping.\n43\t38\t1\tThe target network needs to summarize translated information.\n43\t38\t0.75\tCharacter-level modeling is challenging for the target network.\n43\t38\t0.75\tThe state space grows exponentially for characters.\n43\t38\t0.5\tThe challenges on the target side."
    },
    "(43, 46)": {
        "diff": "paper\t importance weight\t the important aspect\n43\t1\tunclear how to build a neural network that learns a highly nonlinear mapping from a spelling to the meaning of a sentence\n43\t0.75\tcharacter-level modelling on the target side is more challenging\n43\t0.75\tsize of the state space grows exponentially w.r.t. the number of symbols\n43\t0.5\tGating units and one-step processing questions\n46\t1\texperiment with the base decoder will answer whether the existing neural network is enough to handle character-level decoding\n46\t1\tbi-scale decoder is tested to see whether it is possible to design a better decoder\n46\t0.75\ttwo-layer recurrent neural network designed to capture two timescales\n46\t0.5\ttesting two different types of recurrent neural networks on the target side",
        "sim": "dependent paper\tsource paper\timportance weight\tcontent of the relation\n43\t46\t0.75\tbuild a neural network\n43\t46\t0.8\tlearn a highly nonlinear mapping\n43\t46\t0.6\tsummarize what has been translated\n43\t46\t0.9\tgenerate a long, coherent sequence\n43\t46\t0.7\taddress these challenges"
    },
    "(43, 50)": {
        "diff": "paper\timportance weight\tthe important aspect\n43\t1\tneural network challenges\n43\t1\tstate space growth\n43\t0.5\tgating units and one-step processing\n43\t0.5\tframing challenges as questions\n50\t1\tneural machine translation at character level\n50\t1\tbenefits of character-level translation\n50\t1\tanalysis using subword symbols\n50\t0.5\tsource side represented as character sequence in the future",
        "sim": "dependent paper\tsource paper\timportance weight\tcontent of the relation\n43\t50\t1\tA recently proposed neural machine translation system can handle translation at the level of characters.\n43\t50\t0.7\tThe target side challenges can be addressed by using a decoder neural network.\n43\t50\t0.6\tThe target side challenges can benefit from translating at the character level.\n43\t50\t0.5\tThe authors focused on the challenges on the target side of machine translation.\n43\t50\t0.5\tThe authors empirically answered questions about the challenges on the target side."
    },
    "(7, 3)": {
        "diff": "paper\timportance weight\tthe important aspect\n7\t1\tThe Transformer uses multi-head attention in three different ways.\n7\t1\tIn \"encoder-decoder attention\" layers, the queries come from the previous decoder layer.\n7\t1\tThe encoder contains self-attention layers.\n7\t1\tSelf-attention layers in the decoder allow each position to attend to all positions in the decoder up to and including that position.\n3\t1\tThe encoder is composed of a stack of N= 6 identical layers.\n3\t1\tThe first sub-layer in each encoder layer is a multi-head self-attention mechanism.\n3\t1\tThe second sub-layer in each encoder layer is a simple, position-wise fully connected feed-forward network.\n3\t0.5\tThe decoder inserts a third sub-layer, which performs multi-head attention over the output of the encoder stack.",
        "sim": "dependent paper\tsource paper\timportance weight\tcontent of the relation\n3\t7\t0.8\tuses multi-head attention mechanism\n3\t7\t0.7\tencoder-decoder attention layers\n3\t7\t0.6\tencoder contains self-attention layers\n3\t7\t0.6\tdecoder contains self-attention layers\n3\t7\t0.5\tattention mechanism prevents leftward information flow in decoder"
    },
    "(3, 8)": {
        "diff": "paper\timportance weight\tthe important aspect\n3\t1\tencoder composed of stack of identical layers\n3\t1\tThe output of each sub-layer is LayerNorm\n3\t0.5\toutputs of dimension dmodel = 512\n3\t0.5\tdecoder inserts a third sub-layer for multi-head attention\n\n8\t1\tfully connected feed-forward network applied to each position\n8\t1\ttwo linear transformations with a ReLU activation\n8\t0.5\tdimensionality of input and output is dmodel = 512\n8\t0.5\tinner-layer has dimensionality dff = 2048",
        "sim": "dependent paper\tsource paper\timportance weight\tcontent of the relation\n8\t3\t0.8\tencoder has two sub-layers\n8\t3\t0.7\tdecoder has two sub-layers\n8\t3\t0.6\tsub-layers have residual connections\n8\t3\t0.5\tsub-layers have layer normalization\n8\t3\t0.8\tsub-layers have dimensionality of 512"
    },
    "(4, 5)": {
        "diff": "paper\timportance weight\tthe important aspect\n4\t0.5\tAttention function described as mapping\n4\t0.6\tScaled Dot-Product Attention\n4\t0.7\tMulti-Head Attention\n4\t0.8\tWeight assigned to each value computed by a compatibility function\n5\t0.5\tScaled Dot-Product Attention\n5\t0.6\tMatrix multiplication code used for implementation\n5\t0.7\tAdditive attention outperforms dot product attention without scaling for larger values of dk\n5\t0.8\tDot products scaled by1\u221adk",
        "sim": "dependent paper\tsource paper\timportance weight\tcontent of the relation\n4\t5\t1\tScaled Dot-Product Attention\n4\t5\t0.9\tMulti-Head Attention\n4\t5\t0.8\tQuery, keys, values, output are vectors\n4\t5\t0.7\tWeight assigned to each value\n4\t5\t0.6\tMatrix Q, K, V; softmax function"
    },
    "(4, 6)": {
        "diff": "paper\timportance weight\tthe important aspect\n4\t0.8\tScaled Dot-Product Attention\n4\t0.7\tMulti-Head Attention\n4\t0.6\tAttention layers running in parallel\n4\t0.5\tCompatibility function of query with key\n\n6\t0.9\tLinear projection of queries, keys, and values\n6\t0.8\tParallel performance of attention function\n6\t0.7\tMulti-head attention allows attending to different representation subspaces\n6\t0.6\tAveraging inhibits attending to different representation subspaces",
        "sim": "dependent paper\tsource paper\timportance weight\t                      content of the relation\n4\t                6\t            0.8\t  Attention function description with vectors\n4\t                6\t            0.7\t  Scaled Dot-Product Attention\n4\t                6\t            0.6\t  Multi-Head Attention\n4\t                6\t            0.5\t  Different dimensions in attention function\n4\t                6\t            0.5\t  Parallel attention function with projections"
    },
    "(5, 6)": {
        "diff": "paper\timportance weight\tthe important aspect\n5\t1\t\"Scaled Dot-Product Attention\" (Figure 2)\n5\t0.75\t\"Additive attention\" vs \"dot-product attention\"\n5\t0.75\tScaling factor of 1/\u221adk for dot-product attention\n5\t0.5\tOutperforms dot-product attention without scaling for larger values of dk\n6\t1\tPerforming attention function with different linear projections\n6\t0.75\tMulti-head attention allows joint attention to different representation subspaces\n6\t0.75\tUsing h=8 parallel attention layers with reduced dimensions\n6\t0.5\tTotal computational cost similar to single-head attention with full dimensionality",
        "sim": "dependent paper\tsource paper\timportance weight\tcontent of the relation\n6\t5\t1\tThe attention function is performed in parallel\n6\t5\t1\tQueries, keys, and values are projected linearly\n6\t5\t1\tThe projections are concatenated and projected again\n6\t5\t1\tMulti-head attention allows joint attention to different subspaces\n6\t5\t1\tThe total computational cost is similar to single-head attention"
    },
    "(23, 24)": {
        "diff": "paper\timportance weight\tthe important aspect\n23\t1\tmlpconv layer\n23\t0.8\tCNN implicitly assumes linear separability\n23\t0.7\treplacing GLM with nonlinear function approximator enhances abstraction ability\n23\t0.6\tglobal average pooling layer improves interpretability\n\n24\t1\tmaxout network separates concepts within convex sets\n24\t0.8\tNIN introduces micro network for more abstract features\n24\t0.7\tmaxout network performs best on benchmark datasets\n24\t0.6\tNIN integrates micro network into CNN structure",
        "sim": "dependent paper\tsource paper\timportance weight\tcontent of the relation\n23\t24\t1\tconvolutional neural networks consist of convolutional layers\n23\t24\t0.8\tpooling layers are present in convolutional neural networks\n23\t24\t0.7\tboth convolution layers and pooling layers in CNN take part in feature map generation\n23\t24\t0.6\tGLM is a generalized linear model used in CNN\n23\t24\t0.5\tCNN makes the assumption that latent concepts are linearly separable"
    },
    "(23, 25)": {
        "diff": "paper\timportance weight\tthe important aspect\n23\t1.0\tconvolution layers and pooling\n23\t1.0\tGLM replaced with a nonlinear function approximator\n23\t1.0\tNIN has micro networks within mlpconv layers\n23\t1.0\tGlobal average pooling as a structural regularizer\n25\t1.0\tUniversal function approximator for feature extraction of local patches\n25\t1.0\tMLP is compatible with the structure of CNNs\n25\t1.0\tMLP can be a deep model itself\n25\t1.0\tCascaded cross channel parametric pooling allows learnable interactions of cross channel information",
        "sim": "dependent paper\tsource paper\timportance weight\tcontent of the relation\n25\t23\t1\tconvolutional neural networks consist of alternating convolutional layers and pooling layers.\n23\t25\t1\tmlpconv layer replaces the GLM to convolve over the input.\n23\t25\t0.7\tmlpconv layer is a universal function approximator.\n23\t25\t0.6\tmlpconv layer allows complex and learnable interactions of cross channel information.\n23\t25\t0.5\tmlpconv layer has greater capability in modeling various distributions of latent concepts."
    },
    "(23, 26)": {
        "diff": "paper\timportance weight\tthe important aspect\n23\t1\tConvolutional neural networks consist of alternating convolutional layers and pooling layers.\n23\t0.8\tThe convolution filter in CNN is a generalized linear model (GLM) for the underlying data patch.\n23\t0.6\tThe GLM in CNN is replaced with a \"micro network\" structure in NIN.\n23\t0.7\tInstead of adopting the traditional fully connected layers for classification in CNN, spatial average of the feature maps is directly outputted via global average pooling layer.\n26\t1\tConventional CNNs perform convolution in the lower layers and use fully connected layers for classification.\n26\t0.9\tDropout is used as a regularizer in fully connected layers to prevent overfitting.\n26\t0.8\tGlobal average pooling is proposed as a strategy to replace fully connected layers in CNN.\n26\t0.7\tGlobal average pooling enforces correspondences between feature maps and categories.",
        "sim": "dependent paper\tsource paper\timportance weight\tcontent of the relation\n23\t26\t0.8\tperform convolution in the lower layers\n23\t26\t0.7\tfully connected layers prone to overfitting\n23\t26\t0.7\tglobal average pooling as regularizer\n23\t26\t0.6\tglobal average pooling more native to convolution structure\n23\t26\t0.6\tglobal average pooling avoids overfitting at this layer"
    },
    "(23, 27)": {
        "diff": "paper\t importance weight\t the important aspect\n23\t1\tabstraction ability of the local model\n23\t0.9\tnonlinear manifold representation of data\n23\t0.8\tmlpconv layer in NIN\n23\t0.7\tstructure of NIN as stacking of mlpconv layers\n27\t1\tglobal average layers in NIN\n27\t0.9\tmaxout networks in NIN\n27\t0.8\tthree-layer perceptron in mlpconv layer\n27\t0.7\tflexibility of number of layers in NIN and micro networks",
        "sim": "dependent paper\tsource paper\timportance weight\tcontent of the relation\n27\t23\t1\tThe overall structure of NIN is a stack of mlpconv layers\n27\t23\t0.8\tmlpconv layers contain a three-layer perceptron\n27\t23\t0.7\tThe number of layers in NIN is flexible\n27\t23\t0.6\tNIN can be tuned for specific tasks\n27\t23\t0.5\tNIN and micro networks have similar structures"
    },
    "(23, 28)": {
        "diff": "paper\timportance weight\tthe important aspect\n23\t1\tConvolutional neural networks (CNNs)\n23\t1\tReplacing the GLM with a more potent nonlinear function approximator\n23\t0.75\tGLM can achieve a good extent of abstraction when the samples of the latent concepts are linearly separable\n23\t0.75\tThe data for the same concept often live on a nonlinear manifold\n28\t1\tThe MLP convolutional layer and the global averaging pooling layer\n28\t1\tThe overall NIN structure\n28\t1\tSec. 3.1 and Sec. 3.2 detail the key components",
        "sim": "dependent paper\tsource paper\timportance weight\tcontent of the relation\n23\t28\t0.5\tproposed \"Network In Network\" structure\n23\t28\t0.7\tMLP convolutional layer\n23\t28\t0.8\tglobal averaging pooling layer\n23\t28\t0.6\tkey components of the proposed structure\n23\t28\t0.5\toverall NIN structure"
    },
    "(23, 34)": {
        "diff": "paper\timportance weight\tthe important aspect\n23\t1\tcnn consists of alternating convolutional layers and pooling layers\n23\t0.75\tconvolution filter in CNN is a generalized linear model (GLM) for the underlying data patch\n23\t0.5\tmlpconv layer maps the local receptive field to an output feature vector\n23\t0.5\tglobal average pooling is more meaningful and interpretable as it enforces correspondence between feature maps and categories\n34\t1\tfully connected layers can have dense transformation matrices and the values are subject to back-propagation optimization\n34\t0.75\tglobal average pooling achieved the lowest testing error among the three methods\n34\t0.5\treplacing the fully connected layer with global average pooling improves the error rate of the CNN model\n34\t0.5\tglobal average pooling might be too demanding for linear convolution layers as it requires the linear filter with rectified activation to model the confidence maps of the categories",
        "sim": "dependent paper\tsource paper\timportance weight\tcontent of the relation\n34\t23\t0.8\tglobal average pooling is similar\n34\t23\t0.7\tglobal average pooling compared to fully connected layer\n34\t23\t0.6\tglobal average pooling has achieved the lowest testing error\n34\t23\t0.7\tglobal average pooling has the same regularization effect\n34\t23\t0.6\tglobal average pooling might be too demanding"
    },
    "(23, 35)": {
        "diff": "paper\timportance weight\tthe important aspect\n23\t1\tconvolutional neural networks, alternating convolutional layers and pooling layers, feature maps, replacing GLM with nonlinear function approximator\n23\t0.7\tGeneric Linear Model (GLM) is a low level of abstraction, conventional CNN assumes linear separability, multilayer perceptron as a universal function approximator\n23\t0.8\tNIN has a \"micro network\" structure, mlpconv layer is compared with CNN, mlpconv layer maps input local patch to output feature vector, stacking of multiple mlpconv layers in NIN\n23\t0.6\tIn NIN, traditional fully connected layers for classification in CNN are replaced with global average pooling layer, global average pooling prevents overfitting, interpretable correspondance between feature maps and categories\n\n35\t1\tExplicit enforcement of feature maps as confidence maps of categories, global average pooling, stronger local receptive field modeling in mlpconv\n35\t0.8\tVisualization of feature maps from last mlpconv layer for CIFAR-10, largest activations expected for ground truth category, activation in same region of object in original image\n35\t0.9\tEffectiveness of NIN achieved by stronger local receptive field modeling in mlpconv layers, learning of category level feature maps enforced by global average pooling\n35\t0.7\tPotential for further exploration of general object detection using category level feature maps, similar approach used in scene labeling work of Farabet et al.",
        "sim": "dependent paper\tsource paper\timportance weight\tcontent of the relation\n23\t35\t0.8\tenforces stronger local modeling\n23\t35\t0.7\taccomplishes purpose of enforcing feature maps\n23\t35\t0.6\tvisualizes feature maps for categories\n23\t35\t0.5\tdemonstrates effectiveness of NIN\n23\t35\t0.5\tachieves stronger local receptive \ufb01eld modeling"
    },
    "(23, 36)": {
        "diff": "paper\timportance weight\tthe important aspect\n23\t1\tconvolutional neural networks (CNNs) consist of alternating convolutional layers and pooling layers.\n23\t1\tconvolution layers take inner product of the linear filter and the underlying receptive field followed by a nonlinear activation function at every local portion of the input.\n23\t1\tconvolutional neural networks implicitly assume that the latent concepts are linearly separable.\n23\t1\tthe NIN structure consists of multiple mlpconv layers, which are composed of micro networks (MLP).\n36\t1\tNetwork In Network (NIN) is a novel deep network for classification tasks.\n36\t1\tmlpconv layers use multilayer perceptrons to convolve the input, improving the modeling of local patches.\n36\t1\tglobal average pooling layer replaces the fully connected layers in conventional CNN and acts as a structural regularizer.\n36\t1\tNIN demonstrated state-of-the-art performance on CIFAR-10, CIFAR-100, and SVHN datasets.",
        "sim": "dependent paper\tsource paper\timportance weight\tcontent of the relation\n36\t23\t1\tNIN proposed for classification tasks\n36\t23\t0.8\tNIN uses mlpconv layers\n36\t23\t0.6\tNIN uses global average pooling\n36\t23\t0.7\tNIN achieves state-of-the-art performance\n36\t23\t0.9\tNIN feature maps are confidence maps"
    },
    "(24, 25)": {
        "diff": "paper\timportance weight\tthe important aspect\n24\t1\tClassic convolutional neuron networks\n24\t1.0\tlinear convolutional filters\n24\t1.0\tover-complete set of filters\n24\t1.0\tNIN introduces micro network\n25\t0.5\tuniversal function approximator\n25\t0.5\tmlpconv layer replaces GLM\n25\t0.5\tcross channel parametric pooling\n25\t0.5\tgreater capability in modeling various distributions",
        "sim": "dependent paper\tsource paper\timportance weight\tcontent of the relation\n24\t25\t0.8\tCNN structure can be compatible with multilayer perceptron\n24\t25\t0.7\tNIN introduces a micro network within each convolutional layer\n24\t25\t0.6\tMaxout network performs maximum pooling for feature maps reduction\n24\t25\t0.5\tMLP replaces GLM in mlpconv layer for convolution over the input\n24\t25\t0.5\tCascaded cross channel parametric pooling allows learnable interactions"
    },
    "(24, 26)": {
        "diff": "paper\timportance weight\tthe important aspect\n24\t0.5\tspatial pooling layers, non-linear activation functions, over-complete set of filters, \"Network In Network\" structure\n26\t0.5\tfully connected layers prone to overfitting, dropout as a regularizer, global average pooling, mlpconv layers",
        "sim": "dependent paper\tsource paper\timportance weight\tcontent of the relation\n24\t26\t0.8\tperform convolution in lower layers\n24\t26\t0.7\treplace fully connected layers\n24\t26\t0.6\tintroduce global average pooling\n24\t26\t0.7\tenforce correspondences between feature maps and categories\n24\t26\t0.8\tavoid overfitting at this layer"
    },
    "(24, 27)": {
        "diff": "paper\timportance weight\tthe important aspect\n24\t0.8\tconvolutional layers generate feature maps\n24\t0.7\tcompensation for linear separability\n24\t0.6\tmaxout network separates concepts within convex sets\n24\t0.5\t\"Network In Network\" structure for more abstract features\n27\t0.8\tNIN is a stack of mlpconv layers\n27\t0.7\tmicro networks within each mlpconv layer\n27\t0.6\tNIN and micro networks are flexible and tunable\n27\t0.5\tglobal average layers in NIN and maxout networks",
        "sim": "dependent paper\tsource paper\timportance weight\tcontent of the relation\n24\t27\t0.8\tMain similarity is the \"Network In Network\" structure\n24\t27\t0.7\tBoth papers discuss the use of convolutional layers\n24\t27\t0.6\tBoth papers mention the use of linear functions in feature maps\n24\t27\t0.5\tNIN and maxout network are compared in terms of performance\n24\t27\t0.5\tBoth papers mention the use of pooling layers"
    },
    "(24, 28)": {
        "diff": "paper\timportance weight\tthe important aspect\n24\t1\tThe use of linear convolutional filters and nonlinear activation functions in CNNs.\n24\t0.5\tThe compensation of nonlinearity in CNNs by utilizing an over-complete set of filters.\n24\t0.75\tThe benefit of better abstraction on each local patch before combining them into higher level concepts.\n24\t0.5\tThe capability of maxout network to separate concepts within convex sets.\n28\t0.75\tThe introduction of the MLP convolutional layer in the \"Network In Network\" structure.\n28\t0.75\tThe inclusion of the global averaging pooling layer in the \"Network In Network\" structure.\n28\t0.5\tThe detailed explanation of the overall \"Network In Network\" structure in Section 3.3.",
        "sim": "Here are the similarities between Node 24 and Node 28:\n\ndependent paper\tsource paper\timportance weight\tcontent of the relation\n24\t28\t1.0\tproposed \"Network In Network\" structure\n24\t28\t1.0\tmicro network within each convolutional layer\n24\t28\t1.0\tbetter abstractions for all levels of features\n24\t28\t0.8\tconvolutional layers in both papers\n24\t28\t0.6\tabstraction of local patches"
    },
    "(24, 36)": {
        "diff": "paper\timportance weight\tthe important aspect\n24\t1\tClassic convolutional neuron networks\n24\t1.0\tspatial pooling layers\n24\t0.75\tlinear rectifier as activation function\n24\t0.8\tover-complete set of filters\n36\t1\tmlpconv layers\n36\t0.9\tglobal average pooling layer\n36\t0.8\tvisualization of feature maps\n36\t0.7\tobject detection via NIN",
        "sim": "dependent paper\tsource paper\timportance weight\tcontent of the relation\n36\t24\t1\tconvolve input using perceptrons\n36\t24\t0.75\tmodel local patches better\n36\t24\t0.6\treplace fully connected layers\n36\t24\t0.5\tprevent overfitting globally\n36\t24\t0.5\timprove abstraction for all features"
    },
    "(25, 26)": {
        "diff": "paper\timportance weight\tthe important aspect\n25\t1\tThe paper introduces the concept of mlpconv layer in convolutional neural networks, which is a replacement for the GLM in linear convolutional layer.\n25\t0.8\tThe mlpconv layer uses a multilayer perceptron as a universal function approximator for feature extraction of local patches.\n25\t0.7\tThe mlpconv layer allows for complex and learnable interactions of cross channel information through cascaded cross channel parametric pooling.\n25\t0.6\tThe activation function used in the multilayer perceptron is rectified linear unit.\n26\t1\tThe paper proposes global average pooling as a strategy to replace traditional fully connected layers in convolutional neural networks.\n26\t0.9\tGlobal average pooling generates one feature map for each corresponding category of the classification task.\n26\t0.8\tGlobal average pooling enforces correspondences between feature maps and categories, making the interpretation of the feature maps as category confidence maps easier.\n26\t0.7\tGlobal average pooling is a structural regularizer that explicitly enforces feature maps to be confidence maps of concepts (categories).",
        "sim": "\"dependent paper\tsource paper\timportance weight\tcontent of the relation\"\n\"25\t26\t1\tGiven priors about distributions\t\"\n\"26\t25\t1\tGiven no priors about distributions\t\"\n\"25\t26\t0.8\tUse a universal function approximator\t\"\n\"26\t25\t0.8\tUse a universal function approximator\t\"\n\"25\t26\t0.7\tChoose multilayer perceptron\t\""
    },
    "(25, 36)": {
        "diff": "paper\timportance weight\tthe important aspect\n25\t0.5\tUniversal function approximator for feature extraction of local patches\n25\t0.7\tChoice of multilayer perceptron in the mlpconv layer\n25\t0.9\tCross channel parametric pooling for complex and learnable interactions of cross channel information\n25\t1.0\tDifference between mlpconv layer and maxout layer in function approximation capabilities\n36\t0.5\tNetwork In Network (NIN) structure for classification tasks\n36\t0.7\tUse of mlpconv layers to model local patches\n36\t0.9\tGlobal average pooling as a structural regularizer to prevent overfitting\n36\t1.0\tFeature maps from the last mlpconv layer of NIN as con\ufb01dence maps of categories",
        "sim": "dependent paper\tsource paper\timportance weight\tcontent of the relation\r\n25\t36\t0.7\tproposed a novel deep network\r\n25\t36\t0.8\tuse multilayer perceptrons to convolve\r\n25\t36\t0.9\ta replacement for the fully connected layers\r\n25\t36\t0.6\tstate-of-the-art performance on CIFAR-10\r\n25\t36\t0.7\tact as a structural regularizer"
    },
    "(26, 34)": {
        "diff": "paper\timportance weight\tthe important aspect\n26\t0.7\tproposes global average pooling as replacement for fully connected layers in CNN\n26\t0.8\tglobal average pooling enforces correspondence between feature maps and categories\n26\t0.6\tglobal average pooling avoids overfitting by not having parameters to optimize\n26\t0.7\tglobal average pooling is more robust to spatial translations of the input\n34\t0.7\tglobal average pooling layer has a pre\ufb01xed transformation matrix with non-zero values on the diagonal\n34\t0.6\tcomparison of performances of models with fully connected layer and global average pooling\n34\t0.7\tglobal average pooling achieved the lowest testing error among the evaluated models\n34\t0.8\tglobal average pooling acts as a regularization technique for conventional CNNs by improving performance\n",
        "sim": "dependent paper\tsource paper\timportance weight\tcontent of the relation\n26\t34\t1\tConventional CNNs replaced with global average pooling\n26\t34\t0.8\tGlobal average pooling as a structural regularizer\n26\t34\t0.7\tGlobal average pooling more robust to spatial translations\n26\t34\t0.6\tGlobal average pooling enforces feature maps as confidence maps\n26\t34\t0.5\tGlobal average pooling avoids overfitting"
    },
    "(26, 35)": {
        "diff": "paper\t importance weight\t the important aspect\n26\t0.5\tGlobal average pooling replaces fully connected layers in the CNN.\n26\t0.6\tGlobal average pooling enforces correspondences between feature maps and categories.\n26\t0.7\tGlobal average pooling avoids overfitting at this layer.\n26\t0.8\tGlobal average pooling is more robust to spatial translations of the input.\n35\t0.5\tGlobal average pooling enforces feature maps to be confidence maps of categories.\n35\t0.6\tFeature maps in the last mlpconv layer are visualized for each category.\n35\t0.7\tStrongest activations appear in the feature map corresponding to the ground truth category.\n35\t0.8\tStrongest activations roughly appear at the same region of the object in the original image.",
        "sim": "Here is the list of similarities between nodes 26 and 35:\n\n1. 26\t35\t1\tAverage pooling replaces fully connected layers\n2. 26\t35\t1.0\tGlobal average pooling as a structural regularizer\n3. 26\t35\t1.0\tGlobal average pooling enforces feature maps as confidence maps\n4. 26\t35\t0.75\tLast mlpconv layer used in NIN\n5. 26\t35\t0.75\tExplicit enforcement of feature maps as confidence maps"
    },
    "(26, 36)": {
        "diff": "paper\timportance weight\tthe important aspect\n26\t0.9\tglobal average pooling replaces fully connected layers\n26\t0.8\tglobal average pooling avoids overfitting\n26\t0.7\tglobal average pooling enforces correspondence between feature maps and categories\n36\t0.9\tNetwork In Network (NIN) structure\n36\t0.8\tNIN uses mlpconv layers to model local patches\n36\t0.7\tNIN uses global average pooling as a replacement for fully connected layers\n36\t0.6\tNIN achieves state-of-the-art performance on CIFAR-10, CIFAR-100, and SVHN datasets\n",
        "sim": "Dependent paper\t Source paper\t Importance weight\t Content of the relation\n26\t36\t0.8\tGlobal average pooling used in NIN\n26\t36\t0.7\tImprovement in generalization ability\n26\t36\t0.6\tBoth papers propose novel strategies\n26\t36\t0.5\tBoth papers address overfitting\n26\t36\t0.7\tSimilar motivation for using global average pooling"
    },
    "(27, 28)": {
        "diff": "paper\timportance weight\tthe important aspect\n27\t1.0\toverall structure of NIN is a stack of mlpconv layers\n27\t0.8\tglobal average layers as in CNN and maxout networks\n27\t0.7\tNIN has three mlpconv layers\n27\t0.6\tNIN and micro networks have flexible number of layers\n28\t1.0\tkey components of NIN are MLP convolutional layer and global averaging pooling layer\n28\t0.8\tNIN is detailed in section 3.3\n28\t0.7\tNIN is proposed in the paper\n28\t0.6\tNIN structure is described in section 3.1 and 3.2",
        "sim": "dependent paper\t source paper\t importance weight\t content of the relation\n27\t28\t1\tThe overall structure of NIN\n27\t28\t1\tis a stack of mlpconv layers\n27\t28\t1\ton top of which lie\n27\t28\t1\tthe global average layers\n27\t28\t1\tan NIN with three mlpconv layers"
    },
    "(27, 29)": {
        "diff": "paper\t importance weight\t the important aspect\n27\t1\toverall structure of NIN is a stack of mlpconv layers\n27\t1\tglobal average pooling instead of fully connected layers\n27\t0.5\tnumber of layers is flexible and can be tuned\n27\t1\tmicro networks consist of three-layer perceptron\n29\t1\tregularizer dropout is applied on outputs of all but the last mlpconv layers\n29\t1\tregularizer weight decay is applied\n29\t1\ttraining procedure involves manual initialization of weights and learning rates\n29\t1\ttraining process uses mini-batches of size 128",
        "sim": "dependent paper\tsource paper\timportance weight\tcontent of the relation\n27\t29\t0.8\tcompare network structures\n27\t29\t0.7\tevaluate NIN on benchmark datasets\n27\t29\t0.6\tuse mlpconv layers\n27\t29\t0.5\tapply dropout as a regularizer\n27\t29\t0.5\tapply weight decay regularizer"
    },
    "(27, 35)": {
        "diff": "paper\t    importance weight\tcontent of difference\n27\t        0.5\t          overall structure of NIN\n27\t        0.5\t          flexibility in number of layers\n27\t        1\t              stack of mlpconv layers\n27\t        1\t              maxout networks\n\n35\t        0.5\t          enforcement of feature maps as confidence maps\n35\t        0.5\t          visualization of feature maps from trained model\n35\t        1\t              stronger local receptive field modeling\n35\t        1\t              potential for general object detection",
        "sim": "dependent paper\tsource paper\timportance weight\tcontent of the relation\n27\t35\t0.85\taccomplished by global pooling\n27\t35\t0.87\tvisualization demonstrates effectiveness of NIN\n27\t35\t0.78\tenforces learning of category-level feature maps\n27\t35\t0.82\tstrongest activations appear at same region of object\n27\t35\t0.75\teffectiveness achieved via stronger local receptive field modeling"
    },
    "(27, 36)": {
        "diff": "paper\timportance weight\tthe important aspect\n27\t1\tThe overall structure of NIN is a stack of mlpconv layers\n27\t0.8\tThe number of layers in both NIN and the micro networks is flexible\n27\t0.7\tIt has a three-layer perceptron within each mlpconv layer\n27\t0.6\tThe global average pooling layer acts as a structural regularizer\n\n36\t1\tNIN is a novel deep network for classification tasks\n36\t0.9\tMlpconv layers model the local patches better\n36\t0.8\tGlobal average pooling acts as a structural regularizer\n36\t0.7\tFeature maps from the last mlpconv layer of NIN are confidence maps of the categories",
        "sim": "dependent paper\tsource paper\timportance weight\tcontent of the relation\n27\t36\t0.9\tNIN proposed for classification tasks\n27\t36\t0.8\tNIN consists of mlpconv layers\n27\t36\t0.7\tNIN uses multilayer perceptrons\n27\t36\t0.6\tNIN demonstrates state-of-the-art performance\n27\t36\t0.5\tNIN features maps as confidence maps"
    },
    "(28, 29)": {
        "diff": "paper\timportance weight\tthe important aspect\n28\t0.7\tMLP convolutional layer\n28\t0.8\tGlobal averaging pooling layer\n28\t0.9\tOverall NIN structure\n28\t1.0\tKey components of NIN structure\n29\t0.7\tNetwork evaluation on benchmark datasets\n29\t0.8\tRegularizers applied in experiments\n29\t0.9\tDetailed settings of network parameters\n29\t1.0\tTraining procedure and learning rates",
        "sim": "The similarities between nodes 28 and 29 can be summarized as follows:\n\ndependent paper\tsource paper\timportance weight\tcontent of the relation\n- 28\t29\t0.8\tproposed network structure\n- 28\t29\t0.7\tevaluation on benchmark datasets\n- 28\t29\t0.6\tuse of mlpconv layers\n- 28\t29\t0.5\tuse of global average pooling\n- 28\t29\t0.5\tuse of weight decay regularizer"
    },
    "(28, 36)": {
        "diff": "paper\t importance weight\t the important aspect\n28\t1\tproposed \"Network In Network\" structure\n28\t0.8\tMLP convolutional layer\n28\t0.6\tglobal averaging pooling layer\n28\t0.5\toverall NIN details\n36\t1\tproposed \"Network In Network\" (NIN)\n36\t0.8\tmlpconv layers\n36\t0.6\tglobal average pooling layer\n36\t0.5\tdemonstrated state-of-the-art performance",
        "sim": "dependent paper\tsource paper\timportance weight\tcontent of the relation\n28\t36\t0.8\tproposed \"Network In Network\" structure\n28\t36\t0.9\tMLP convolutional layer\n28\t36\t0.7\tglobal averaging pooling layer\n28\t36\t0.6\toverall NIN\n28\t36\t0.8\tnovel deep network called \"Network In Network\""
    },
    "(35, 36)": {
        "diff": "paper\timportance weight\tthe important aspect\n35\t0.5\tenforce feature maps in last mlpconv layer\n35\t0.5\tvisualize feature maps from last mlpconv layer\n35\t0.5\tglobal average pooling enforces learning of category level feature maps\n35\t0.5\texplore general object detection based on category level feature maps\n36\t0.5\tNIN is a novel deep network structure\n36\t0.5\tmlpconv layers model local patches better\n36\t0.5\tglobal average pooling acts as structural regularizer\n36\t0.5\tNIN demonstrates state-of-the-art performance on multiple datasets",
        "sim": "dependent paper\tsource paper\timportance weight\tcontent of the relation\n35\t36\t0.5\tmotivates the possibility of performing object detection\n35\t36\t0.5\tmodels the local patches better\n35\t36\t0.5\tacts as a structural regularizer\n35\t36\t0.5\tdemonstrated state-of-the-art performance\n35\t36\t0.5\tdemonstrated that feature maps"
    },
    "(36, 29)": {
        "diff": "paper   importance weight   the important aspect\n36      1                   NIN is a novel deep network for classification tasks\n36      0.8                 NIN uses mlpconv layers for better local patch modeling\n36      0.7                 NIN uses global average pooling as a replacement for fully connected layers\n36      0.6                 NIN achieves state-of-the-art performance on CIFAR-10, CIFAR-100, and SVHN datasets\n29      1                   NIN is evaluated on four benchmark datasets: CIFAR-10, CIFAR-100, SVHN, and MNIST\n29      0.8                 NIN consists of three stacked mlpconv layers followed by spatial max pooling\n29      0.7                 NIN utilizes global average pooling instead of fully connected layers at the top\n29      0.6                 NIN applies dropout and weight decay as regularizers",
        "sim": "dependent paper\tsource paper\timportance weight\tcontent of the relation\n36\t29\t1\tmain network architecture\t\n36\t29\t0.8\tclassification performance on datasets\t\n36\t29\t0.7\tfeature maps as confidence maps\t\n36\t29\t0.6\tpossibility of object detection\t\n36\t29\t0.5\tvisualization of feature maps\t"
    },
    "(29, 30)": {
        "diff": "paper\timportance weight\tthe important aspect\n29\t1\ttraining procedure details\n29\t1\tpreprocessing methods used\n29\t0.8\tuse of dropout as regularizer\n29\t0.7\tuse of weight decay regularizer\n\n30\t1\tuse of global contrast normalization and ZCA whitening\n30\t1\ttuning hyperparameters using validation set\n30\t0.8\ttest error rate improvement compared to state-of-the-art\n30\t0.7\ttraining with translation and horizontal flipping augmentation",
        "sim": "dependent paper\tsource paper\timportance weight\tcontent of the relation\n29\t30\t1\tsame benchmark dataset (CIFAR-10)\n29\t30\t0.8\tapply global contrast normalization and ZCA whitening\n29\t30\t0.7\tuse the last 10,000 images of the training set as validation data\n29\t30\t0.6\tset same number of feature maps for each mlpconv layer\n29\t30\t0.5\tachieve lower test error compared to state-of-the-art"
    },
    "(29, 33)": {
        "diff": "paper\timportance weight\tthe important aspect\n29\t1\tNIN evaluated on four benchmark datasets\n29\t0.5\tDetailed settings of the parameters provided in supplementary materials\n29\t0.5\tTraining process starts from initial weights and learning rates\n29\t0.5\tLearning rate is lowered by a scale of 10 after accuracy stops improving\n33\t1\tSame network structure as CIFAR-10 adopted for MNIST dataset\n33\t0.5\tFewer feature maps generated from each mlpconv layer\n33\t0.5\tComparison with previous works shown in Table 4\n33\t0.5\tAchieving comparable but not better performance than the current best",
        "sim": "dependent paper\tsource paper\timportance weight\tcontent of the relation\n29\t33\t1\tThe same network structure is adopted\n29\t33\t0.8\tThe numbers of feature maps generated are reduced\n29\t33\t0.7\tMNIST is a simpler dataset\n29\t33\t0.6\tThe result is compared with previous works\n29\t33\t0.5\tComparable but not better performance achieved"
    },
    "(30, 31)": {
        "diff": "paper\timportance weight\tthe important aspect\n30\t1\tThe paper discusses the use of dropout in between the mlpconv layers in the NIN network, which improves the generalization ability of the model.\n30\t0.9\tThe paper achieves a test error rate of 10.41% on the CIFAR-10 dataset, which is an improvement compared to the state-of-the-art methods.\n30\t0.8\tThe paper compares different methods for the CIFAR-10 dataset, including Stochastic Pooling, CNN + Spearmint, Conv. maxout + Dropout, NIN + Dropout, CNN + Spearmint + Data Augmentation, Conv. maxout + Dropout + Data Augmentation, and DropConnect + 12 networks + Data Augmentation. \n31\t1\tThe paper focuses on the CIFAR-100 dataset, which contains 100 classes and is similar in size and format to the CIFAR-10 dataset.\n31\t0.9\tThe paper achieves a test error rate of 35.68% on the CIFAR-100 dataset, surpassing the current best performance without data augmentation.\n31\t0.8\tThe paper compares different methods for the CIFAR-100 dataset, including Learned Pooling, Stochastic Pooling, Conv. maxout + Dropout, Tree based priors, and NIN + Dropout.",
        "sim": "dependent paper\tsource paper\timportance weight\tcontent of the relation\n30\t31\t0.7\tThe networks were tested on CIFAR-100 and CIFAR-10 datasets\n30\t31\t0.65\tThe test error rate for CIFAR-100 is 35.68%\n30\t31\t0.6\tBoth datasets have similar size and format\n30\t31\t0.55\tThe number of images in CIFAR-100 is one tenth of CIFAR-10\n30\t31\t0.5\tThe last mlpconv layer in CIFAR-100 outputs 100 feature maps"
    },
    "(30, 32)": {
        "diff": "paper\timportance weight\tthe important aspect\n30\t1\tThe CIFAR-10 dataset consists of 10 classes of natural images\n30\t0.8\tThe model achieves a test error of 10.41% on the CIFAR-10 dataset\n30\t0.7\tDropout layers in between the mlpconv layers improve the generalization ability of the model\n30\t0.6\tThe model with dropout achieves an error rate of 14.51% for the CIFAR-10 dataset\n32\t1\tThe SVHN dataset consists of 630,420 32x32 color images\n32\t0.8\tThe model achieves a test error rate of 2.35% on the SVHN dataset\n32\t0.7\tThe SVHN dataset focuses on classifying the digit located at the center of each image\n32\t0.6\tThe model with dropout achieves a test error rate of 2.35% on the SVHN dataset",
        "sim": "dependent paper\tsource paper\timportance weight\tcontent of the relation\n30\t32\t0.7\tCIFAR-10 dataset compared to SVHN dataset\n30\t32\t0.6\tTest error rates for CIFAR-10 and SVHN\n30\t32\t0.8\tBoth datasets have similar training and testing procedures\n30\t32\t0.7\tCIFAR-10 achieves a test error rate of 10.41%, while SVHN achieves 2.35%\n30\t32\t0.6\tState-of-the-art performance for CIFAR-10 is 10.41% and for SVHN is 2.35%"
    },
    "(30, 33)": {
        "diff": "paper\timportance weight\tthe important aspect\n30\t1\tThe paper discusses the CIFAR-10 dataset.\n30\t1\tThe paper applies global contrast normalization and ZCA whitening for preprocessing.\n30\t0.5\tThe paper tunes hyper-parameters including local receptive field size and weight decay.\n30\t1\tThe paper achieves a test error of 10.41% on the CIFAR-10 dataset.\n33\t1\tThe paper discusses the MNIST dataset.\n33\t1\tThe paper uses the same network structure as CIFAR-10 but with reduced feature maps.\n33\t0.5\tThe paper achieves a test error of 0.47% on the MNIST dataset.\n33\t0.5\tThe paper achieves comparable but not better performance than the current best on MNIST.",
        "sim": "dependent paper\t source paper\t importance weight\t content of the relation\n30\t33\t1\tadopted network structure\tfrom CIFAR-10 to MNIST\n30\t33\t0.8\treduced number of feature maps\t\n30\t33\t0.6\tused convolutional structures\t\n30\t33\t0.5\ttested without data augmentation\t\n30\t33\t0.5\tcomparable performance in error rate"
    },
    "(33, 31)": {
        "diff": "paper\timportance weight\tthe important aspect\n33\t0.5\tMNIST dataset consists of hand written digits 0-9\n33\t0.5\tFewer parameters are needed for MNIST compared to CIFAR-10\n33\t1\tComparison with previous works on MNIST dataset\n33\t0.7\tPerformance achieved on MNIST dataset is 0.47%\n31\t0.5\tCIFAR-100 dataset contains 100 classes\n31\t0.5\tNumber of images in each class is one tenth of CIFAR-10 dataset\n31\t1\tComparison with previous works on CIFAR-100 dataset\n31\t0.8\tPerformance achieved on CIFAR-100 dataset is 35.68%",
        "sim": "dependent paper\t source paper\t importance weight\t content of the relation\n33\t31\t0.8\tComparison of performance on different datasets\n33\t31\t0.7\tBoth papers use convolutional structures\n33\t31\t0.6\tDifferent number of classes in the datasets\n33\t31\t0.9\tBoth papers mention the use of dropout\n33\t31\t0.8\tTest error rates mentioned in both papers"
    },
    "(33, 32)": {
        "diff": "paper\timportance weight\tthe important aspect\n33\t0.5\tMNIST dataset consists of hand written digits\n33\t0.5\tTest set error rates for MNIST of various methods\n33\t0.7\tNumbers of feature maps generated from each mlpconv layer are reduced in MNIST dataset\n33\t1\tMNIST dataset has been tuned to a very low error rate\n32\t0.5\tSVHN dataset is composed of color images\n32\t0.5\tTest set error rates for SVHN of various methods\n32\t0.8\tMulti-digit Number Recognition method achieves a test error rate of 2.16% in SVHN dataset\n32\t0.7\tPreprocessing of the SVHN dataset includes local contrast normalization",
        "sim": "dependent paper\tsource paper\timportance weight\tcontent of the relation\n33\t32\t0.6\tMNIST dataset compared with SVHN dataset\n33\t32\t0.8\tSimilar network structure used for both datasets\n33\t32\t0.7\tDifferent number of feature maps for each mlpconv layer\n33\t32\t0.6\tComparison of test error rates in Table 4 and Table 3\n33\t32\t0.5\tComparable performance achieved for both datasets"
    },
    "(37, 38)": {
        "diff": "paper\timportance weight\tthe important aspect\n37\t1\tThe paper asks a fundamental question\n38\t1\tThe paper addresses the question of whether neural machine translation can be done directly on a sequence of characters\n37\t0.5\tMachine translation systems have relied exclusively on word-level modelling\n38\t0.5\tMachine translation systems have relied exclusively on word-level modelling, mainly due to the issue of data sparsity\n37\t0.5\tThe existing machine translation systems are phrase-based or neural\n38\t0.5\tNeural machine translation is a more recently proposed paradigm",
        "sim": "dependent paper\tsource paper\timportance weight\tcontent of the relation\n37\t38\t1\tThe existing machine translation systems rely on word-level modeling.\n37\t38\t0.9\tThe existing machine translation systems make translation as mapping from a sequence of source-language words to a sequence of target-language words.\n37\t38\t0.85\tThe existing machine translation systems suffer from data sparsity.\n37\t38\t0.8\tThe existing machine translation systems can be done directly on a sequence of characters without any explicit word segmentation.\n37\t38\t0.75\tThe existing machine translation systems benefit from translating at the character-level."
    },
    "(37, 42)": {
        "diff": "paper\timportance weight\tthe important aspect\n37\t1\tWord-level modelling\n37\t0.8\tExplicit segmentation\n37\t0.6\tGeneralization to novel words\n37\t0.7\tInefficiency in modelling\n\n42\t1\tCharacter-level modelling\n42\t0.8\tParametric approach based on recurrent neural networks\n42\t0.6\tAbility to capture long-term dependencies\n42\t0.7\tAvoiding data sparsity in character-level translation",
        "sim": "37\t42\t1\tThe papers discuss machine translation systems\n37\t42\t1\tThe papers mention word-level processing\n37\t42\t0.9\tThe papers mention the issue of word segmentation algorithm\n37\t42\t0.8\tThe papers discuss the use of rule-based tokenization approach\n37\t42\t0.7\tThe papers mention the problem of data sparsity in character-level translation"
    },
    "(37, 50)": {
        "diff": "paper\timportance weight\tthe important aspect\n37\t0.5\tmachine translation systems, whether phrase-based or neural, have relied on word-level modelling\n37\t0.5\texplicit segmentation\n37\t0.5\tasks a fundamental question\n50\t0.5\tneural machine translation can handle translation at the level of characters without word segmentation\n50\t0.5\tdecoder generates one character at a time\n50\t0.5\tsoft-aligning between target character and source subword\n50\t0.5\tpossible and beneficial for neural machine translation to translate at character level",
        "sim": "Here is the output in csv format with delimiter=\"   \":\n\ndependent paper\tsource paper\timportance weight\tcontent of the relation\n37\t50\t0.8\texisting machine translation systems\n37\t50\t0.7\texplicit segmentation\n37\t50\t0.6\task a fundamental question\n37\t50\t0.6\tneural machine translation system\n37\t50\t0.7\tdirectly handle translation at the level of characters"
    },
    "(38, 42)": {
        "diff": "paper\timportance weight\tthe important aspect\n38\t  0.75\t      character-level modelling  \n38\t  0.85\t      addressing data sparsity in character-level translation  \n38\t  1.00\t      evaluating neural machine translation models with a character-level decoder \n38\t  0.90\t      using a novel recurrent neural network (RNN) called bi-scale recurrent network  \n\n42\t  0.75\t      inefficiency in modelling unsegmented words  \n42\t  0.85\t      inability to generalize well to novel words\n42\t  1.00\t      difficulty in translating rare morphological variants  \n42\t  0.90\t      lack of an optimal segmentation algorithm",
        "sim": "dependent paper\tsource paper\timportance weight\tcontent of the relation\n42\t38\t1\tThe source paper discusses issues with word-level translation.\n38\t42\t1\tThe dependent paper addresses whether neural machine translation can be done directly on a sequence of characters without word segmentation.\n42\t38\t1\tThe source paper mentions the issues of data sparsity and increased computational complexity in word-level modelling.\n38\t42\t0.7\tThe dependent paper evaluates neural machine translation models with a character-level decoder on four language pairs.\n42\t38\t0.5\tThe source paper suggests that neural machine translation can learn to translate at the character-level and benefit from doing so."
    },
    "(38, 47)": {
        "diff": "paper\timportance weight\tthe important aspect\n38\t1\tneural machine translation at character-level\n38\t0.75\tcharacter-level decoder outperformed subword-level decoder\n38\t0.6\tbi-scale recurrent network handles multiple timescales\n38\t0.5\tneural networks suffer from word-level modeling issues\n47\t1\tsequence of subword-based symbols as target sentence representation\n47\t0.75\ttokenization script included in Moses is used for corpus tokenization\n47\t0.6\tbeam search used for finding most likely translation\n47\t0.5\tensemble of neural machine translation models compared to state-of-the-art phrase-based systems",
        "sim": "dependent paper\tsource paper\timportance weight\tcontent of the relation\n38\t47\t1\tBoth papers discuss machine translation systems\n38\t47\t0.8\tPaper 38 proposes neural machine translation directly on character sequences\n38\t47\t0.7\tPaper 47 uses byte-pair encoding to represent source and target sentences\n38\t47\t0.6\tBoth papers experiment with different decoding strategies\n38\t47\t0.5\tBoth papers evaluate translation performance using BLEU scores"
    },
    "(38, 48)": {
        "diff": "paper\timportance weight\tthe important aspect\n38\t1\tneural machine translation can be done directly on a sequence of characters without any explicit word segmentation\n38\t0.8\tmodels with a character-level decoder outperform the ones with a subword-level decoder\n38\t0.8\tneural machine translation at the character-level surpasses the translation quality of word-level translation\n38\t0.7\tusing a character-level decoder benefits from aligning a larger chunk in the target with a subword unit in the source\n48\t1\tusing the slower layer (h2) for the soft-alignment mechanism improves the performance of the character-level decoder\n48\t0.8\tcharacter-level decoders outperform the subword-level decoder in all language pairs\n48\t0.7\tneural machine translation performs comparably to, or often better than, the state-of-the-art non-neural translation system\n48\t0.6\tthe effectiveness of character-level modeling is validated by the performance of the character-level base decoder",
        "sim": "dependent paper\tsource paper\timportance weight\tcontent of the relation\n48\t38\t0.8\tslower layer used for soft-alignments\n48\t38\t0.75\tsoft-alignment mechanism benefits from aligning larger chunks in the target\n48\t38\t0.8\tcharacter-level decoder outperforms subword-level decoder for En-Cs and En-Fi\n48\t38\t0.75\tcharacter-level base decoder outperforms subword-level decoder and character-level bi-scale decoder for En-De\n48\t38\t0.7\tcharacter-level decoders outperform subword-level decoder for En-Ru"
    },
    "(38, 49)": {
        "diff": "paper\timportance weight\tthe important aspect\n38\t1\tThe paper addresses the question of whether neural machine translation can be done directly on a sequence of characters without any explicit word segmentation.\n38\t0.5\tThe paper evaluates neural machine translation models with a character-level decoder on four language pairs from WMT\u201915 and compares the performance with a subword-level decoder.\n38\t0.5\tThe paper proposes a novel recurrent neural network called bi-scale recurrent network that better handles multiple timescales in a sequence and tests its performance.\n38\t0.5\tThe paper demonstrates that models with a character-level decoder outperformed the ones with a subword-level decoder in terms of translation quality on all of the four language pairs.\n49\t1\tThe character-level decoder can generate long, coherent sentences even though the translations in characters are generally 5-10 times longer than translations in words.\n49\t0.5\tThe character-level decoder helps with rare words by better modeling the composition of any character sequence, leading to a growing gap in the average negative log-probability of words between the subword-level and character-level decoders as word frequency decreases.\n49\t0.5\tThe character-level decoder exhibits soft-alignment between a source word and a target character, which allows the model to correctly align and generate compound words or meaningful chunks of characters in the translation.\n49\t0.5\tThe character-level decoder has a decoding speed of 27.5 words per second for the base decoder and 25.6 words per second for the bi-scale decoder, which is slightly slower than the subword-level base decoder.",
        "sim": "dependent paper\tsource paper\timportance weight\tcontent of the relation\n38\t49\t0.8\tgenerate long, coherent sentence\n38\t49\t0.9\thelp with rare words\n38\t49\t0.7\tsoft-align between source word and target character\n38\t49\t0.6\tdecoding speed of character-level decoder\n38\t49\t0.8\tcan learn to translate at character-level"
    },
    "(38, 50)": {
        "diff": "paper\timportance weight\tthe important aspect\n38\t1\tThe paper focuses on the issue of data sparsity when representing sentences as sequences of characters rather than words.\n38\t0.8\tThe paper addresses the question of whether neural machine translation can be done directly on a sequence of characters without any explicit word segmentation.\n38\t0.7\tThe paper evaluates neural machine translation models with a character-level decoder on four language pairs from WMT\u201915.\n38\t0.6\tThe paper introduces a novel recurrent neural network called bi-scale recurrent network for handling multiple timescales in a sequence.\n\n50\t1\tThe paper suggests that neural machine translation can translate at the level of characters without any word segmentation.\n50\t0.8\tThe paper experiments with a decoder generating one character at a time and soft-aligning between target characters and source subwords.\n50\t0.7\tThe paper finds that it is possible for neural machine translation to benefit from translating at the character level.\n50\t0.6\tThe paper acknowledges the limitation of using subword symbols in the source side and suggests investigating a setting where the source side is also represented as a character sequence.",
        "sim": "dependent paper\tsource paper\timportance weight\tcontent of the relation\n38\t50\t1\tneural machine translation at character-level\t\n38\t50\t0.9\tneural machine translation benefits from character-level\t\n38\t50\t0.8\tneural machine translation can handle characters\t\n38\t50\t0.7\tneural machine translation soft-align between characters\t\n38\t50\t0.6\tneural machine translation without word segmentation"
    },
    "(42, 41)": {
        "diff": "paper\timportance weight\tthe important aspect\n42\t1\tWord-level processing not optimal\n42\t0.8\tInef\ufb01ciency in modeling morphology variants\n42\t0.7\tDif\ufb01culty in segmenting words accurately\n42\t0.6\tDif\ufb01culty in generalizing to novel words\n41\t1\tCharacter-level modeling avoids morphological variants\n41\t0.8\tAddressing data sparsity in character-level translation\n41\t0.7\tDif\ufb01culty in mapping word's character sequence to meaning\n41\t0.6\tDif\ufb01culty in modeling long-term dependencies.",
        "sim": "dependent paper\tsource paper\timportance weight\tcontent of the relation\n42\t41\t0.8\tmost pressing issue with word-level processing \n42\t41\t0.7\tdoes not have a perfect word segmentation algorithm\n42\t41\t0.6\trequires decades of research\n42\t41\t0.7\topt to using either a rule-based tokenization approach or a suboptimal, but still available, learning based segmentation algorithm\n42\t41\t0.6\tprevents any machine translation system from modeling these morphological variants effectively"
    },
    "(42, 49)": {
        "diff": "paper\timportance weight\tthe important aspect\n42\t1\tWord-level processing has imperfect segmentation algorithm for any language.\n42\t0.8\tVocabulary is inefficiently filled with similar words due to suboptimal tokenization.\n42\t0.7\tTranslation system cannot generalize well to novel words.\n42\t0.6\tRare morphological variants in training corpus are not well translated.\n49\t1\tCharacter-level translation generates coherent sentences.\n49\t0.8\tCharacter-level modeling better handles rare morphological variants.\n49\t0.7\tCharacter-level decoder can soft-align between source words and target characters.\n49\t0.6\tCharacter-level decoding speed is slightly slower than subword-level decoding.",
        "sim": "dependent paper\tsource paper\timportance weight\tcontent of the relation\n42\t49\t0.8\tlong, coherent sentence generation\n42\t49\t0.7\trare word modeling\n42\t49\t0.9\tcharacter-level soft-alignment\n42\t49\t0.6\tdecoing speed of character-level decoder"
    },
    "(42, 50)": {
        "diff": "paper\timportance weight\tthe important aspect\n42\t1\tWord-level processing and segmentation algorithm\n42\t1\tinefficiency in modeling and generalization to novel words\n42\t1\tlarge number of morphological variants and vocabulary size\n42\t1\tword-level translation disadvantages and need for optimal segmentation\n50\t1\tNeural machine translation at the character level\n50\t1\tBenefits of translating at the character level\n50\t1\tExperiments on four language pairs\n50\t1\tLimitation of using subword symbols in the source side",
        "sim": "dependent paper\tsource paper\timportance weight\tcontent of the relation\n50\t42\t1\ttranslate at character level\n50\t42\t0.9\tbenefits from translating at character level\n50\t42\t0.8\tdecoder generates one character at a time\n50\t42\t0.7\tsoft-aligning between target character and source subword\n50\t42\t0.6\tpossible to handle translation without word segmentation"
    },
    "(50, 48)": {
        "diff": "paper\t importance weight\t the important aspect\n50\t 0.8\t Neural machine translation at the character-level\n50\t 0.7\t Experimental results on four language pairs\n50\t 0.6\t Use of subword symbols in the source side\n50\t 0.5\t Acknowledgments and research funding support\n\n48\t 0.8\t Use of slower layer for soft-alignments\n48\t 0.7\t Comparison of different decoders for translation quality\n48\t 0.6\t Possibility of character-level translation without explicit segmentation\n48\t 0.5\t Ensemble models outperforming non-neural translation systems",
        "sim": "dependent paper\tsource paper\timportance weight\tcontent of the relation\n50\t48\t0.8\ttranslate at the level of characters\t\n50\t48\t0.7\tpossible for neural machine translation\t\n50\t48\t0.6\tfocused on the target side\t\n50\t48\t0.5\tbenefits from doing so\t\n50\t48\t0.5\tbetter than word-level translation"
    },
    "(50, 49)": {
        "diff": "paper\timportance weight\tthe important aspect\n50\t0.8\tneural machine translation at character level\t\n50\t0.9\tbenefits of translating at character level\t\n50\t0.6\tfine-grained analysis using subword symbols\t\n50\t0.7\timportance of investigating character sequence representation\t\n49\t0.8\tcoherence of sentence generation at character level\t\n49\t0.7\timproved modelling of rare morphological variants\t\n49\t0.9\tsuccess of character-level decoding\t\n49\t0.6\tcorrect soft-alignment between source word and target character\t\n49\t0.7\tcomparative decoding speed of different decoders",
        "sim": "dependent paper\tsource paper\timportance weight\tcontent of the relation\n50\t49\t0.8\taddr\tneural machine translation at character level can handle translation\n50\t49\t0.7\tbene\tneural machine translation benefits from translating at character level\n50\t49\t0.6\tlimitation\tsubword symbols used in the source side\n50\t49\t0.5\tanaly\tfine-grained analysis allowed with subword symbols\n50\t49\t0.5\tinv\tneed to investigate representing source side as a character sequence"
    },
    "(47, 48)": {
        "diff": "paper\timportance weight\tthe important aspect\n47\t0.5\tsource sentence representation with byte-pair encoding\n47\t0.6\tutilizing parallel corpora from WMT'15 for evaluation\n47\t0.7\ttokenization of target side corpora to subword symbols or characters\n47\t0.8\tuse of all available parallel corpora for En-Cs, En-De, En-Ru, and En-Fi language pairs\n\n48\t0.5\tchoice of slower layer (h2) for soft-alignment mechanism\n48\t0.6\timprovement in soft-alignment mechanism with slower layer (h2)\n48\t0.7\tsignificant improvement of character-level base decoder over subword-level decoder for En-De\n48\t0.8\tcomparable performance of single models for En-Ru and character-level decoders for other language pairs",
        "sim": "dependent paper\tsource paper\timportance weight\tcontent of the relation\n47\t48\t1\tSimilarities between papers 47 and 48\n47\t48\t0.8\tBoth papers discuss evaluation methods\n47\t48\t0.7\tBoth papers use parallel corpora for training\n47\t48\t0.6\tBoth papers use tokenization for preprocessing\n47\t48\t0.5\tBoth papers use recurrent neural networks for modeling."
    },
    "(48, 49)": {
        "diff": "paper\timportance weight\tthe important aspect\n48\t1.0\tSlower layer improves soft-alignment in character-level decoder\n48\t0.8\tCharacter-level decoder outperforms subword-level decoder for En-Cs and En-Fi\n48\t0.6\tCharacter-level decoders outperform subword-level decoder in En-Ru\n48\t0.7\tCharacter-level translation often surpasses word-level translation quality\n49\t1.0\tCharacter-level decoder generates coherent sentences in characters\n49\t0.9\tCharacter-level decoder better models rare morphological variants\n49\t0.8\tCharacter-level decoder can soft-align between source words and target characters\n49\t0.7\tCharacter-level decoding speed is slower than subword-level decoding",
        "sim": "Output:\n\ndependent paper\tsource paper\timportance weight\tcontent of the relation\n48\t49\t1\tThe character-level decoder generates coherent sentences in characters\n48\t49\t1\tThe character-level decoder helps with rare words\n48\t49\t0.75\tThe character-level decoder soft-aligns between a source word and a target character\n48\t49\t0.75\tThe decoding speed of the character-level decoder is slower compared to the subword-level decoder\n48\t49\t0.5\tThe character-level decoder captures meaningful chunks of characters in the source"
    }
}