{
    "summary": "# Developing a Scalable Benchmark for Assessing Large Language Models in Knowledge Graph Engineering\n\n**Abstract**\n\nThe paper introduces a benchmarking framework called LLM-KG-Bench for assessing Large Language Models (LLMs) in the context of knowledge graph engineering (KGE). The framework includes three challenges related to syntax and error correction, facts extraction, and dataset generation. The paper shows that while LLMs are useful tools, they are currently unfit for zero-shot prompting in knowledge graph generation. The LLM-KG-Bench framework provides automatic evaluation and storage of LLM responses, as well as statistical data and visualization tools to track prompt engineering and model performance.\n\n**Keywords**\n\nLarge Language Model, Knowledge Graph Engineering, Large Language Model Benchmark\n\n## 1. Introduction\n\nThe introduction states that Large Language Models (LLMs), such as GPT-3 and GPT-4, have shown proficiency in solving textual assignments and have sparked advancements in prompt engineering. However, due to the fast evolution of LLMs, it is challenging to keep track of their capabilities and choose the best model and prompt for specific tasks. While there are existing benchmarks for LLMs, their application and assessment in the context of knowledge graph engineering (KGE) and the Semantic Web are under-explored. To address this gap, the paper proposes the LLM-KG-Bench framework for benchmarking LLMs in KGE scenarios.\n\n## 2. Related Work\n\nThis section highlights previous work on the utilization of LLMs in the semantic web domain and the significance of combining LLMs with knowledge graphs (KGs). It mentions the Knowledge Base Construction from Pre-trained Language Models (LM-KBC) Challenge and other community efforts that assess LLM performance on various tasks. However, these existing benchmarks lack specific scoring for KGE tasks and do not evaluate scores relative to problem size, which is relevant in KGE due to the large size of KGs.\n\n## 3. The LLM-KG-Bench Framework\n\nThe paper describes the design and implementation of the modular LLM-KG-Bench framework for benchmarking LLMs in knowledge graph engineering. The framework focuses on automated evaluation procedures and supports configurable task sizing based on prior work suggesting the relevance of the LLM's context size for KGE tasks. The framework is organized around benchmark tasks and LLM model connectors, connected by code for execution organization and result persistence. LLM model connectors encapsulate the connection to a specific LLM and offer a function for generating text. Benchmark tasks handle the evaluation of LLMs for specific tasks and provide score values and additional information. The framework also supports result visualization using the seaborn library.\n\n## 4. Initial Evaluation of the Framework with First Tasks\n\nTo test the LLM-KG-Bench framework, the paper presents an evaluation using three high-ranking LLMs. The evaluation includes three benchmark tasks: fixing errors in Turtle files, KG creation from factsheet plaintext, and synthetic dataset generation. The results of the evaluation show the F1 scores for the Turtle fixing task, the F1 scores for the KG creation task, and the mean error in dataset generation. The evaluation demonstrates the benefit of the LLM-KG-Bench framework for automated evaluation.\n\n## 5. Conclusion and Future Work\n\nThe paper concludes that there is a need for measuring the knowledge graph engineering capabilities of LLMs, and the LLM-KG-Bench framework addresses this need. The framework provides automated evaluation and storage of LLM responses, as well as statistical data and visualization tools. Future work includes evaluating LLMs' capabilities to fix their answers with feedback and expanding the framework to enable dialogs between benchmark tasks and LLMs.\n\n## Acknowledgments\n\nThe work was partially supported by grants from the German Federal Ministry for Economic Affairs and Climate Action (BMWK) and the German Federal Ministry of Education and Research (BMBF).\n\n## References\n\nThe paper provides references to related works, including technical reports and research papers, on the topic of LLMs and knowledge graph engineering.\n\n## A. Online Resources\n\nThe paper includes links to the LLM-KG-Bench repository and experiment data for further exploration and access to the framework.",
    "keywords": [
        "Large Language Model Benchmark",
        "Large Language Model",
        "Knowledge Graph Engineering"
    ],
    "keywords_explanations": {
        "Large Language Model": "A large language model (LLM) refers to advanced natural language processing models, such as GPT-3 and GPT-4. These models have proven to be effective in understanding and generating text. In the context of knowledge graph engineering (KGE), a benchmarking framework called LLM-KG-Bench has been developed to evaluate the performance of LLMs. The framework includes challenges related to syntax and error correction, facts extraction, and dataset generation. It aims to assess the capabilities of LLMs in generating accurate and relevant information in the context of knowledge graphs. The LLM-KG-Bench framework provides automated evaluation, storage of LLM responses, and statistical visualization tools to track prompt engineering and model performance.",
        "Large Language Model Benchmark": "The 'Large Language Model Benchmark' (LLM-KG-Bench) is a benchmarking framework introduced in this paper. It aims to assess the performance of Large Language Models (LLMs) in the field of knowledge graph engineering (KGE). The framework consists of three challenges related to syntax and error correction, facts extraction, and dataset generation. It provides automatic evaluation of LLM responses, storage of results, and statistical data and visualization tools for tracking prompt engineering and model performance. The paper demonstrates the framework's effectiveness through an initial evaluation using three benchmark tasks. The LLM-KG-Bench framework addresses the need for evaluating the capabilities of LLMs in KGE and provides a scalable and comprehensive benchmarking solution.",
        "Knowledge Graph Engineering": "Knowledge Graph Engineering refers to the process of creating, managing, and utilizing knowledge graphs. Knowledge graphs are structured representations of knowledge that organize information in a way that is machine-readable and can be used for various applications, such as search engines, recommendation systems, and question answering. Knowledge Graph Engineering involves tasks such as data extraction, data integration, ontology design, and knowledge graph reasoning. The paper introduces a benchmarking framework called LLM-KG-Bench for evaluating the performance of Large Language Models (LLMs) in the context of knowledge graph engineering. This framework provides standardized tasks and evaluation metrics to assess the capabilities of LLMs in tasks related to syntax and error correction, facts extraction, and dataset generation. The goal is to understand the strengths and limitations of LLMs in knowledge graph engineering and guide their development and application in this field."
    }
}