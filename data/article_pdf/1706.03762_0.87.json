{
    "(0, 1)": {
        "diff": "paper\t importance weight\t the important aspect\n0\t1\tComplex recurrent and convolutional neural networks are not used in Transformer\n0\t0.8\tTransformer achieves superior quality in machine translation\n0\t0.7\tTransformer is more parallelizable and requires less training time\n0\t0.6\tTransformer achieves better results than existing models in English-to-German and English-to-French translation tasks\n1\t1\tRecurrent neural networks are not used in Transformer\n1\t0.9\tTransformer relies entirely on attention mechanism to draw global dependencies between input and output\n1\t0.8\tTransformer allows for significantly more parallelization\n1\t0.7\tTransformer can achieve state-of-the-art translation quality in just twelve hours training time.",
        "sim": "dependent paper\t source paper\t importance weight\t content of the relation\n0\t1\t0.8\tdominant sequence transduction models\n0\t1\t0.7\tcomplex recurrent or convolutional neural networks\n0\t1\t0.6\tencoder and decoder\n0\t1\t0.5\tattention mechanism\n0\t1\t0.5\tparallelizable and requiring significantly less time to train"
    },
    "(0, 2)": {
        "diff": "paper\t importance weight\t the important aspect\n0\t1\tThe Transformer architecture, improved translation quality, parallelizable training, reduced training time\n0\t0.8\t28.4 BLEU score on the WMT 2014 English-to-German translation task, outperforming existing best results\n0\t0.7\t41.8 BLEU score on the WMT 2014 English-to-French translation task, establishing a new single-model state-of-the-art\n0\t0.6\tSuccessful application to English constituency parsing with both large and limited training data\n2\t1\tUse of convolutional neural networks as basic building blocks for reducing sequential computation\n2\t0.8\tAbility to relate signals from two arbitrary positions with constant number of operations\n2\t0.7\tUse of self-attention for computing representations of input and output without using sequence-aligned RNNs or convolution\n2\t0.6\tSelf-attention used successfully in various tasks including reading comprehension and text summarization",
        "sim": "dependent paper\tsource paper\timportance weight\tcontent of the relation\r\n0\t2\t1\tattention mechanisms in Transformer\r\n0\t2\t0.7\tTransformer as a simple network architecture\r\n0\t2\t0.6\tTransformer outperforms models with recurrence and convolutions\r\n0\t2\t0.8\tTransformer achieves high BLEU scores\r\n0\t2\t0.5\tTransformer generalizes well to other tasks"
    },
    "(0, 11)": {
        "diff": "paper\timportance weight\tthe important aspect\n0\t1\tTransformer architecture\tbased solely on attention mechanisms\n0\t0.9\tParallelizable and less time to train\n0\t0.8\t28.4 BLEU on WMT 2014 English-to-German task\n0\t0.7\t41.8 BLEU on WMT 2014 English-to-French task\n11\t1\tEncoder-decoder structure\n11\t0.9\tStacked self-attention and point-wise, fully connected layers\n11\t0.8\tAuto-regressive model\n11\t0.7\tTransforms input sequence of symbol representations to continuous representations",
        "sim": "dependent paper\tsource paper\timportance weight\tcontent of the relation\n0\t11\t0.5\tbased on complex recurrent or convolutional neural networks\n0\t11\t0.6\tincludes an encoder and a decoder\n0\t11\t0.7\tconnects the encoder and decoder through an attention mechanism\n0\t11\t0.8\tproposes a new simple network architecture, the Transformer\n0\t11\t0.9\tdispenses with recurrence and convolutions entirely"
    },
    "(0, 16)": {
        "diff": "paper\timportance weight\tthe important aspect\n0\t0.5\ttransformer architecture\n0\t0.7\tattention mechanism\n0\t0.9\tparallelizable and less time to train\n0\t1\tbetter BLEU scores than previous models\n16\t0.5\tresidual dropout\n16\t0.7\tlabel smoothing\n16\t0.9\timproved accuracy and BLEU score\n16\t1\tregularization techniques during training",
        "sim": "dependent paper\tsource paper\timportance weight\tcontent of the relation\n0\t16\t1\tachieves better BLEU scores\t\n0\t16\t1\tprevious state-of-the-art models\t\n0\t16\t1\trequires significantly less time\t\n0\t16\t1\ttraining costs of the best models\t\n0\t16\t1\tapplies dropout to the output"
    },
    "(0, 18)": {
        "diff": "paper\timportance weight\tthe important aspect\n0\t1\tThe Transformer architecture\n0\t0.8\tMore parallelizable and less time to train\n0\t0.7\tAchieves 28.4 BLEU on English-to-German translation task\n0\t0.6\tAchieves 41.8 BLEU on English-to-French translation task\n18\t1\tThe big Transformer model\n18\t0.9\tSurpasses previously published models and ensembles\n18\t0.8\tAchieves 28.4 BLEU on English-to-German translation task\n18\t0.7\tAchieves 41.0 BLEU on English-to-French translation task",
        "sim": "dependent paper\tsource paper\timportance weight\tcontent of the relation\n18\t0\t1\tTransformer outperforms existing models by 2 BLEU\n18\t0\t0.9\tTraining cost fraction of competitive models\n18\t0\t0.7\tBLEU score improvement on English-to-German task\n18\t0\t0.6\tBLEU score improvement on English-to-French task\n18\t0\t0.5\tBase model surpasses previously published models"
    },
    "(0, 19)": {
        "diff": "paper\t importance weight\t the important aspect\n0\t1.0\tTransforming network architecture\n0\t0.75\tMore parallelizable and faster training\n0\t0.75\tBetter translation quality (BLEU score)\n0\t0.5\tGeneralizability to other tasks (English constituency parsing)\n\n19\t1.0\tImportance evaluation of Transformer components\n19\t0.75\tEffect of varying attention heads and dimensions\n19\t0.75\tEffect of reducing attention key size (dk)\n19\t0.5\tImportance of model size and dropout in improving quality\n19\t0.5\tReplacement of sinusoidal positional encoding with learned positional embeddings",
        "sim": "dependent paper\tsource paper\timportance weight\tcontent of the relation\n19\t0\t0.9\tdifferent components of the Transformer\n19\t0\t0.9\tmeasuring the change in performance\n19\t0\t0.9\tbetter models with bigger size\n19\t0\t0.9\tavoiding over-fitting with dropout\n19\t0\t0.9\tpositonal encoding vs learned embeddings"
    },
    "(0, 20)": {
        "diff": "paper\timportance weight\tthe important aspect\n0\t1\tEncoder-decoder based models replaced\tAttention mechanism and parallelization\n0\t0.8\tSuperior quality and faster training\t28.4 BLEU on English-to-German, 41.8 BLEU on English-to-French\n0\t0.7\tGeneralizes well to other tasks\tSuccessful application to English constituency parsing\n20\t1\tTransformed outperforms RNN models\tThe Transformer performs better than RNN models for constituency parsing\n20\t0.8\tSuccessful performance without task-specific tuning\tThe model performs well without specific parameter adjustments\n20\t0.7\tOutperforms Berkeley-Parser\tThe Transformer model is better than the Berkeley-Parser\n20\t0.6\tBetter performance than all previously reported models\tThe model performs better than previously reported models\n20\t0.5\tTraining only on the WSJ training set\tThe model achieves good results even with limited training data.",
        "sim": "dependent paper\tsource paper\timportance weight\tcontent of the relation\n20\t0\t0.7\tperforms well on English constituency parsing\n20\t0\t0.6\toutperforms RNN sequence-to-sequence models\n20\t0\t0.5\tattains state-of-the-art results\n20\t0\t0.6\ttrained on Wall Street Journal dataset\n20\t0\t0.7\texperiments on small-data regimes"
    },
    "(0, 21)": {
        "diff": "paper\timportance weight\tthe important aspect\n0\t1.0\thigher BLEU score on English-to-German translation task\n0\t1.0\tlower training costs compared to existing models\n0\t0.8\tno recurrence or convolutions in the network architecture\n0\t0.7\tsuccessful application to English constituency parsing\n21\t1.0\tachieves state of the art on translation tasks\n21\t1.0\tpotential application to tasks involving non-text modalities\n21\t1.0\tplan to investigate local, restricted attention mechanisms\n21\t0.8\tfaster training compared to recurrent or convolutional layers",
        "sim": "dependent paper\tsource paper\timportance weight\tcontent of the relation\n0\t21\t0.8\tbased entirely on attention, replacing the recurrent layers\n0\t21\t0.8\tcan be trained significantly faster than architectures\n0\t21\t0.8\tachieve a new state of the art\n0\t21\t0.8\tplan to apply them to other tasks\n0\t21\t0.8\tplan to extend the Transformer to problems involving input and output modalities"
    },
    "(1, 2)": {
        "diff": "paper\timportance weight\tthe important aspect\n1\t1\tRecurrent neural networks and long short-term memory have been established as state of the art approaches in sequence modeling.\n1\t0.8\tRecurrent models factor computation and generate a sequence of hidden states.\n1\t0.7\tAttention mechanisms are used in conjunction with recurrent networks in sequence modeling.\n1\t0.6\tThe Transformer model architecture relies entirely on an attention mechanism for global dependencies between input and output.\n\n2\t1\tReducing sequential computation is the goal of the Extended Neural GPU, ByteNet, and ConvS2S models.\n2\t0.9\tConvolutional neural networks are used as the basic building block in the Extended Neural GPU, ByteNet, and ConvS2S models.\n2\t0.8\tSelf-attention is used to compute a representation of a single sequence in various tasks.\n2\t0.7\tThe Transformer is the first transduction model relying entirely on self-attention without using sequence-aligned RNNs or convolution.",
        "sim": "dependent paper\tsource paper\timportance weight\tcontent of the relation\n1\t2\t0.7\treducing sequential computation in goal\n1\t2\t0.8\tConvS2S, ByteNet, and Extended Neural GPU as previous models\n1\t2\t0.6\tconvolutional neural networks as basic building blocks\n1\t2\t0.9\tlinear and logarithmic growth of operations in ConvS2S and ByteNet models\n1\t2\t1.0\treduced effective resolution in the Transformer architecture"
    },
    "(1, 11)": {
        "diff": "paper\timportance weight\tthe important aspect\n1\t1\tthe paper introduces the Transformer model architecture\n1\t1\tthe Transformer relies entirely on an attention mechanism instead of recurrence\n1\t0.8\tthe Transformer allows for significantly more parallelization\n1\t0.6\tthe Transformer can achieve state-of-the-art translation quality with a relatively short training time\n11\t1\tthe paper discusses the encoder-decoder structure in sequence transduction models\n11\t1\tthe encoder maps symbol representations to continuous representations\n11\t0.8\tthe decoder generates output symbols one at a time based on the previously generated symbols\n11\t0.6\tthe model follows a stacked self-attention and point-wise, fully connected layers architecture",
        "sim": "dependent paper\tsource paper\timportance weight\tcontent of the relation\n1\t11\t0.7\tencoder-decoder structure\n1\t11\t0.6\tsequence transduction models\n1\t11\t0.6\tattention mechanism\n1\t11\t0.5\tneural sequence transduction models\n1\t11\t0.5\tcontinuous representations"
    },
    "(1, 12)": {
        "diff": "paper\timportance weight\tthe important aspect\n1\t0.5\trecurrent models factor computation along symbol positions\n1\t0.5\tlimitations of sequential computation\n1\t1\tTransformer architecture relies entirely on attention mechanism\n1\t0.5\tTransformer allows for more parallelization\n\n12\t0.5\ttotal computational complexity per layer\n12\t1\tcomparisons between self-attention and recurrent layers\n12\t0.5\tmaximum path length between input and output positions\n12\t1\tadvantages of self-attention in learning long-range dependencies",
        "sim": "dependent paper\tsource paper\timportance weight\tcontent of the relation\n1\t12\t1.0\trecurrent neural networks versus self-attention\n1\t12\t1.0\tlong short-term memory versus self-attention\n1\t12\t0.8\tsequential computation versus parallelization\n1\t12\t0.9\tattention mechanism with recurrent network\n1\t12\t0.6\trecurrent models and transformer architecture"
    },
    "(1, 21)": {
        "diff": "paper\timportance weight\tthe important aspect\n1\t1\tThe paper mentions recurrent neural networks as state-of-the-art approaches in sequence modeling and transduction problems.\n1\t1\tThe paper discusses the use of attention mechanisms in conjunction with recurrent networks.\n1\t1\tThe paper proposes the Transformer, a model architecture that relies entirely on an attention mechanism to draw global dependencies between input and output.\n1\t0.5\tThe paper mentions recent work that has achieved improvements in computational efficiency through factorization tricks and conditional computation.\n21\t1\tThe paper presents the Transformer, the first sequence transduction model based entirely on attention.\n21\t1\tThe paper mentions that the Transformer can be trained significantly faster than architectures based on recurrent or convolutional layers.\n21\t1\tThe paper achieves a new state of the art in translation quality for both English-to-German and English-to-French translation tasks.\n21\t0.5\tThe paper discusses plans to apply attention-based models to other tasks and extend the Transformer to handle inputs and outputs such as images, audio, and video.",
        "sim": "Dependent paper\tSource paper\tImportance weight\tContent of the relation\n21\t1\t1\tThe Transformer model architecture\n21\t1\t0.75\tThe Transformer can be trained significantly faster\n21\t1\t0.5\tThe Transformer achieves a new state of the art in translation quality\n21\t1\t0.5\tThe Transformer outperforms previously reported ensembles\n21\t1\t0.5\tAttention-based models are the future"
    },
    "(2, 7)": {
        "diff": "paper\t importance weight\t the important aspect\n2\t1\tsame input and output positions, reduced effective resolution\n2\t0.8\tuses convolutional neural networks as basic building blocks\n2\t0.6\tself-attention mechanism, no sequence-aligned RNNs or convolution\n2\t0.5\tperforms well on simple-language question answering and language modeling tasks\n7\t1\tencoder-decoder attention allows every position in the decoder to attend over all positions in the input sequence\n7\t0.8\tself-attention layers in the encoder allow each position to attend to all positions in the previous layer of the encoder\n7\t0.6\tmask out illegal connections in the decoder to preserve the auto-regressive property\n7\t0.5\tmulti-head attention is used in three different ways in the Transformer",
        "sim": "dependent paper\tsource paper\timportance weight\tcontent of the relation\n7\t2\t1\tencoder-decoder attention layers mimic typical attention mechanisms\n7\t2\t1\tmulti-head attention used in encoder self-attention layers\n7\t2\t1\tmulti-head attention used in decoder self-attention layers\n7\t2\t1\tpreventing leftward information flow in decoder\n7\t2\t1\tscaled dot-product attention implements masking for illegal connections"
    },
    "(2, 11)": {
        "diff": "paper\t importance weight\t the important aspect\n2\t0.5\tTransformers rely entirely on self-attention\n2\t0.5\tReducing sequential computation is the goal\n2\t0.5\tTransformers use Multi-Head Attention\n2\t0.5\tTransformers have reduced effective resolution due to averaging attention-weighted positions\n11\t0.5\tThe encoder-decoder structure is used in most competitive neural sequence transduction models\n11\t0.5\tThe encoder maps input sequence to continuous representations\n11\t0.5\tThe decoder generates output sequence one element at a time\n11\t0.5\tThe Transformer uses stacked self-attention and point-wise, fully connected layers for both the encoder and decoder",
        "sim": "2\t11\t0.8\treducing sequential computation forms foundation\t\n2\t11\t0.7\tuse convolutional neural networks\t\n2\t11\t0.6\tlearning dependencies between distant positions\t\n2\t11\t0.5\tperform well on simple-language question answering\t\n2\t11\t0.5\tusing stacked self-attention"
    },
    "(2, 12)": {
        "diff": "paper\timportance weight\tthe important aspect\n2\t1\tSelf-attention as a key feature\n2\t0.75\tTransforming input and output representations\n2\t0.5\tSuperiority over models using RNNs or convolution\n2\t0.5\tLearning dependencies between distant positions\n12\t1\tTotal computational complexity per layer\n12\t0.75\tAbility to parallelize computation\n12\t0.5\tLongest path length between input and output positions\n12\t0.5\tInterpretability of models using self-attention",
        "sim": "source paper\tdependent paper\timportance weight\tcontent of the relation\n2\t12\t1.0\treduce sequential computation foundation with Extended Neural GPU\n2\t12\t1.0\tConvS2S, ByteNet use CNN as building block\n2\t12\t0.5\tdifficult to learn dependencies between distant positions\n2\t12\t1.0\tTransformer uses self-attention for input and output representations\n2\t12\t0.5\tadvantages of self-attention over RNNs and convolutions"
    },
    "(2, 21)": {
        "diff": "paper\timportance weight\tthe important aspect\n2\t1\tself-attention as the foundation\n2\t0.8\treducing sequential computation as the goal\n2\t0.7\tTransformers relying entirely on self-attention\n2\t0.6\ttransformers using Multi-Head Attention\n21\t1\tTransformer as the first sequence transduction model\n21\t0.8\tfaster training compared to recurrent or convolutional architectures\n21\t0.7\tstate of the art performance in translation tasks\n21\t0.6\tplans to apply attention-based models to other tasks",
        "sim": "dependent paper\tsource paper\timportance weight\tcontent of the relation\n2\t21\t0.8\tfirst transduction model relying on self-attention\n2\t21\t0.7\tsequence transduction model based entirely on attention\n2\t21\t0.6\treplacing recurrent layers with multi-headed self-attention\n2\t21\t0.5\tfaster training than recurrent or convolutional architectures\n2\t21\t0.7\tachieving state of the art in translation tasks"
    },
    "(11, 7)": {
        "diff": "paper\timportance weight\tthe important aspect\n11\t1\tThe encoder-decoder structure\n11\t0.8\tStacked self-attention and point-wise, fully connected layers\n11\t0.6\tThe auto-regressive property\n11\t0.5\tThe overall architecture using stacked self-attention and point-wise, fully connected layers\n7\t1\tMulti-head attention in \"encoder-decoder attention\" layers\n7\t0.8\tSelf-attention layers in the encoder\n7\t0.7\tSelf-attention layers in the decoder\n7\t0.6\tThe prevention of leftward information flow in the decoder.",
        "sim": "dependent paper\tsource paper\timportance weight\tcontent of the relation\n11\t7\t0.75\tuses multi-head attention in three different ways\n11\t7\t0.7\tencoder-decoder attention mechanisms\n11\t7\t0.65\tself-attention layers\n11\t7\t0.6\tattention to all positions\n11\t7\t0.55\tTypic_encoder-decoder attention mechanisms"
    },
    "(11, 21)": {
        "diff": "paper\timportance weight\tthe important aspect\n11\t1\tThe encoder-decoder structure\n11\t0.7\tThe auto-regressive model\n11\t0.9\tThe use of self-attention and point-wise, fully connected layers\n11\t0.6\tThe model architecture shown in Figure 1\n21\t1\tThe use of attention instead of recurrent layers\n21\t0.8\tFaster training compared to recurrent or convolutional layers\n21\t0.9\tAchieving state of the art performance on translation tasks\n21\t0.7\tThe plan to extend the Transformer to handle inputs and outputs other than text",
        "sim": "dependent paper\tsource paper\timportance weight\tcontent of the relation\n11\t21\t0.8\tencoder-decoder structure\n11\t21\t0.6\tneural sequence transduction models\n11\t21\t0.7\tself-attention and point-wise, fully connected layers\n11\t21\t0.5\tmulti-headed self-attention\n11\t21\t0.6\ttranslation tasks"
    },
    "(16, 18)": {
        "diff": "paper\t importance weight\t the important aspect\n16\t0.5\tregularization during training\n16\t0.5\tdropout rate and label smoothing during training\n16\t0.5\ttraining cost and BLEU scores\n16\t0.5\tmaximum output length during inference\n18\t1\tbig transformer model outperforms previously reported models\n18\t1\testablishes a new state-of-the-art BLEU score\n18\t1\ttraining took 3.5 days on 8P100 GPUs\n18\t1\tachieves a BLEU score of 41.0",
        "sim": "dependent paper\t source paper\t importance weight\t content of the relation\n16\t18\t1.0\tThe big transformer model outperforms previously reported models.\n16\t18\t0.8\tThe base model surpasses previously published models.\n16\t18\t0.7\tThe big model achieves a BLEU score of 41.0.\n16\t18\t0.6\tTraining took 3.5 days on 8P100 GPUs.\n16\t18\t0.5\tThe configuration of the big model is listed in Table 3."
    },
    "(16, 19)": {
        "diff": "paper\timportance weight\tthe important aspect\n16\t1\tregularization during training: Residual Dropout and Label Smoothing\n16\t1\ttraining cost of the Transformer model is lower compared to previous models\n16\t1\tPdrop rate of 0.1 is used for dropout regularization\n16\t0.5\tresulting improvements in accuracy and BLEU score from label smoothing of \u03f5ls= 0.1\n19\t1\tdifferent variations of the base model were evaluated\n19\t1\tvarying number of attention heads and attention key and value dimensions\n19\t1\treducing attention key size hurts model quality\n19\t1\tbigger models perform better and dropout is helpful in avoiding overfitting\n19\t0.5\treplace sinusoidal positional encoding with learned positional embeddings achieves nearly identical results",
        "sim": "dependent paper\tsource paper\timportance weight\tcontent of the relation\n16\t19\t0.8\ttrains model with different variations\t\n16\t19\t0.7\tvaries base model\t\n16\t19\t0.6\tmeasures change in performance\t\n16\t19\t0.6\tobserves effect of different components\t\n16\t19\t0.6\tcompares models with different variations\t"
    },
    "(18, 19)": {
        "diff": "paper\timportance weight\tthe important aspect\n18\t1\tOn the WMT 2014 English-to-German translation task, the big transformer model outperforms previous models by 2.0 BLEU.\n18\t1\tTraining took 3.5 days on 8P100 GPUs.\n18\t1\tThe configuration of the model is listed in Table 3.\n18\t1\tThe base model surpasses all previously published models at a fraction of the training cost.\n19\t1\tWe varied the base model to evaluate the importance of different components of the Transformer.\n19\t0.5\tVarying the number of attention heads and key dimensions affects model quality.\n19\t0.8\tReducing the attention key size hurts model quality.\n19\t1\tBigger models and dropout are helpful in improving model performance.\n19\t1\tThe base model uses sinusoidal positional encoding, but replacing it with learned positional embeddings results in similar performance.",
        "sim": "The similarities between Node 18 and Node 19 are as follows:\n\ndependent paper\tsource paper\timportance weight\tcontent of the relation\n18\t19\t1.0\tTransformer architecture performance comparison\n18\t19\t1.0\tEffect of different components on translation quality\n18\t19\t0.8\tEffect of attention heads on model quality\n18\t19\t0.7\tEffect of attention key size on model quality\n18\t19\t0.6\tModel size and dropout's impact on quality"
    },
    "(18, 20)": {
        "diff": "paper\timportance weight\tthe important aspect\n18\t1\tNew state-of-the-art BLEU score of 28.4\n18\t0.8\tTraining took 3.5 days on 8P100 GPUs\n18\t0.6\tBase model surpasses all previously published models and ensembles\n18\t0.7\tUsed averaging of last 5 checkpoints for base models\n20\t1\tPerforms surprisingly well in English constituency parsing\n20\t0.9\tOutperforms the Berkeley-Parser even with limited training data\n20\t0.7\tTrained on Wall Street Journal portion of Penn Treebank\n20\t0.8\tUsed a vocabulary of 32K tokens for semi-supervised setting",
        "sim": "dependent paper\tsource paper\timportance weight\tcontent of the relation\n18\t20\t0.75\toutperforms in English constituency parsing\n18\t20\t0.8\toutperforms RNN sequence-to-sequence models\n18\t20\t0.6\tachieves better results than all previously reported models\n18\t20\t0.7\tperforms surprisingly well in constituency parsing\n18\t20\t0.6\toutperforms the Berkeley-Parser in English constituency parsing"
    },
    "(21, 7)": {
        "diff": "paper\t importance weight\t the important aspect\n21\t     1\t       Encoder-decoder attention mechanism\n21\t     0.8\t     Transformer's potential for tasks beyond translation\n21\t     0.7\t     Local and restricted attention mechanisms for handling large inputs and outputs\n21\t     0.5\t     Research goal of making generation less sequential\n7\t       1\t       Three different uses of multi-head attention in the Transformer\n7\t       0.8\t     Self-attention layers in the encoder and decoder\n7\t       0.7\t     Preventing leftward information flow in the decoder\n7\t       0.5\t     Implementation of masking in scaled dot-product attention",
        "sim": "dependent paper\tsource paper\timportance weight\tcontent of the relation\n21\t7\t1\tThe Transformer uses multi-head attention\n21\t7\t1\tqueries come from the previous decoder layer\n21\t7\t1\tmemory keys and values come from the output of the encoder\n21\t7\t1\tencoder-decoder attention mechanisms in sequence-to-sequence models\n21\t7\t1\tself-attention layers in the decoder allow each position to attend to all positions"
    },
    "(12, 9)": {
        "diff": "paper\timportance weight\tthe important aspect\n12\t1\tcompare self-attention to recurrent and convolutional layers\n12\t0.8\tcompare computational complexity and parallelization ability\n12\t0.7\tcompare path length between long-range dependencies\n12\t0.9\tcompare maximum path length between input and output positions\n9\t1\tuse learned embeddings for input and output tokens\n9\t0.7\tshare weight matrix between embedding layers and pre-softmax linear transformation\n9\t0.6\tmultiply weights in embedding layers by \u221admodel\n9\t0.5\tuse linear transformation and softmax function for decoder output",
        "sim": "dependent paper\tsource paper\timportance weight\tcontent of the relation\n9\t12\t1\tSimilarities between sequence transduction models\n9\t12\t0.8\tComparison of computational complexity and parallelization\n9\t12\t0.6\tComparison of path lengths for learning long-range dependencies\n9\t12\t0.7\tComparison of self-attention and recurrent layers\n9\t12\t0.9\tComparison of self-attention and convolutional layers"
    },
    "(7, 3)": {
        "diff": "paper\timportance weight\tthe important aspect\n7\t1\tThe Transformer uses multi-head attention in three different ways\n7\t1\tIn \"encoder-decoder attention\" layers, the queries come from the previous decoder layer\n7\t1\tThe encoder contains self-attention layers\n7\t1\tSelf-attention layers in the decoder allow each position in the decoder to attend to all positions in the decoder up to and including that position\n3\t1\tEncoder is composed of a stack of N= 6 identical layers\n3\t1\tThe first sub-layer of each encoder layer is a multi-head self-attention mechanism\n3\t1\tThe decoder inserts a third sub-layer, which performs multi-head attention over the output of the encoder stack\n3\t1\tThe self-attention sub-layer in the decoder stack is modified to prevent positions from attending to subsequent positions",
        "sim": "dependent paper\tsource paper\timportance weight\tcontent of the relation\n7\t3\t0.8\tThe Transformer uses multi-head attention\n7\t3\t0.7\tallows every position in the decoder to attend over all positions in the input sequence\n7\t3\t0.6\tself-attention layers in the decoder allow each position in the decoder to attend to all positions in the decoder\n7\t3\t0.5\tto prevent leftward information flow in the decoder\n7\t3\t0.5\timplement this inside of scaled dot-product attention by masking out all values in the input of the softmax which correspond to illegal connections."
    },
    "(3, 8)": {
        "diff": "paper\t importance weight\t the important aspect\n3\t1\tEncoder has stack of 6 identical layers\n3\t1\tEncoder has multi-head self-attention mechanism\n3\t1\tEncoder uses residual connections and layer normalization\n3\t1\tDecoder performs multi-head attention over encoder output\n8\t1\tEach layer in the encoder and decoder has a fully connected feed-forward network\n8\t1\tFeed-forward network consists of two linear transformations with ReLU activation\n8\t1\tLinear transformations have different parameters from layer to layer\n8\t1\tInner-layer of feed-forward network has dimensionality dff= 2048",
        "sim": "dependent paper\tsource paper\timportance weight\tcontent of the relation\n3\t8\t1\tentails fully connected feed-forward network\n3\t8\t0.9\tincludes residual connection around sub-layers\n3\t8\t0.8\temploys layer normalization\n3\t8\t0.7\thas dimensionality dmodel = 512\n3\t8\t0.6\thas dimensionality dff = 2048"
    },
    "(4, 5)": {
        "diff": "paper\timportance weight\tthe important aspect\n4\t1\tScaled Dot-Product Attention\n4\t0.8\tMulti-Head Attention\n4\t0.6\tFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several attention layers running in parallel.\n4\t0.7\tcompatibility function of the query with the corresponding key.\n5\t1\tAttention( Q, K, V ) = softmax(QKT \u221adk)V\n5\t0.7\tAdditive attention computes the compatibility function using a feed-forward network with a single hidden layer.\n5\t0.8\tAdditive attention outperforms dot product attention without scaling for larger values of dk[3].\n5\t0.6\tWe scale the dot products by1\u221adk to counteract the effect of large gradients.",
        "sim": "dependent paper\tsource paper\timportance weight\tcontent of the relation\n4\t5\t1\tAttention function described as mapping query and key-value pairs to output\n4\t5\t1\tScaled Dot-Product Attention is called our particular attention\n4\t5\t1\tDot-product attention is identical to our algorithm except for scaling factor\n4\t5\t1\tAdditive attention outperforms dot product attention without scaling for larger values of dk\n4\t5\t1\tDot products grow large in magnitude"
    },
    "(4, 6)": {
        "diff": "paper\timportance weight\tthe important aspect\n4\t0.5\tAttention function described as mapping query and key-value pairs\n4\t0.7\tScaled Dot-Product Attention\n4\t0.8\tMulti-Head Attention\n4\t1\tWeight assigned to each value computed by compatibility function\n6\t0.5\tLinearly project queries, keys, and values h times\n6\t0.7\tPerform attention function in parallel on projected versions\n6\t0.8\tMulti-head attention allows joint attention to different representation subspaces\n6\t1\tTotal computational cost similar to single-head attention with full dimensionality",
        "sim": "dependent paper\tsource paper\timportance weight\tcontent of the relation\n4\t6\t0.8\tperforming attention function with projected versions\n4\t6\t0.7\tmulti-head attention jointly attends to different subspaces\n4\t6\t0.6\tlinearly project queries, keys, and values\n4\t6\t0.7\tparallel attention function on projected versions\n4\t6\t0.8\tconcatenating projected versions to final values"
    },
    "(5, 6)": {
        "diff": "paper\t importance weight\t the important aspect\n5\t0.75\t\"Multiplicative attention is faster and more space-efficient compared to additive attention\"\n5\t1\t\"The dot products in dot-product attention are scaled by 1/sqrt(dk)\"\n5\t0.75\t\"Additive attention performs better than dot-product attention for larger values of dk\"\n6\t0.5\t\"Linear projection of queries, keys, and values leads to better performance\"\n6\t0.75\t\"Multi-head attention allows joint attention to different information at different positions\"\n6\t1\t\"Employing 8 parallel attention layers with reduced dimensionality (64) reduces computational cost\"",
        "sim": "dependent paper\t source paper\t importance weight\t content of the relation\n5\t6\t0.8\tperforming attention function with linear projections\n5\t6\t0.7\tproject queries, keys, and values separately\n5\t6\t0.6\tyielding output values in parallel\n5\t6\t0.6\tusing multi-head attention\n5\t6\t0.5\tconcatenating and projecting final values"
    }
}