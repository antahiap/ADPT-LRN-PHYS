[
    {
        "id": "",
        "section": "Abstract",
        "text": "Sequenceto sequencelearninghasrecentlyemergedasa new p aradigminsuper-\nvised learning. To date, most ofits applicationsfocusedon onlyone task and not\nmuchworkexploredthisframeworkformultipletasks. Thisp aperexaminesthree\nmulti-tasklearning(MTL)settingsforsequenceto sequenc emodels: (a)the one-\nto-manysetting \u2013 where the encoder is shared between several tasks s uch as ma-\nchinetranslationandsyntacticparsing,(b)the many-to-one setting\u2013 usefulwhen\nonly the decoder can be shared, as in the case of translation a nd image caption\ngeneration, and (c) the many-to-many setting \u2013 where multiple encoders and de-\ncodersareshared,whichis the case with unsupervisedobjec tivesandtranslation.\nOur results show that training on a small amount of parsing an d image caption\ndatacanimprovethetranslationqualitybetweenEnglishan dGermanbyupto 1.5\nBLEUpointsoverstrongsingle-taskbaselinesontheWMTben chmarks. Further-\nmore,wehaveestablishedanew state-of-the-art resultinconstituentparsingwith\n93.0F1. Lastly, we reveal interestingpropertiesof the two unsupe rvisedlearning\nobjectives,autoencoderandskip-thought,inthe MTLconte xt: autoencoderhelps\nlessin termsofperplexitiesbutmoreonBLEUscorescompare dtoskip-thought.",
        "subsection": []
    },
    {
        "id": "1",
        "section": "Introduction",
        "text": "Multi-task learning (MTL) is an important machine learning paradigm that aims at improving\nthe generalization performance of a task using other relate d tasks. Such framework has been\nwidelystudiedbyThrun(1996);Caruana(1997);Evgeniou&P ontil(2004);Ando& Zhang(2005);\nArgyriouetal. (2007); Kumar& III (2012), among manyothers . In the context of deep neural net-\nworks, MTL has been applied successfully to various problem s ranging from language (Liuet al.,\n2015),to vision(Donahueet al.,2014), andspeech(Heigold etal., 2013;Huangetal.,2013).\nRecently, sequence to sequence ( seq2seq) learning, proposed by Kalchbrenner&Blunsom (2013),\nSutskeveret al. (2014), and Cho etal. (2014), emerges as an e ffective paradigm for dealing with\nvariable-length inputs and outputs. seq2seqlearning, at its core, uses recurrent neural networks\nto map variable-length input sequences to variable-length output sequences. While relatively new,\ntheseq2seqapproachhas achieved state-of-the-artresults in not only its original application \u2013 ma-\nchine translation \u2013 (Luonget al., 2015b; Jean etal., 2015a; Luongetal., 2015a; Jean etal., 2015b;\nLuong&Manning, 2015), but also image caption generation (V inyalsetal., 2015b), and con-\nstituencyparsing(Vinyalset al., 2015a).\nDespitethepopularityofmulti-tasklearningandsequence tosequencelearning,therehasbeenlittle\nwork in combining MTL with seq2seqlearning. To the best of our knowledge, there is only one\nrecent publication by Donget al. (2015) which applies a seq2seqmodels for machine translation,\nwhere the goal is to translate from one language to multiple l anguages. In this work, we propose\nthree MTL approachesthat complementone another: (a) the one-to-many approach\u2013 for tasksthat\ncan have an encoder in common, such as translation and parsin g; this applies to the multi-target\ntranslation setting in (Donget al., 2015) as well, (b) the many-to-one approach \u2013 useful for multi-\nsource translation or tasks in which only the decoder can be e asily shared, such as translation and\nimage captioning, and lastly, (c) the many-to-many approach \u2013 which share multiple encoders and\ndecoders through which we study the effect of unsupervised l earning in translation. We show that\nsyntactic parsing and image caption generation improves th e translation quality between English\n\u2217Publishedasa conferencepaperat ICLR2016\nFigure 1: Sequence to sequence learning examples \u2013 (left) machine translation (Sutskeveret al.,\n2014)and( right)constituentparsing(Vinyalset al.,2015a).\nandGermanbyupto+ 1.5BLEUpointsoverstrongsingle-taskbaselinesontheWMTben chmarks.\nFurthermore, we have established a new state-of-the-art result in constituent parsing with 93.0 F 1.\nWealsoexploretwounsupervisedlearningobjectives,sequ enceautoencoders(Dai &Le,2015)and\nskip-thoughtvectors (Kiroset al., 2015), and reveal their interesting propertiesin the MTL setting:\nautoencoderhelpslessintermsofperplexitiesbutmoreonB LEUscorescomparedtoskip-thought.",
        "subsection": []
    },
    {
        "id": "2",
        "section": "Sequence to sequence learning",
        "text": "Sequencetosequencelearning( seq2seq)aimstodirectlymodeltheconditionalprobability p(y|x)of\nmappingan inputsequence, x1,...,x n, into an outputsequence, y1,...,y m. It accomplishessuch\ngoal through the encoder-decoder framework proposed by Sutskeveretal. (2014) and Cho etal.\n(2014). AsillustratedinFigure1,the encodercomputesarepresentation sforeachinputsequence.\nBasedonthatinputrepresentation,the decodergeneratesanoutputsequence,oneunitatatime,and\nhence,decomposestheconditionalprobabilityas:\nlogp(y|x) =/summationdisplaym\nj=1logp(yj|y<j,x,s) (1)\nAnaturalmodelforsequentialdataistherecurrentneuraln etwork(RNN),whichisusedbymostof\nthe recent seq2seqwork. These work, however,differin termsof: (a) architecture \u2013 fromunidirec-\ntional, to bidirectional, and deep multi-layer RNNs; and (b )RNN type \u2013 which are long-short term\nmemory(LSTM)(Hochreiter& Schmidhuber,1997)andthegate drecurrentunit(Choetal.,2014).\nAnother important difference between seq2seqwork lies in what constitutes the input represen-\ntations. The early seq2seqwork (Sutskeveret al., 2014; Choet al., 2014; Luonget al., 2 015b;\nVinyalset al., 2015b) uses only the last encoder state to ini tialize the decoder and sets s= [ ]\nin Eq. (1). Recently, Bahdanauet al. (2015) proposes an attention mechanism , a way to provide\nseq2seqmodelswitharandomaccessmemory,tohandlelonginputsequ ences. Thisisaccomplished\nbysetting sinEq.(1)tobetheset ofencoderhiddenstatesalreadycompu ted. Onthedecoderside,\nat each time step, the attention mechanism will decide how mu ch informationto retrieve from that\nmemory by learning where to focus, i.e., computing the align ment weights for all input positions.\nRecent work such as (Xuet al., 2015; Jeanet al., 2015a; Luong et al., 2015a; Vinyalsetal., 2015a)\nhasfoundthatit iscrucialtoempower seq2seqmodelswiththe attentionmechanism.",
        "subsection": [
            {
                "id": "3.1",
                "section": "One-to-manysetting",
                "text": "This scheme involves one encoder andmultiple decoders for tasks in which the encoder can be\nshared,asillustratedin Figure2. The inputtoeach task isa sequenceofEnglishwords. A separate\nPublishedasa conferencepaperat ICLR2016\nEnglish (unsupervised)German (translation)\nTags (parsing) English\nFigure 2: One-to-manySetting \u2013 one encoder,multiple decoders. Thisscheme is useful for e ither\nmulti-target translation as in Dongetal. (2015) or between different tasks. Here, English and Ger-\nman imply sequences of words in the respective languages. Th e\u03b1values give the proportions of\nparameterupdatesthatareallocatedforthe differenttask s.\nforconstituencyparsingas usedin (Vinyalset al., 2015a), (b)a sequenceofGermanwordsforma-\nchinetranslation(Luongetal.,2015a),and(c)thesameseq uenceofEnglishwordsforautoencoders\nora relatedsequenceofEnglishwordsfortheskip-thoughto bjective(Kiroset al.,2015).",
                "subsection": []
            },
            {
                "id": "3.2",
                "section": "Many-to-onesetting",
                "text": "Thisschemeistheoppositeofthe one-to-many setting. AsillustratedinFigure3,itconsistsof mul-\ntipleencoders andonedecoder . Thisisusefulfortasksinwhichonlythedecodercanbeshar ed,for\nexample, when our tasks include machine translation and ima ge caption generation (Vinyalset al.,\n2015b). In addition, from a machine translation perspectiv e, this setting can bene\ufb01t from a large\namount of monolingual data on the target side, which is a stan dard practice in machine translation\nsystemandhasalso beenexploredforneuralMT byGulcehreet al.(2015).\nEnglish (unsupervised)Image (captioning) EnglishGerman (translation)\nFigure3: Many-to-onesetting \u2013multipleencoders,onedecoder. Thisschemeishandyforta sksin\nwhichonlythedecoderscanbeshared.",
                "subsection": []
            },
            {
                "id": "3.3",
                "section": "Many-to-manysetting",
                "text": "Lastly,asthenamedescribes,thiscategoryisthemostgene ralone,consistingofmultipleencoders\nand multiple decoders. We will explore this scheme in a trans lation setting that involves sharing\nmultipleencodersandmultipledecoders. Inadditiontothe machinetranslationtask,wewillinclude\ntwounsupervisedobjectivesoverthesourceandtargetlang uagesasillustratedin Figure4.",
                "subsection": []
            },
            {
                "id": "3.4",
                "section": "Unsupervised learning tasks",
                "text": "Ourvery\ufb01rstunsupervisedlearningtaskinvolveslearning autoencoders frommonolingualcorpora,\nwhich has recently been applied to sequence to sequence lear ning (Dai &Le, 2015). However, in\nDai &Le (2015)\u2019s work, the authors only experiment with pret raining and then \ufb01netuning, but not\njoint training which can be viewed as a form of multi-task lea rning (MTL). As such, we are very\ninterestedinknowingwhetherthe sametrendextendsto ourM TLsettings.\nAdditionally,weinvestigatetheuseofthe skip-thought vectors(Kiroset al.,2015) inthecontextof\nourMTL framework. Skip-thoughtvectorsare trained by trai ningsequenceto sequencemodelson\npairs of consecutive sentences, which makes the skip-thoug ht objective a natural seq2seqlearning\nPublishedasa conferencepaperat ICLR2016\nGerman (translation)\nEnglish (unsupervised) German (unsupervised)English\nFigure4: Many-to-manysetting \u2013 multipleencoders,multipledecoders. We considerthissc heme\nin a limited context of machine translation to utilize the la rge monolingual corpora in both the\nsource and the target languages. Here, we consider a single t ranslation task and two unsupervised\nautoencodertasks.\nconsist of ordered sentences, e.g., paragraphs. Unfortuna tely, in many applications that include\nmachinetranslation,weonlyhavesentence-leveldatawher ethesentencesareunordered. Toaddress\nthat,we spliteachsentenceintotwohalves;we thenuse oneh alftopredicttheotherhalf.",
                "subsection": []
            },
            {
                "id": "3.5",
                "section": "Learning",
                "text": "Donget al. (2015) adopted an alternating training approach, where they optimize each task for a\n\ufb01xed number of parameter updates (or mini-batches) before s witching to the next task (which is a\ndifferentlanguagepair). In oursetting, ourtasks are more diverseand containdifferentamountsof\ntraining data. As a result, we allocate different numbersof parameter updates for each task, which\nare expressed with the mixingratio values \u03b1i(for each task i). Each parameter update consists of\ntraining data from one task only. When switching between tas ks, we select randomly a new task i\nwithprobability\u03b1i/summationtext\nj\u03b1j.\nOur conventionis that the \ufb01rst task is the reference task with \u03b11= 1.0and the number of training\nparameterupdatesforthattaskisprespeci\ufb01edtobe N. Atypicaltask iwillthenbetrainedfor\u03b1i\n\u03b11\u00b7N\nparameterupdates. Suchconventionmakesiteasierforusto fairlycomparethesamereferencetask\nina single-tasksetting whichhasalsobeentrainedforexac tlyNparameterupdates.\nWhensharinganencoderoradecoder,weshareboththerecurr entconnectionsandthecorrespond-\ningembeddings.",
                "subsection": []
            }
        ]
    },
    {
        "id": "3",
        "section": "Multi-tasksequence -to-sequence learning",
        "text": "We generalize the work of Donget al. (2015) to the multi-task sequence-to-sequencelearning set-\nting that includes the tasks of machine translation (MT), co nstituency parsing, and image caption\ngeneration. Dependingwhich tasks involved,we proposeto c ategorize multi-task seq2seqlearning\ninto three general settings. In addition, we will discuss th e unsupervised learning tasks considered\naswell asthelearningprocess.",
        "subsection": [
            {
                "id": "4.1",
                "section": "Data",
                "text": "Ourexperimentsarecenteredaroundthe translation task,whereweaimtodeterminewhetherother\ntasks can improve translation and vice versa. We use the WMT\u2019 15 data (Bojaret al., 2015) for\nthe English \u21c6German translation problem. Following Luonget al. (2015a) , we use the 50K most\nfrequent words for each language from the training corpus.1These vocabularies are then shared\nwithothertasks,exceptforparsinginwhichthetarget\u201clan guage\u201dhasavocabularyof104tags. We\nuse newstest2013 (3000 sentences) as a validation set to sel ect our hyperparameters, e.g., mixing\ncoef\ufb01cients. For testing, to be comparablewith existing re sultsin (Luonget al., 2015a), we use the\n\ufb01lterednewstest2014(2737sentences)2fortheEnglish \u2192Germantranslationtaskandnewstest2015\n(2169sentences)3fortheGerman \u2192Englishtask. See thesummaryinTable1.\n1The corpus has already beentokenized using the default toke nizer from Moses. Wordsnot inthese vocab-\nularies are represented bythe token <unk>.\n2http://statmt.org/wmt14/test-filtered.tgz\n3Publishedasa conferencepaperat ICLR2016\nTaskTrain Valid Test VocabSize Train Finetune\nSize Size Size Source Target Epoch Start Cycle\nEnglish\u2192GermanTranslation 4.5M 3000 3003 50K 50K 12 8 1\nGerman\u2192EnglishTranslation 4.5M 3000 2169 50K 50K 12 8 1\nEnglishunsupervised 12.1MDetailsintext50K 50K 6 4 0.5\nGermanunsupervised 13.8M 50K 50K 6 4 0.5\nPennTreeBankParsing 40K 1700 2416 50K 104 40 20 4\nHigh-Con\ufb01denceCorpusParsing 11.0M 1700 2416 50K 104 6 4 0.5\nImageCaptioning 596K 4115 - - 50K 10 5 1\nTable 1:Data& TrainingDetails \u2013 Informationaboutthe differentdatasetsused inthis work . For\neach task, we display the following statistics: (a) the numb er of training examples, (b) the sizes of\nthe vocabulary, (c) the number of training epochs, and (d) de tails on when and how frequent we\nhalvethe learningrates( \ufb01netuning ).\nFor theunsupervised tasks, we use the English and German monolingualcorpora fro m WMT\u201915.4\nSince in our experiments, unsupervisedtasks are always cou pled with translation tasks, we use the\nsamevalidationandtest setsastheaccompaniedtranslatio ntasks.\nForconstituencyparsing ,we experimentwithtwotypesofcorpora:\n1. asmall corpus\u2013 thewidelyusedPennTreeBank(PTB)datase t (Marcuset al.,1993)and,\n2. alargecorpus\u2013thehigh-con\ufb01dence(HC)parsetreesprovi dedbyVinyalset al.(2015a).\nThe two parsing tasks, however, are evaluated on the same val idation (section 22) and test (sec-\ntion 23) sets from the PTB data. Note also that the parse trees have been linearized following\nVinyalset al. (2015a). Lastly, for image captiongeneration , we use a dataset of image and caption\npairsprovidedbyVinyalset al. (2015b).",
                "subsection": []
            },
            {
                "id": "4.2",
                "section": "Training details",
                "text": "Inallexperiments,followingSutskeveret al.(2014)andLu ongetal.(2015b),wetraindeepLSTM\nmodelsasfollows: (a)weuse4LSTMlayerseachofwhichhas10 00-dimensionalcellsandembed-\ndings,5(b)parametersareuniformlyinitializedin[-0.06,0.06], (c)weuseamini-batchsize of128,\n(d) dropout is applied with probability of 0.2 over vertical connections (Phamet al., 2014), (e) we\nuse SGD with a \ufb01xed learning rate of 0.7, (f) input sequencesa re reversed, and lastly, (g) we use a\nsimple\ufb01netuningschedule\u2013after xepochs,wehalvethelearningrateevery yepochs. Thevalues x\nandyarereferredas \ufb01netunestart and\ufb01netunecycle inTable1togetherwiththenumberoftraining\nepochspertask.\nAsdescribedinSection3,foreachmulti-taskexperiment,w eneedtochooseonetasktobethe refer-\nencetask (whichcorrespondsto \u03b11= 1). Thechoiceofthereferencetaskhelpsspecifythenumber\nof training epochs and the \ufb01netune start/cycle values which we also when training that reference\ntask alone for fair comparison. To make sure our \ufb01ndings are r eliable, we run each experimental\ncon\ufb01gurationtwiceandreporttheaverageperformancein th eformatmean(stddev) .",
                "subsection": []
            },
            {
                "id": "4.3",
                "section": "Results",
                "text": "We explore several multi-task learning scenarios by combin ing alargetask (machine translation)\nwith: (a) a smalltask \u2013 Penn Tree Bank (PTB) parsing, (b) a medium-sized task \u2013 image caption\ngeneration, (c) another largetask \u2013 parsing on the high-con\ufb01dence (HC) corpus, and (d) las tly,\nunsupervised tasks , such as autoencodersand skip-thoughtvectors. In terms of evaluation metrics,\nwereportbothvalidationandtestperplexitiesforalltask s. Additionally,wealsocomputetestBLEU\nscores(Papineniet al., 2002)forthetranslationtask.\n4The training sizes reported for the unsupervised tasks are o nly 10% of the original WMT\u201915 monolingual\ncorpora which we randomly sample from. Such reduced sizes ar e for faster training time and already about\nthree times largerthan thatof the parallel data. We conside r usingall the monolingual data infuture work.\n5Publishedasa conferencepaperat ICLR2016",
                "subsection": [
                    {
                        "id": "4.3.1",
                        "section": "Largetasks with smalltasks",
                        "text": "In this setting, we want to understand if a small task such as PTB parsing can help improve the\nperformance of a large task such as translation. Since the pa rsing task maps from a sequence of\nEnglish wordsto a sequenceof parsingtags (Vinyalset al., 2 015a), only the encodercan be shared\nwithanEnglish \u2192Germantranslationtask. Asa result,thisisa one-to-many MTLscenario( \u00a73.1).\nTo our surprise, the results in Table 2 suggest that by adding a very small number of parsing mini-\nbatches (with mixing ratio 0.01, i.e., one parsing mini-batch per 100 translation mini-bat ches), we\ncan improvethe translationquality substantially. Moreco ncretely,ourbest multi-task modelyields\na gain of + 1.5BLEU points over the single-task baseline. It is worth point ing out that as shown in\nTable 2, our single-taskbaseline is verystrong,evenbette r thanthe equivalentnon-attentionmodel\nreportedin(Luonget al.,2015a). Largermixingcoef\ufb01cient s,however,over\ufb01tthesmallPTBcorpus;\nhence,achievesmallergainsintranslationquality.\nFor parsing, as Vinyalset al. (2015a) have shown that attent ion is crucial to achieve good parsing\nperformancewhentrainingonthe small PTB corpus,we donots et a highbarforourattention-free\nsystems in this setup (better performancesare reported in S ection 4.3.3). Nevertheless, the parsing\nresultsin Table2indicatethatMTLisalso bene\ufb01cialforpar sing,yieldinganimprovementofupto\n+8.9F1pointsoverthe baseline.6It would be interestingto study howMTL can be usefulwith the\npresenceofthe attention mechanism,whichweleaveforfuturework.\nTaskTranslation Parsing\nValid ppl Test ppl Test BLEU Test F 1\n(Luonget al.,2015a) - 8.1 14.0 -\nOursingle-tasksystems\nTranslation 8.8(0.3) 8.3(0.2) 14.3(0.3) -\nPTBParsing - - - 43.3(1.7)\nOurmulti-tasksystems\nTranslation +PTBParsing(1x) 8.5(0.0) 8.2(0.0) 14.7(0.1) 54.5(0.4)\nTranslation +PTBParsing(0.1x) 8.3(0.1) 7.9(0.0) 15.1(0.0) 55.2(0.0)\nTranslation +PTBParsing(0.01x) 8.2(0.2)7.7(0.2)15.8(0.4) 39.8(2.7)\nTable 2: English\u2192German WMT'14 translation & Penn Tree Bank parsing results \u2013 shown\nare perplexities (ppl), BLEU scores, and parsing F 1for various systems. For muli-task models,\nreference tasks are in italic with the mixing ratio in parentheses. Our results are averaged overtwo\nrunsintheformat mean(stddev) . Best resultsare highlightedinboldface.",
                        "subsection": []
                    },
                    {
                        "id": "4.3.2",
                        "section": "Largetaskswithmediumtasks",
                        "text": "We investigate whether the same pattern carries over to a med ium task such as image caption gen-\neration. Since the image caption generation task maps images to a seq uence of English words\n(Vinyalset al., 2015b; Xu etal., 2015), only the decoder can be shared with a German \u2192English\ntranslationtask. Hence,thissettingfallsunderthe many-to-one MTLsetting( \u00a73.2).\nTheresultsinTable3showthesametrendweobservedbefore, thatis,bytrainingonanothertaskfor\naverysmallfractionoftime,themodelimprovesitsperform anceonitsmaintask. Speci\ufb01cally,with\n5parameterupdatesforimagecaptiongenerationper100upd atesfortranslation(sothemixingratio\nof0.05), we obtain a gain of + 0.7BLEU scores over a strong single-task baseline. Our baselin e is\nalmostaBLEUpointbetterthantheequivalentnon-attentio nmodelreportedinLuongetal.(2015a).",
                        "subsection": []
                    },
                    {
                        "id": "4.3.3",
                        "section": "Largetasks with largetasks",
                        "text": "Our \ufb01rst set of experiments is almost the same as the one-to-m any setting in Section 4.3.1 which\ncombines translation ,as thereferencetask, withparsing. Theonlydifferenceis in termsof parsing\n6While perplexities correlate well withBLEUscores as shown in(Luong et al., 2015b), we observe empir-\nically in Section 4.3.3 that parsing perplexities are only r eliable if it is less than 1.3. Hence, we omit parsing\nperplexities in Table 2 for clarity. The parsing test perple xities (averaged over two runs) for the last four rows\nPublishedasa conferencepaperat ICLR2016\nTaskTranslation Captioning\nValidppl Test ppl Test BLEU Validppl\n(Luonget al.,2015a) - 14.3 16.9 -\nOursingle-tasksystems\nTranslation 11.0(0.0) 12.5(0.2) 17.8(0.1) -\nCaptioning - - - 30.8(1.3)\nOurmulti-tasksystems\nTranslation + Captioning(1x) 11.9 14.0 16.7 43.3\nTranslation + Captioning(0.1x) 10.5(0.4) 12.1(0.4) 18.0(0.6) 28.4(0.3)\nTranslation + Captioning(0.05x) 10.3(0.1)11.8(0.0)18.5(0.0) 30.1(0.3)\nTranslation + Captioning(0.01x) 10.6(0.0) 12.3(0.1) 18.1(0.4) 35.2(1 .4)\nTable 3:German\u2192English WMT'15 translation & captioning results \u2013 shown are perplexities\n(ppl) and BLEU scores for various tasks with similar format a s in Table 2. Reference tasks are in\nitalic withmixingratiosin parentheses. Theaverageresul tsof2runsarein mean(stddev) format.\ndata. Instead of using the small Penn Tree Bank corpus, we con sider a large parsing resource, the\nhigh-con\ufb01dence(HC)corpus,whichisprovidedbyVinyalset al.(2015a). AshighlightedinTable4,\nthetrendisconsistent;MTLhelpsboosttranslationqualit ybyupto+ 0.9BLEUpoints.\nTaskTranslation\nValidppl Test ppl Test BLEU\n(Luonget al.,2015a) - 8.1 14.0\nOursystems\nTranslation 8.8(0.3) 8.3(0.2) 14.3(0.3)\nTranslation + HCParsing(1x) 8.5(0.0) 8.1(0.1) 15.0(0.6)\nTranslation + HCParsing(0.1x) 8.2(0.3)7.7(0.2)15.2(0.6)\nTranslation + HCParsing(0.05x) 8.4(0.0) 8.0(0.1) 14.8(0.2)\nTable4:English\u2192GermanWMT'14translation \u2013shownareperplexities(ppl)andBLEUscores\nof varioustranslationmodels. Our multi-tasksystems comb inetranslationand parsingon the high-\ncon\ufb01dencecorpustogether. Mixingratiosare in parenthese sandthe averageresultsover2runsare\ninmean(stddev) format. Best resultsarebolded.\nThe second set of experimentsshifts the attention to parsingby having it as the referencetask. We\nshow in Table 5 results that combine parsing with either (a) t he English autoencoder task or (b)\nthe English \u2192German translation task. Our models are compared against th e best attention-based\nsystemsin(Vinyalset al.,2015a),includingthestate-of- the-artresultof92.8F 1.\nBefore discussing the multi-task results, we note a few inte resting observations. First, very small\nparsing perplexities, close to 1.1, can be achieved with lar ge training data.7Second, our baseline\nsystemcanobtainaverycompetitiveF 1scoreof92.2,rivalingVinyalset al.(2015a)\u2019ssystems. Th is\nis rather surprising since our models do not use any attentio n mechanism. A closer look into these\nmodels reveal that there seems to be an architectural differ ence: Vinyalset al. (2015a) use 3-layer\nLSTMwith256cellsand512-dimensionalembeddings;wherea sourmodelsuse4-layerLSTMwith\n1000 cells and 1000-dimensionalembeddings. This further s upports \ufb01ndings in (Jozefowiczet al.,\n2016)that largernetworksmatterforsequencemodels.\nFor the multi-task results, while autoencoder does not seem to help parsing, translation does. At\nthe mixing ratio of 0.05, we obtain a non-negligible boost of 0.2 F1over the baseline and with",
                        "subsection": []
                    },
                    {
                        "id": "4.3.4",
                        "section": "Multi-tasks and unsupervised learning",
                        "text": "Our main focus in this section is to determine whether unsupe rvised learning can help improve\ntranslation. Speci\ufb01cally, we follow the many-to-many approach described in Section 3.3 to couple\ntheGerman \u2192Englishtranslationtaskwithtwounsupervisedlearningta sksonmonolingualcorpora,\none per language. The results in Tables 6 show a similar trend as before, a small amount of other\ntasks, in this case the autoencoder objective with mixing coef\ufb01cient 0.05, improvesthe transl ation\nquality by + 0.5BLEU scores. However, as we train more on the autoencodertas k, i.e. with larger\nmixingratios,thetranslationperformancegetsworse.\nTaskTranslation German English\nValidppl Test ppl Test BLEU Test ppl Test ppl\n(Luongetal., 2015a) - 14.3 16.9 - -\nOursingle-tasksystems\nTranslation 11.0(0.0) 12.5(0.2) 17.8(0.1) - -\nOurmulti-tasksystemswith Autoencoders\nTranslation + autoencoders(1.0x) 12.3 13.9 16.0 1.01 2.10\nTranslation + autoencoders(0.1x) 11.4 12.7 17.7 1.13 1.44\nTranslation + autoencoders(0.05x) 10.9(0.1)12.0(0.0)18.3(0.4) 1.40(0.01) 2.38(0.39)\nOurmulti-tasksystems with Skip-thoughtVectors\nTranslation + skip-thought(1x) 10.4(0.1)10.8(0.1) 17.3(0.2) 36.9(0.1)31.5(0.4)\nTranslation + skip-thought(0.1x) 10.7(0.0) 11.4(0.2) 17.8(0.4) 52.8( 0.3) 53.7(0.4)\nTranslation + skip-thought(0.01x) 11.0(0.1) 12.2(0.0) 17.8(0.3) 76.3(0.8) 142.4(2.7)\nTable 6:German\u2192English WMT'15 translation & unsupervised learning result s\u2013 shown are\nperplexitiesfortranslationandunsupervisedlearningta sks. We experimentwithboth autoencoders\nandskip-thoughtvectors for the unsupervisedobjectives. Numbersin mean (stddev) format are the\naverageresultsof2 runs;othersarefor1runonly.\nSkip-thought objectives, on the other hand, behave differently. If we mer ely look at the perplexity\nmetric,theresultsareveryencouraging:withmoreskip-th oughtdata,weperformbetterconsistently\nacrossboththetranslationandtheunsupervisedtasks. How ever,whencomputingtheBLEUscores,\nthe translation quality degrades as we increase the mixing c oef\ufb01cients. We anticipate that this is\ndue to the fact that the skip-thoughtobjectivechangesthe n atureof the translationtask when using\none half of a sentence to predict the other half. It is not a pro blem for the autoencoder objectives,\nPublishedasa conferencepaperat ICLR2016\nWebelievethese\ufb01ndingsposeinterestingchallengesinthe questtowardsbetterunsupervisedobjec-\ntives,whichshouldsatisfythefollowingcriteria: (a)ade sirableobjectiveshouldbecompatiblewith\nthesupervisedtaskinfocus,e.g.,autoencoderscanbeview edasaspecialcaseoftranslation,and(b)\nwith moreunsuperviseddata,bothintrinsic andextrinsicm etricsshouldbe improved;skip-thought\nobjectivessatisfy thiscriterionin termsoftheintrinsic metricbutnottheextrinsicone.",
                        "subsection": []
                    }
                ]
            }
        ]
    },
    {
        "id": "4",
        "section": "Experiments",
        "text": "We evaluate the multi-task learning setup on a wide variety o f sequence-to-sequence tasks: con-\nstituency parsing, image caption generation, machine tran slation, and a number of unsupervised\nlearningassummarizedin Table1.",
        "subsection": []
    },
    {
        "id": "5",
        "section": "Conclusion",
        "text": "In this paper, we showed that multi-task learning (MTL) can i mprove the performance of the\nattention-free sequence to sequence model of (Sutskeveret al., 2014). We found it surprising that\ntraining on syntactic parsing and image caption data improv ed our translation performance, given\nthatthesedatasetsareordersofmagnitudesmallerthantyp icaltranslationdatasets. Furthermore,we\nhave established a new state-of-the-art result in constituent parsing with an ensemble of multi-tas k\nmodels. Wealsoshowthatthetwounsupervisedlearningobje ctives,autoencoderandskip-thought,\nbehavedifferentlyintheMTLcontextinvolvingtranslatio n. We hopethattheseinteresting\ufb01ndings\nwill motivate future work in utilizing unsupervised data fo r sequence to sequence learning. A crit-\nicism of our work is that our sequence to sequence modelsdo no t employ the attention mechanism\n(Bahdanauet al.,2015). We leavetheexplorationofMTLwith attentionforfuturework.\nACKNOWLEDGMENTS\nWe thankChrisManningforhelpfulfeedbackonthe paperandm embersofthe GoogleBrain team\nforthoughtfuldiscussionsandinsights.\nREFERENCES\nAndo, Rie Kubota and Zhang, Tong. A frameworkfor learning pr edictive structures from multiple\ntasksandunlabeleddata. JMLR,6:1817\u20131853,2005.\nArgyriou,Andreas,Evgeniou,Theodoros,andPontil,Massi miliano. Multi-taskfeaturelearning. In\nNIPS,2007.\nBahdanau, Dzmitry, Cho, Kyunghyun,and Bengio, Yoshua. Neu ral machine translation by jointly\nlearningtoalignandtranslate. In ICLR, 2015.\nBojar, Ond\u02c7 rej, Chatterjee, Rajen, Federmann, Christian, Haddow, Barry, Huck, Matthias, Hokamp,\nChris, Koehn, Philipp, Logacheva, Varvara, Monz, Christof , Negri, Matteo, Post, Matt, Scarton,\nCarolina,Specia,Lucia,andTurchi,Marco. Findingsofthe 2015workshoponstatisticalmachine\ntranslation. In WMT, 2015.\nCaruana,Rich. Multitasklearning. MachineLearning ,28(1):41\u201375,1997.\nCho, Kyunghyun, van Merrienboer, Bart, Gulcehre, Caglar, B ougares, Fethi, Schwenk, Holger,\nand Bengio, Yoshua. Learningphrase representationsusing RNN encoder-decoderfor statistical\nmachinetranslation. In EMNLP, 2014.\nDai,AndrewM. andLe, QuocV. Semi-supervisedsequencelear ning. InNIPS,2015.\nDonahue,Jeff,Jia,Yangqing,Vinyals,Oriol,Hoffman,Jud y,Zhang,Ning,Tzeng,Eric,andDarrell,\nTrevor. DeCAF: A deepconvolutionalactivationfeaturefor genericvisualrecognition,2014.\nDong, Daxiang, Wu, Hua, He, Wei, Yu, Dianhai, and Wang, Haife ng. Multi-task learning for\nmultiplelanguagetranslation. In ACL,2015.\nEvgeniou, Theodoros and Pontil, Massimiliano. Regularize d multi\u2013task learning. In SIGKDD,\n2004.\nGulcehre, Caglar, Firat, Orhan, Xu, Kelvin, Cho, Kyunghyun , Barrault, Loic, Lin, Huei-Chi,\nBougares,Fethi,Schwenk,Holger,andBengio,Yoshua. Onus ingmonolingualcorporainneural\nmachinetranslation.Publishedasa conferencepaperat ICLR2016\nHeigold,Georg,Vanhoucke,Vincent,Senior,Alan,Nguyen, Patrick,Ranzato,Marc\u2019Aurelio,Devin,\nMatthieu,andDean,Jeffrey.Multilingualacousticmodels usingdistributeddeepneuralnetworks.\nInICASSP,2013.\nHochreiter, Sepp and Schmidhuber, J\u00a8 urgen. Long short-ter m memory. Neural Computation , 9(8):\n1735\u20131780,1997.\nHuang, Jui-Ting, Li, Jinyu, Yu, Dong, Deng, Li, and Gong, Yif an. Cross-language knowledge\ntransferusingmultilingualdeepneuralnetworkwith share dhiddenlayers. In ICASSP,2013.\nJean, S\u00b4 ebastien, Cho, Kyunghyun, Memisevic, Roland, and B engio, Yoshua. On using very large\ntargetvocabularyforneuralmachinetranslation. In ACL,2015a.\nJean,S\u00b4 ebastien,Firat,Orhan,Cho,Kyunghyun,Memisevic ,Roland,andBengio,Yoshua. Montreal\nneuralmachinetranslationsystemsforWMT\u201915. In WMT, 2015b.\nJozefowicz,R., Vinyals,O.,Schuster,M.,Shazeer,N.,and Wu,Y. Exploringthelimitsoflanguage\nmodeling. arXiv preprintarXiv:1602.02410 ,2016.\nKalchbrenner,Nal andBlunsom,Phil. Recurrentcontinuous translationmodels. In EMNLP, 2013.\nKiros, Ryan, Zhu, Yukun, Salakhutdinov, Ruslan, Zemel, Ric hard S., Torralba, Antonio, Urtasun,\nRaquel,andFidler,Sanja. Skip-thoughtvectors. In NIPS,2015.\nKumar, Abhishek and III, Hal Daum\u00b4 e. Learning task grouping and overlap in multi-task learning.\nInICML, 2012.\nLiu, Xiaodong, Gao, Jianfeng, He, Xiaodong, Deng, Li, Duh, K evin, and Wang, Ye-Yi. Represen-\ntationlearningusingmulti-taskdeepneuralnetworksfors emanticclassi\ufb01cation andinformation\nretrieval. In NAACL,2015.\nLuong,Minh-ThangandManning,ChristopherD. Stanfordneu ralmachinetranslationsystemsfor\nspokenlanguagedomain. In IWSLT, 2015.\nLuong,Minh-Thang,Pham, Hieu, and Manning,Christopher D. Effectiveapproachesto attention-\nbasedneuralmachinetranslation. In EMNLP, 2015a.\nLuong, Minh-Thang, Sutskever, Ilya, Le, Quoc V., Vinyals, O riol, and Zaremba, Wojciech. Ad-\ndressingthe rarewordproblemin neuralmachinetranslatio n. InACL,2015b.\nMarcus,MitchellP.,Marcinkiewicz,MaryAnn,andSantorin i,Beatrice. Buildingalargeannotated\ncorpusofenglish: Thepenntreebank. ComputationalLinguistics ,19(2):313\u2013330,1993.\nPapineni, Kishore, Roukos, Salim, Ward, Todd, and jing Zhu, Wei. Bleu: a method for automatic\nevaluationofmachinetranslation. In ACL,2002.\nPham,Vu,Bluche,Th\u00b4 eodore,Kermorvant,Christopher,and Louradour,J\u00b4 er\u02c6 ome. Dropoutimproves\nrecurrent neural networks for handwriting recognition. In Frontiers in Handwriting Recognition\n(ICFHR),201414thInternationalConferenceon ,pp.285\u2013290.IEEE,2014.\nSutskever, Ilya, Vinyals, Oriol, and Le, Quoc V. Sequence to sequence learning with neural net-\nworks. In NIPS,2014.\nThrun,Sebastian. Islearningthen-ththinganyeasier than learningthe\ufb01rst? In NIPS,1996.\nVinyals, Oriol, Kaiser, Lukasz, Koo, Terry, Petrov, Slav, S utskever, Ilya, and Hinton, Geoffrey.\nGrammarasaforeignlanguage. In NIPS,2015a.\nVinyals, Oriol, Toshev, Alexander, Bengio, Samy, and Erhan , Dumitru. Show and tell: A neural\nimagecaptiongenerator. In CVPR,2015b.\nXu,Kelvin,Ba,Jimmy,Kiros,Ryan,Cho,Kyunghyun,Courvil le,AaronC.,Salakhutdinov,Ruslan,\nZemel, Richard S., and Bengio, Yoshua. Show, attend and tell : Neural image caption generation\nwithvisualattention. In",
        "subsection": []
    },
    {
        "missing": [
            "92.4 F 1, our multi-task system is on par with the best single system r eported in (Vinyalset al.,"
        ]
    },
    {
        "references": []
    },
    {
        "title": "Multi-task Sequence to Sequence Learning",
        "arxiv_id": "1511.06114"
    }
]