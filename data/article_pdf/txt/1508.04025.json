[
    {
        "id": "",
        "section": "Abstract",
        "text": "An attentional mechanism has lately been\nused to improve neural machine transla-\ntion (NMT) by selectively focusing on\nparts of the source sentence during trans-\nlation. However, there has been little\nwork exploring useful architectures for\nattention-based NMT. This paper exam-\ninestwosimpleandeffectiveclassesofat-\ntentional mechanism",
        "subsection": []
    },
    {
        "id": "1",
        "section": "Introduction",
        "text": "Neural Machine Translation (NMT) achieved\nstate-of-the-art performances in large-scale trans-\nlation tasks such as from English to French\n(Luong et al., 2015) and English to German\n(Jean et al., 2015). NMT is appealing since it re-\nquires minimal domain knowledge and is concep-\ntually simple. The model by Luong et al. (2015)\nreadsthroughallthesourcewordsuntiltheend-of-\nsentence symbol <eos>isreached. It then starts\n1All our code and models are publicly available at\nhttp://nlp.stanford.edu/projects/nmt .B C D <eos> X Y ZXY Z <eos>\nA\nFigure 1: Neural machine translation \u2013 a stack-\ning recurrent architecture for translating a source\nsequenceA B C D into a target sequence X Y\nZ.Here,<eos>marks the end of asentence.\nemittingonetarget wordatatime,asillustrated in\nFigure1. NMTisoftenalargeneuralnetworkthat\nistrainedinanend-to-endfashionandhastheabil-\nitytogeneralizewelltoverylongwordsequences.\nThis means the model does not have to explicitly\nstore gigantic phrase tables and language models\nas in the case of standard MT; hence, NMT has\na small memory footprint. Lastly, implementing\nNMT decoders is easy unlike the highly intricate\ndecoders in standard MT(Koehn et al., 2003).\nIn parallel, the concept of \u201cattention\u201d has\ngained popularity recently in training neural net-\nworks, allowing models to learn alignments be-\ntween different modalities, e.g., between image\nobjects and agent actions in the dynamic con-\ntrol problem (Mnih et al., 2014), between speech\nframes and text in the speech recognition task\n(?), or between visual features of a picture and\nits text description in the image caption gener-\nation task (Xuet al.,2015). In the context of\nNMT,Bahdanau et al. (2015) has successfully ap-\nplied such attentional mechanism to jointly trans-\nlate and align words. To the best of our knowl-\nedge, there has not been any other workexploring\ntheuse of attention-based architectures for NMT.\nfectiveness in mind, two novel types of attention-\nbased models: a globalapproach in which all\nsourcewordsareattendedanda localonewhereby\nonly a subset of source words are considered at a\ntime. The former approach resembles the model\nof (Bahdanau et al., 2015) but is simpler architec-\nturally. The latter can be viewed as an interesting\nblend between the hardandsoftattention models\nproposed in (Xuet al., 2015): it is computation-\nally less expensive than the global model or the\nsoft attention; atthesametime, unlike thehardat-\ntention, the local attention is differentiable almost\neverywhere, making it easier to implement and\ntrain.2Besides, we also examine various align-\nment functions for our attention-based models.\nExperimentally, we demonstrate that both of\nour approaches are effective in the WMT trans-\nlation tasks between English and German in both\ndirections. Our attentional models yield a boost\nof up to 5.0 BLEU over non-attentional systems\nwhich already incorporate known techniques such\nas dropout. For English to German translation,\nwe achieve new state-of-the-art (SOTA) results\nfor both WMT\u201914 and WMT\u201915, outperforming\nprevious SOTA systems, backed by NMT mod-\nels andn-gram LM rerankers, by more than 1.0\nBLEU. We conduct extensive analysis to evaluate\nourmodelsintermsoflearning, theability tohan-\ndlelongsentences, choices ofattentional architec-\ntures, alignment quality, and translation outputs.",
        "subsection": []
    },
    {
        "id": "2",
        "section": "Neural machinetranslation",
        "text": "A neural machine translation system is a neural\nnetworkthat directly modelstheconditional prob-\nabilityp(y|x)of translating a source sentence,\nx1,...,x n, to a target sentence, y1,...,ym.3A\nbasic form of NMT consists of two components:\n(a) anencoderwhich computes a representation s\nfor each source sentence and (b) a decoderwhich\ngenerates one target word at a time and hence de-\ncomposes the conditional probability as:\nlogp(y|x) =/summationdisplaym\nj=1logp(yj|y<j,s)(1)\nA natural choice to model such a de-\ncomposition in the decoder is to use a\n2There is a recent work by Gregor etal. (2015), which is\nvery similar to our local attention and applied to the image\ngeneration task. However, as we detail later, our model is\nmuch simpler andcan achieve good performance for NMT.\n3All sentences are assumed to terminate with a special\n\u201cend-of-sentence\u201d token <eos>.recurrent neural network (RNN) architec-\nture, which most of the recent NMT work\nsuch as (Kalchbrenner and Blunsom, 2013;\nSutskever et al., 2014; Choet al.,2014;\nBahdanau et al., 2015; Luong et al.,2015;\nJean et al., 2015) have in common. They, how-\never, differ in terms of which RNN architectures\nare used for the decoder and how the encoder\ncomputes the source sentence representation s.\nKalchbrenner and Blunsom (2013) used an\nRNN with the standard hidden unit for the\ndecoder and a convolutional neural network for\nencoding the source sentence representation. On\nthe other hand, both Sutskever et al. (2014) and\nLuong et al. (2015) stacked multiple layers of an\nRNN with a Long Short-Term Memory (LSTM)\nhidden unit for both the encoder and the decoder.\nChoet al. (2014), Bahdanau et al. (2015), and\nJean et al. (2015)alladoptedadifferent versionof\nthe RNN with an LSTM-inspired hidden unit, the\ngatedrecurrentunit(GRU),forbothcomponents.4\nIn more detail, one can parameterize the proba-\nbility of decoding each word yjas:\np(yj|y<j,s) = softmax( g(hj))(2)\nwithgbeing the transformation function that out-\nputs a vocabulary-sized vector.5Here,hjis the\nRNNhidden unit, abstractly computed as:\nhj=f(hj\u22121,s), (3)\nwherefcomputes the current hidden state\ngiven the previous hidden state and can be\neither a vanilla RNN unit, a GRU, or an LSTM\nunit. In (Kalchbrenner and Blunsom, 2013;\nSutskever et al., 2014; Choet al.,2014;\nLuong et al., 2015), the source representa-\ntionsis only used once to initialize the\ndecoder hidden state. On the other hand, in\n(Bahdanau et al.,2015; Jean et al., 2015) and\nthis work, s, in fact, implies a set of source\nhidden states which are consulted throughout the\nentire course of the translation process. Such an\napproach isreferred toasanattention mechanism,\nwhich wewill discuss next.\nIn this work, following (Sutskever et al.,2014;\nLuong et al., 2015), we use the stacking LSTM\narchitecture for our NMT systems, as illustrated\n4TheyallusedasingleRNNlayerexceptforthelattertwo\nworks whichutilizeda bidirectional RNNfor the encoder.\n5Onecanprovide gwithotherinputssuchasthecurrently\npredictedword yjin Figure 1. We use the LSTM unit de\ufb01ned in\n(Zaremba et al., 2015). Our training objective is\nformulated asfollows:\nJt=/summationdisplay\n(x,y)\u2208D\u2212logp(y|x)(4)\nwithDbeing our parallel training corpus.",
        "subsection": [
            {
                "id": "3.1",
                "section": "Global attention",
                "text": "The idea of a global attentional model is to con-\nsider all thehidden states of the encoder when de-\nriving the context vector ct. In this model type,\na variable-length alignment vector at, whose size\nequalsthenumberoftimestepsonthesourceside,\nis derived by comparing the current target hidden\nstatehtwith each source hidden state \u00afhs:\nat(s) = align( ht,\u00afhs) (7)\n=exp/parenleftbig\nscore(ht,\u00afhs)/parenrightbig\n/summationtext\ns\u2032exp/parenleftbig\nscore(ht,\u00afhs\u2032)/parenrightbigyt\n\u02dcht\nct\nat\nht\u00afhsGlobal align weightsAttention Layer\nContext vector\nFigure2: Globalattentionalmodel \u2013ateachtime\nstept, the model infers a variable-length align-\nment weight vector atbased on the current target\nstatehtand all source states \u00afhs. A global context\nvectorctis then computed as the weighted aver-\nage, according to at, over all the source states.\nHere,scoreisreferredasa content-based function\nfor which weconsider three different alternatives:\nscore(ht,\u00afhs)=\uf8f1\n\uf8f4\uf8f2\n\uf8f4\uf8f3h\u22a4\nt\u00afhs dot\nh\u22a4\ntWa\u00afhs general\nv\u22a4\natanh/parenleftbig\nWa[ht;\u00afhs]/parenrightbig\nconcat\nBesides,inourearlyattemptstobuildattention-\nbased models, we use a location-based function\nin which the alignment scores are computed from\nsolely thetarget hidden state htasfollows:\nat= softmax( Waht) location (8)\nGiventhealignment vectorasweights,thecontext\nvectorctiscomputedastheweightedaverageover\nall the source hidden states.6\nComparisonto(Bahdanau et al., 2015) \u2013While\nour global attention approach is similar in spirit\nto the model proposed by Bahdanau et al. (2015),\nthereareseveralkeydifferenceswhichre\ufb02ecthow\nwe have both simpli\ufb01ed and generalized from\nthe original model. First, we simply use hid-\nden states at the top LSTM layers in both the\nencoder and decoder as illustrated in Figure 2.\nBahdanau et al. (2015), on the other hand, use\nthe concatenation of the forward and backward\nsource hidden states in the bi-directional encoder\n6Eq. (8) implies that all alignment vectors atare of the\nsame length. For short sentences, we only use the top part of\natyt\n\u02dcht\nct\nat\nhtpt\n\u00afhsAttention Layer\nContext vector\nLocal weightsAligned position\nFigure3: Local attention model \u2013themodel \ufb01rst\npredictsasinglealignedposition ptforthecurrent\ntargetword. Awindowcenteredaroundthesource\npositionptis then used to compute a context vec-\ntorct, a weighted average of the source hidden\nstates in the window. The weights atare inferred\nfrom the current target state htand those source\nstates\u00afhsinthe window.\nand target hidden states in their non-stacking uni-\ndirectionaldecoder. Second,ourcomputationpath\nis simpler; we go from ht\u2192at\u2192ct\u2192\u02dcht\nthen make a prediction as detailed in Eq. (5),\nEq. (6), and Figure 2. On the other hand, at\nany time t, Bahdanau et al. (2015) build from the\nprevious hidden state ht\u22121\u2192at\u2192ct\u2192\nht, which, in turn, goes through a deep-output\nand a maxout layer before making predictions.7\nLastly, Bahdanau et al. (2015) only experimented\nwith one alignment function, the concatproduct;\nwhereas we show later that the other alternatives\nare better.",
                "subsection": []
            },
            {
                "id": "3.2",
                "section": "Local attention",
                "text": "The global attention has a drawback that it has to\nattend to all words on the source side for each tar-\nget word, which is expensive and can potentially\nrender itimpractical totranslate longer sequences,\ne.g., paragraphs or documents. To address this\nde\ufb01ciency, we propose a localattentional mech-\nanism thatchooses tofocusonlyonasmall subset\nof the source positions per target word.\nThis model takes inspiration from the tradeoff\nbetween the softandhardattentional models pro-\nposed by Xuet al. (2015) to tackle the image cap-\ntion generation task. In their work, soft attention\n7We willrefer tothis difference againinSection3.3.refers to the global attention approach in which\nweights are placed \u201csoftly\u201d over all patches in the\nsource image. The hard attention, on the other\nhand,selects onepatchoftheimagetoattendtoat\natime. Whilelessexpensive atinference time,the\nhard attention model is non-differentiable and re-\nquires more complicated techniques such as vari-\nance reduction or reinforcement learning totrain.\nOur local attention mechanism selectively fo-\ncuses on a small window of context and is differ-\nentiable. Thisapproachhasanadvantageofavoid-\ningthe expensive computation incurred in thesoft\nattention and at the same time, is easier to train\nthan the hard attention approach. In concrete de-\ntails, the model \ufb01rst generates an aligned position\nptfor each target word at time t. Thecontext vec-\ntorctis then derived as a weighted average over\nthe set of source hidden states within the window\n[pt\u2212D,pt+D];Disempiricallyselected.8Unlike\nthe global approach, the local alignment vector at\nis now \ufb01xed-dimensional, i.e., \u2208R2D+1. Wecon-\nsider twovariants of the model asbelow.\nMonotonic alignment ( local-m)\u2013wesimplyset\npt=tassuming that source and target sequences\nareroughlymonotonically aligned. Thealignment\nvectoratisde\ufb01ned according toEq. (7).9\nPredictive alignment ( local-p) \u2013 instead of as-\nsumingmonotonic alignments, ourmodelpredicts\nanaligned position as follows:\npt=S\u00b7sigmoid(v\u22a4\nptanh(Wpht)),(9)\nWpandvpare the model parameters which will\nbelearnedtopredictpositions. Sisthesourcesen-\ntence length. As a result of sigmoid,pt\u2208[0,S].\nTo favor alignment points near pt, we place a\nGaussian distribution centered around pt. Specif-\nically, our alignment weights are now de\ufb01ned as:\nat(s) = align( ht,\u00afhs)exp/parenleftbigg\n\u2212(s\u2212pt)2\n2\u03c32/parenrightbigg\n(10)\nWe use the same alignfunction as in Eq. (7) and\nthe standard deviation is empirically set as \u03c3=D\n2.\nNote that ptis arealnummber; whereas sis an\nintegerwithin the window centered at pt.10\n8If the window crosses the sentence boundaries, we sim-\nplyignoretheoutsidepartandconsiderwordsinthewindow.\n9local-mis the same as the global model except that the\nvectoratis \ufb01xed-lengthand shorter.\n10local-pissimilartothelocal-mmodelexceptthatwedy-\nnamically compute ptand use a truncated Gaussian distribu-\ntion to modify the original alignment weights align(ht,\u00afhs)\nas shown in Eq. (10). By utilizing ptto derive at, we can\ncompute backprop gradients for Wpandvp. This model is\n\u02dcht\nAttention Layer\nB C D <eos> X Y ZXY Z <eos>\nA\nFigure 4: Input-feeding approach \u2013 Attentional\nvectors\u02dchtarefedasinputstothenexttimestepsto\ninform the model about past alignment decisions.\nComparisonto(Gregor et al., 2015) \u2013havepro-\nposed aselective attention mechanism, very simi-\nlar to our local attention, for the image generation\ntask. Their approach allows themodel toselect an\nimage patch of varying location and zoom. We,\ninstead, use the same \u201czoom\u201d for all target posi-\ntions,whichgreatlysimpli\ufb01estheformulationand\nstill achieves good performance.",
                "subsection": []
            },
            {
                "id": "3.3",
                "section": "Input-feedingapproach",
                "text": "In our proposed global and local approaches,\nthe attentional decisions are made independently,\nwhich is suboptimal. Whereas, in standard MT,\nacoverage set is often maintained during the\ntranslation process to keep track of which source\nwords have been translated. Likewise, in atten-\ntional NMTs,alignment decisions shouldbemade\njointly taking into account past alignment infor-\nmation. To address that, we propose an input-\nfeedingapproach in which attentional vectors \u02dcht\nareconcatenated withinputs at thenext timesteps\nas illustrated in Figure 4.11The effects of hav-\ning such connections are two-fold: (a) we hope\nto make the model fully aware of previous align-\nment choices and (b) we create a very deep net-\nwork spanning both horizontally and vertically.\nComparison to other work \u2013\nBahdanau et al. (2015) use context vectors,\nsimilar to our ct, in building subsequent hidden\nstates, which can also achieve the \u201ccoverage\u201d\neffect. However, there has not been any analysis\nof whether such connections are useful as done\n11Ifnis the number of LSTM cells, the input size of the\n\ufb01rstLSTMlayer is 2n; those of subsequent layers are n.in this work. Also, our approach is more general;\nas illustrated in Figure 4, it can be applied to\ngeneral stacking recurrent architectures, including\nnon-attentional models.\nXuet al. (2015) propose a doubly attentional\napproach with an additional constraint added to\nthetraining objective tomakesurethemodel pays\nequal attention to all parts of the image during the\ncaption generation process. Such a constraint can\nalso be useful to capture the coverage set effect\nin NMT that we mentioned earlier. However, we\nchose to use the input-feeding approach since it\nprovides \ufb02exibility for the model todecide on any\nattentional constraints it deems suitable.",
                "subsection": []
            }
        ]
    },
    {
        "id": "3",
        "section": "Attention-based models",
        "text": "Our various attention-based models are classifed\nintotwobroad categories, globalandlocal. These\nclasses differ in terms of whether the \u201cattention\u201d\nis placed on all source positions or on only a few\nsource positions. We illustrate these two model\ntypes inFigure 2and 3 respectively.\nCommontothesetwotypesofmodelsisthefact\nthatateachtimestep tinthedecoding phase,both\napproaches \ufb01rst take as input the hidden state ht\nat the top layer of a stacking LSTM. The goal is\nthentoderiveacontext vector ctthat captures rel-\nevant source-side information to help predict the\ncurrent target word yt. While these models differ\nin how the context vector ctis derived, they share\nthe samesubsequent steps.\nSpeci\ufb01cally,giventhetargethiddenstate htand\nthe source-side context vector ct, we employ a\nsimple concatenation layer to combine the infor-\nmationfrombothvectorstoproduceanattentional\nhidden state asfollows:\n\u02dcht= tanh(Wc[ct;ht]) (5)\nTheattentional vector \u02dchtisthenfedthroughthe\nsoftmax layer to produce the predictive distribu-\ntion formulated as:\np(yt|y<t,x) = softmax( Ws\u02dcht)(6)\nWe now detail how each model type computes\nthe source-side context vector ct.",
        "subsection": [
            {
                "id": "4.1",
                "section": "Training details",
                "text": "All our models are trained on the WMT\u201914 train-\ningdataconsisting of4.5Msentencespairs(116M\nEnglish words, 110M German words). Similar\nto (Jean et al., 2015), we limit our vocabularies to\nbe the top 50K most frequent words for both lan-\nguages. Words not in these shortlisted vocabular-\niesare converted into a universal token <unk>.\nWhen training our NMT systems, following\n(Bahdanau et al.,2015; Jean et al., 2015), we \ufb01l-\nter out sentence pairs whose lengths exceed\n50 words and shuf\ufb02e mini-batches as we pro-\nceed. Our stacking LSTM models have 4 lay-\ners, each with 1000 cells, and 1000-dimensional\nembeddings. We follow (Sutskever et al.,2014;\nLuong et al., 2015) in training NMT with similar\nsettings: (a) our parameters are uniformly initial-\nized in[\u22120.1,0.1], (b) we train for 10 epochs us-\n12All texts are tokenized with tokenizer.perl and\nBLEUscores are computed with multi-bleu.perl .\n13Withthemteval-v13aSystem Ppl BLEU\nWinning WMT\u201914system \u2013 phrase-based +large LM (Bucket al., 2014) 20.7\nExisting NMTsystems\nRNNsearch (Jean et al., 2015) 16.5\nRNNsearch +unk replace (Jean etal., 2015) 19.0\nRNNsearch +unk replace +large vocab + ensemble 8models (Jean etal., 2015) 21.6\nOurNMTsystems\nBase 10.6 11.3\nBase +reverse 9.9 12.6 ( +1.3)\nBase +reverse +dropout 8.1 14.0 ( +1.4)\nBase +reverse +dropout +global attention ( location) 7.3 16.8 ( +2.8)\nBase +reverse +dropout +global attention ( location) +feed input 6.4 18.1 ( +1.3)\nBase +reverse +dropout +local-p attention ( general) +feed input5.919.0 (+0.9)\nBase +reverse +dropout +local-p attention ( general) +feed input +unk replace 20.9 ( +1.9)\nEnsemble 8models +unk replace 23.0 (+2.1)\nTable1:WMT'14 English-German results \u2013shown arethe perplexities (ppl) and the tokenized BLEU\nscores of various systems on newstest2014. We highlight the bestsystem in bold and give progressive\nimprovementsinitalicbetweenconsecutivesystems. local-preferestothelocalattentionwithpredictive\nalignments. Weindicate for each attention model the alignm ent score function used inpararentheses.\ning plain SGD, (c) a simple learning rate sched-\nule is employed \u2013 we start with a learning rate of\n1; after 5 epochs, we begin to halve the learning\nrate every epoch, (d) our mini-batch size is 128,\nand (e) the normalized gradient is rescaled when-\never its norm exceeds 5. Additionally, we also\nusedropoutwithprobability 0.2forourLSTMsas\nsuggested by (Zaremba et al., 2015). For dropout\nmodels, we train for 12 epochs and start halving\nthe learning rate after 8 epochs. For local atten-\ntion models, we empirically set the window size\nD= 10.\nOur code is implemented in MATLAB. When\nrunning on a single GPU device Tesla K40, we\nachieve a speed of 1K targetwords per second.\nIt takes 7\u201310 days tocompletely train amodel.",
                "subsection": []
            },
            {
                "id": "4.2",
                "section": "English-german results",
                "text": "We compare our NMT systems in the English-\nGerman task with various other systems. These\ninclude the winning system in WMT\u201914\n(Buck et al., 2014), a phrase-based system\nwhose language models were trained on a huge\nmonolingual text, the Common Crawl corpus.\nFor end-to-end NMT systems, to the best of\nour knowledge, (Jean et al., 2015) is the only\nwork experimenting with this language pair and\ncurrently the SOTA system. We only present\nresults for some of our attention models and will\nlater analyze therest in Section 5.\nAs shown in Table 1, we achieve pro-gressive improvements when (a) reversing the\nsource sentence, + 1.3BLEU, as proposed in\n(Sutskever et al., 2014) and (b) using dropout,\n+1.4BLEU. On top of that, (c) the global atten-\ntion approach gives a signi\ufb01cant boost of + 2.8\nBLEU, making our model slightly better than the\nbase attentional system of Bahdanau et al. (2015)\n(rowRNNSearch ). When (d) using the input-\nfeedingapproach, we seize another notable gain\nof +1.3BLEU and outperform their system. The\nlocal attention model with predictive alignments\n(rowlocal-p) proves to be even better, giving\nus a further improvement of + 0.9BLEU on top\nof the global attention model. It is interest-\ning to observe the trend previously reported in\n(Luong et al., 2015)thatperplexitystronglycorre-\nlates with translation quality. In total, we achieve\na signi\ufb01cant gain of 5.0 BLEU points over the\nnon-attentional baseline, which already includes\nknown techniques such as source reversing and\ndropout.\nThe unknown replacement technique proposed\nin(Luong et al., 2015;Jean et al.,2015)yieldsan-\nother nice gain of + 1.9BLEU,demonstrating that\nour attentional models do learn useful alignments\nfor unknown works. Finally, by ensembling 8\ndifferent models of various settings, e.g., using\ndifferent attention approaches, with and without\ndropout etc., wewere able to achieve a new SOTA\nresult of 23.0best system (Jean et al., 2015) by + 1.4BLEU.\nSystem BLEU\nTop\u2013NMT+5-gram rerank (Montreal) 24.9\nOur ensemble 8models + unk replace 25.9\nTable 2: WMT'15 English-German results \u2013\nNISTBLEU scores of the winning entry in\nWMT\u201915and our best one on newstest2015.\nLatest results in WMT\u201915 \u2013despite the fact that\nourmodelsweretrainedonWMT\u201914withslightly\nlessdata,wetestthemonnewstest2015 todemon-\nstratethattheycangeneralize welltodifferent test\nsets. As shown in Table 2, our best system es-\ntablishes a newSOTA performance of 25.9BLEU,\noutperforming the existing best system backed by\nNMTand a5-gram LMreranker by+ 1.0BLEU.",
                "subsection": []
            },
            {
                "id": "4.3",
                "section": "German-english results",
                "text": "We carry out a similar set of experiments for the\nWMT\u201915 translation task from German to En-\nglish. While our systems have not yet matched\nthe performance of the SOTA system, we never-\ntheless show the effectiveness of our approaches\nwithlargeandprogressivegainsintermsofBLEU\nas illustrated in Table 3. The attentional mech-\nanism gives us + 2.2BLEU gain and on top of\nthat, we obtain another boost of up to + 1.0BLEU\nfrom the input-feeding approach. Using a better\nalignment function, the content-based dotproduct\none, together with dropoutyields another gain of\n+2.7BLEU. Lastly, when applying the unknown\nword replacement technique, we seize an addi-\ntional +2.1BLEU, demonstrating the usefulness\nof attention in aligning rare words.",
                "subsection": []
            }
        ]
    },
    {
        "id": "4",
        "section": "Experiments",
        "text": "We evaluate the effectiveness of our models\non the WMT translation tasks between En-\nglish and German in both directions. new-\nstest2013 (3000 sentences) is used as a develop-\nment set to select our hyperparameters. Transla-\ntion performances are reported in case-sensitive\nBLEU (Papineni et al., 2002) on newstest2014\n(2737 sentences) and newstest2015 (2169 sen-\ntences). Following (Luong et al., 2015), wereport\ntranslation quality using two types of BLEU: (a)\ntokenized12BLEUto be comparable with existing\nNMT work and (b) NIST13BLEU to be compara-\nblewith WMTresults.",
        "subsection": [
            {
                "id": "5.1",
                "section": "Learningcurves",
                "text": "Wecompare modelsbuilt ontopofone another as\nlisted in Table 1. It is pleasant to observe in Fig-\nure 5 a clear separation between non-attentional\nand attentional models. The input-feeding ap-\nproach and the local attention model also demon-\nstrate their abilities in driving the test costs lower.\nThe non-attentional model with dropout (the blueSystem Ppl. BLEU\nWMT\u201915systems\nSOTA\u2013phrase-based (Edinburgh) 29.2\nNMT+ 5-gram rerank (MILA) 27.6\nOurNMTsystems\nBase (reverse) 14.3 16.9\n+ global ( location) 12.7 19.1 ( +2.2)\n+ global ( location) + feed 10.9 20.1 ( +1.0)\n+ global ( dot) + drop + feed9.722.8 (+2.7)\n+ global ( dot) + drop + feed + unk 24.9 ( +2.1)\nTable 3: WMT'15 German-English results \u2013\nperformances of various systems (similar to Ta-\nble 1). The basesystem already includes source\nreversing on which we add globalattention,\ndropout, input feeding, andunkreplacement.\n0.20.40.60.811.21.41.61.8\nx 10523456\nMini\u2212batchesTest cost\n  \nbasic\nbasic+reverse\nbasic+reverse+dropout\nbasic+reverse+dropout+globalAttn\nbasic+reverse+dropout+globalAttn+feedInput\nbasic+reverse+dropout+pLocalAttn+feedInput\nFigure5: Learningcurves \u2013test cost ( lnperplex-\nity) on newstest2014 for English-German NMTs\nastraining progresses.\n+ curve) learns slower than other non-dropout\nmodels, but as time goes by, it becomes more ro-\nbust in termsof minimizing test errors.",
                "subsection": []
            },
            {
                "id": "5.2",
                "section": "Effects of translating longsentences",
                "text": "We follow (Bahdanau et al., 2015) to group sen-\ntences of similar lengths together and compute\na BLEU score per group. Figure 6 shows that\nour attentional models are more effective than the\nnon-attentional one in handling long sentences:\nthe quality does not degrade as sentences become\nlonger. Our best model (the blue + curve) outper-\nformsall other systems in all length buckets.",
                "subsection": []
            },
            {
                "id": "5.3",
                "section": "Choices of attentional architectures",
                "text": "We examine different attention models ( global,\nlocal-m, local-p ) and different alignment func-\ntions (location, dot, general, concat ) as described\nin Section 3. Due to limited resources, we can-\nnot run all the possible combinations. However,\nresults in Table 4 do give us some idea about dif-\nferent choices. The location-based10 20 30 40 50 60 7010152025\nSent LengthsBLEU\t\t\t\t\t\n  \nours, no attn (BLEU 13.9)\nours, local\u2212p attn (BLEU 20.9)\nours, best system (BLEU 23.0)\nWMT\u201914 best (BLEU 20.7)\nJeans et al., 2015 (BLEU 21.6)\nFigure 6: Length Analysis \u2013 translation qualities\nof different systems assentences become longer.\nSystem PplBLEU\nBefore After unk\nglobal (location) 6.4 18.1 19.3 (+1.2)\nglobal (dot) 6.1 18.6 20.5 (+1.9)\nglobal (general) 6.1 17.3 19.1 (+1.8)\nlocal-m (dot) >7.0 x x\nlocal-m (general) 6.2 18.6 20.4 (+1.8)\nlocal-p (dot) 6.6 18.0 19.6 (+1.9)\nlocal-p (general) 5.9 19 20.9(+1.9)\nTable 4: Attentional Architectures \u2013 perfor-\nmancesofdifferentattentionalmodels. Wetrained\ntwolocal-m (dot) models; both have ppl >7.0.\nnot learn good alignments: the global (location)\nmodel can only obtain a small gain when per-\nforming unknown word replacement compared to\nusing other alignment functions.14Forcontent-\nbasedfunctions, our implementation concatdoes\nnot yield good performances and more analysis\nshould be done to understand the reason.15It is\ninteresting to observe that dotworks well for the\nglobal attention and generalis better for the local\nattention. Among the different models, the local\nattention model withpredictive alignments ( local-\np)isbest, bothintermsofperplexities andBLEU.",
                "subsection": []
            },
            {
                "id": "5.4",
                "section": "Alignmentquality",
                "text": "Aby-productofattentionalmodelsarewordalign-\nments. While (Bahdanau et al., 2015) visualized\n14There is a subtle difference in how we retrieve align-\nments for the different alignment functions. At time step tin\nwhich we receive yt\u22121as input and then compute ht,at,ct,\nand\u02dchtbefore predicting yt, the alignment vector atis used\nas alignment weights for (a) the predicted word ytin the\nlocation-based alignment functions and (b) the input word\nyt\u22121inthecontent-based functions.\n15Withconcat, the perplexities achieved bydifferent mod-\nels are 6.7 (global), 7.1 (local-m), and 7.1 (local-p). Such\nhighperplexities couldbedue tothefactthatwesimplifyth e\nmatrixWatoset the part that corresponds to \u00afhstoidentity.Method AER\nglobal (location) 0.39\nlocal-m (general) 0.34\nlocal-p (general) 0.36\nensemble 0.34\nBerkeley Aligner 0.32\nTable 6:AER scores \u2013 results of various models\nonthe RWTHEnglish-German alignment data.\nalignments for some sample sentences and ob-\nserved gains in translation quality as an indica-\ntionofaworkingattention model,noworkhasas-\nsessed the alignments learned as a whole. In con-\ntrast, we set out to evaluate the alignment quality\nusing the alignment error rate (AER)metric.\nGiven the gold alignment data provided by\nRWTH for 508 English-German Europarl sen-\ntences, we \u201cforce\u201d decode our attentional models\nto produce translations that match the references.\nWe extract only one-to-one alignments by select-\ning the source word with the highest alignment\nweight per target word. Nevertheless, as shown in\nTable6,wewereabletoachieveAERscorescom-\nparable to the one-to-many alignments obtained\nbythe Berkeley aligner (Liang et al., 2006).16\nWe also found that the alignments produced by\nlocal attention models achieve lower AERs than\nthose of the global one. The AERobtained by the\nensemble, while good, is not better than the local-\nm AER, suggesting the well-known observation\nthat AER and translation scores are not well cor-\nrelated (Fraser and Marcu, 2007). We show some\nalignment visualizations inAppendix A.",
                "subsection": []
            },
            {
                "id": "5.5",
                "section": "Sampletranslations",
                "text": "We show in Table 5 sample translations in both\ndirections. It it appealing to observe the ef-\nfect of attentional models in correctly translating\nnames such as \u201cMiranda Kerr\u201dand \u201cRoger Dow\u201d.\nNon-attentional models, while producing sensi-\nble names from a language model perspective,\nlack the direct connections from the source side\nto make correct translations. We also observed\nan interesting case in the second example, which\nrequires translating the doubly-negated phrase,\n\u201cnot incompatible\u201d. The attentional model cor-\nrectly produces \u201cnicht ...unvereinbar\u201d; whereas\nthenon-attentional model generates \u201cnicht verein-\n16Weconcatenatethe508sentencepairswith1Msentence\nEnglish-German translations\nsrc Orlando Bloom and Miranda Kerr still loveeach other\nref Orlando Bloom und Miranda Kerr lieben sich noch immer\nbestOrlando Bloom und Miranda Kerr lieben einander noch immer.\nbase Orlando Bloom und LucasMiranda lieben einander noch immer.\nsrc\u2032\u2032We\u2032re pleased the FAA recognizes that an enjoyable passenger ex perience is not incompatible\nwithsafety and security ,\u2032\u2032said Roger Dow, CEOof the U.S.Travel Association .\nref \u201cWir freuen uns , dass die FAAerkennt , dass ein angenehme s Passagiererlebnis nicht im Wider-\nspruch zur Sicherheit steht \u201d,sagte Roger Dow , CEOder U.S.Travel Association .\nbest\u2032\u2032Wir freuen uns , dass die FAA anerkennt , dass ein angenehmes i st nicht mit Sicherheit und\nSicherheit unvereinbar ist\u2032\u2032, sagteRoger Dow ,CEOder US- die .\nbase\u2032\u2032Wirfreuenuns \u00a8 uberdie <unk>,dassein <unk><unk>mitSicherheit nicht vereinbar istmit\nSicherheit und Sicherheit\u2032\u2032,sagteRogerCameron ,CEOder US- <unk>.\nGerman-English translations\nsrc Ineinem Interview sagte Bloom jedoch ,dass er und Kerr si ch noch immerlieben .\nref However ,in aninterview , Bloom has said that he and Kerrstill love each other .\nbestInan interview ,however ,Bloom said that heand Kerrstill love.\nbase However ,in aninterview , Bloom said that he and Tinawerestill <unk>.\nsrc Wegen der von Berlin und der Europ\u00a8 aischen Zentralbank v erh\u00a8 angten strengen Sparpolitik in\nVerbindung mit der Zwangsjacke , in die die jeweilige nation ale Wirtschaft durch das Festhal-\nten an der gemeinsamen W\u00a8 ahrung gen\u00a8 otigt wird , sind viele M enschen der Ansicht , das Projekt\nEuropa sei zuweit gegangen\nref The austerity imposed by Berlin and the European Central Bank , c oupled with the straitjacket\nimposedonnational economies through adherence tothecomm oncurrency ,hasledmanypeople\ntothink Project Europe has gone too far .\nbestBecause of the strict austerity measures imposed by Berlin and the European Centr al Bank in\nconnection with the straitjacket in which the respective national economy is forced to adhere to\nthe common currency ,many people believe that theEuropean p roject has gone too far .\nbase Becauseofthepressure imposedbytheEuropeanCentralBankandtheFederalCentral Bank\nwith the strict austerity imposed on the national economy in the face of the single curr ency ,\nmanypeople believe that the European project has gone too fa r .\nTable5:Sampletranslations \u2013foreach example, weshow thesource ( src), thehumantranslation ( ref),\nthe translation from our best model ( best), and the translation of a non-attentional model ( base). We\nitalicize some correcttranslation segments and highlight a few wrongones in bold.\nbar\u201d,meaning\u201cnotcompatible\u201d.17Theattentional\nmodelalsodemonstratesitssuperiority intranslat-\ning long sentences as inthe last example.",
                "subsection": []
            }
        ]
    },
    {
        "id": "5",
        "section": "Analysis",
        "text": "Weconductextensiveanalysistobetterunderstand\nourmodelsintermsoflearning, theability tohan-\ndlelongsentences, choices ofattentional architec-\ntures, and alignment quality. All results reported\nhere are on English-German newstest2014.",
        "subsection": []
    },
    {
        "id": "6",
        "section": "Conclusion",
        "text": "Inthispaper, wepropose twosimple andeffective\nattentional mechanisms for neural machine trans-\nlation: the globalapproach which always looks\nat all source positions and the localone that only\nattends to a subset of source positions at a time.\nWe test the effectiveness of our models in the\nWMT translation tasks between English and Ger-\nman in both directions. Our local attention yields\nlargegainsofupto 5.0BLEUovernon-attentional\n17The reference uses a more fancy translation of \u201cincom-\npatible\u201d, which is \u201cim Widerspruch zu etwas stehen\u201d. Both\nmodels, however, failedtotranslate \u201cpassenger experienc e\u201d.models which already incorporate known tech-\nniques such as dropout. For the English to Ger-\nmantranslation direction, ourensemblemodelhas\nestablished new state-of-the-art results for both\nWMT\u201914 and WMT\u201915, outperforming existing\nbestsystems, backedbyNMTmodelsand n-gram\nLMrerankers, by more than 1.0BLEU.\nWehave compared various alignment functions\nand shed light on which functions are best for\nwhichattentional models. Ouranalysisshowsthat\nattention-based NMT models are superior to non-\nattentional ones in many cases, for example in\ntranslating names and handling long sentences.\nAcknowledgment\nWe gratefully acknowledge support from a gift\nCorporationwiththedonationofTeslaK40GPUs.\nWe thank Andrew Ng and his group as well as\nthe Stanford Research Computing for letting us\nuse their computing resources. We thank Rus-\nsell Stewart forhelpful discussions onthemodels.\nLastly, we thank Quoc Le, Ilya Sutskever, Oriol\nVinyals, Richard Socher, Michael Kayser, Jiwei\nLi, Panupong Pasupat, Kelvin Guu, members of\nthe Stanford NLP Group and the annonymous re-\nviewersfortheirvaluablecommentsandfeedback.",
        "subsection": []
    },
    {
        "missing": []
    },
    {
        "references": []
    },
    {
        "title": "Effective Approaches to Attention-based Neural Machine Translation",
        "arxiv_id": "1508.04025"
    }
]