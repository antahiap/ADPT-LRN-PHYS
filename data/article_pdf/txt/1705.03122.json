[
    {
        "id": "",
        "section": "Abstract",
        "text": "The prevalent approach to sequence to sequence\nlearning maps an input sequence to a variable\nlength output sequence via recurrent neural net-\nworks. We introduce an architecture based en-\ntirely on convolutional neural networks.1Com-\npared to recurrent models, computations over all\nelements can be fully parallelized during training\nto better exploit the GPU hardware and optimiza-\ntion is easier since the number of non-linearities\nis \ufb01xed and independent of the input length. Our\nuse of gated linear units eases gradient propaga-\ntion and we equip each decoder layer with a sep-\narate attention module. We outperform the accu-\nracy of the deep LSTM setup of Wu et al. (2016)\non both WMT\u201914 English-German and WMT\u201914\nEnglish-French translation at an order of magni-\ntude faster speed, both on GPU and CPU.",
        "subsection": [
            {
                "id": "1.9",
                "section": "Bleu",
                "text": "the strong LSTM setup of Wu et al. (2016) by 0.5 BLEU\nand on WMT\u201914 English-French we outperform the like-\nlihood trained system of Wu et al. (2016) by 1.6 BLEU.\nFurthermore, our model can translate unseen sentences at\nan order of magnitude faster speed than Wu et al. (2016)\non GPU and CPU hardware ( \u00a74,\u00a75).",
                "subsection": []
            },
            {
                "id": "1.9",
                "section": "Bleu",
                "text": "prove over the LSTM model of Wu et al. (2016) by 1.6\nBLEU in a comparable setting, and on WMT\u201914 English-\nGerman translation we ouperform the same model by 0.5\nBLEU. In future work, we would like to apply convolu-\ntional architectures to other sequence to sequence learn-\ning problems which may bene\ufb01t from learning hierarchical\nrepresentations as well.\nAcknowledgements\nWe thank Benjamin Graham for providing a fast 1-D con-\nvolution, and Ronan Collobert as well as Yann LeCun for\nhelpful discussions related to this work.\nReferences\nBa, Jimmy Lei, Kiros, Jamie Ryan, and Hinton, Ge-\noffrey E. Layer normalization. arXiv preprint\narXiv:1607.06450 , 2016.\nBahdanau, Dzmitry, Cho, Kyunghyun, and Bengio,\nYoshua. Neural machine translation by jointly learning\nto align and translate. arXiv preprint arXiv:1409.0473 ,\n2014.\nBojar, Ondej, Chatterjee, Rajen, Federmann, Christian,\nConvolutional Sequence to Sequence Learning\nJimeno-Yepes, Antonio, Koehn, Philipp, Logacheva,\nVarvara, Monz, Christof, Negri, Matteo, N \u00b4ev\u00b4eol,\nAur\u00b4elie, Neves, Mariana L., Popel, Martin, Post, Matt,\nRubino, Rapha \u00a8el, Scarton, Carolina, Specia, Lucia,\nTurchi, Marco, Verspoor, Karin M., and Zampieri, Mar-\ncos. Findings of the 2016 conference on machine trans-\nlation. In Proc. of WMT , 2016.\nBradbury, James, Merity, Stephen, Xiong, Caiming, and\nSocher, Richard. Quasi-Recurrent Neural Networks.\narXiv preprint arXiv:1611.01576 , 2016.\nCho, Kyunghyun, Van Merri \u00a8enboer, Bart, Gulcehre,\nCaglar, Bahdanau, Dzmitry, Bougares, Fethi, Schwenk,\nHolger, and Bengio, Yoshua. Learning Phrase Represen-\ntations using RNN Encoder-Decoder for Statistical Ma-\nchine Translation. In Proc. of EMNLP , 2014.\nChorowski, Jan K, Bahdanau, Dzmitry, Serdyuk, Dmitriy,\nCho, Kyunghyun, and Bengio, Yoshua. Attention-based\nmodels for speech recognition. In Advances in Neural\nInformation Processing Systems , pp. 577\u2013585, 2015.\nCollobert, Ronan, Kavukcuoglu, Koray, and Farabet,\nClement. Torch7: A Matlab-like Environment for Ma-\nchine Learning. In BigLearn, NIPS Workshop , 2011.\nURL http://torch.ch .\nDauphin, Yann N., Fan, Angela, Auli, Michael, and Grang-\nier, David. Language modeling with gated linear units.\narXiv preprint arXiv:1612.08083 , 2016.\nDyer, Chris, Chahuneau, Victor, and Smith, Noah A. A\nSimple, Fast, and Effective Reparameterization of IBM\nModel 2. In Proc. of ACL , 2013.\nElman, Jeffrey L. Finding Structure in Time. Cognitive\nScience , 14:179\u2013211, 1990.\nGehring, Jonas, Auli, Michael, Grangier, David, and\nDauphin, Yann N. A Convolutional Encoder Model\nfor Neural Machine Translation. arXiv preprint\narXiv:1611.02344 , 2016.\nGlorot, Xavier and Bengio, Yoshua. Understanding the\ndif\ufb01culty of training deep feedforward neural networks.\nThe handbook of brain theory and neural networks ,\n2010.\nGraff, David, Kong, Junbo, Chen, Ke, and Maeda,\nKazuaki. English gigaword. Linguistic Data Consor-\ntium, Philadelphia , 2003.\nHa, David, Dai, Andrew, and Le, Quoc V . Hypernetworks.\narXiv preprint arXiv:1609.09106 , 2016.\nHe, Kaiming, Zhang, Xiangyu, Ren, Shaoqing, and Sun,\nJian. Deep Residual Learning for Image Recognition. In\nProc. of CVPR , 2015a.He, Kaiming, Zhang, Xiangyu, Ren, Shaoqing, and Sun,\nJian. Delving deep into recti\ufb01ers: Surpassing human-\nlevel performance on imagenet classi\ufb01cation. In Pro-\nceedings of the IEEE International Conference on Com-\nputer Vision , pp. 1026\u20131034, 2015b.\nHochreiter, Sepp and Schmidhuber, J \u00a8urgen. Long short-\nterm memory. Neural computation , 9(8):1735\u20131780,\n1997.\nIoffe, Sergey and Szegedy, Christian. Batch normalization:\nAccelerating deep network training by reducing internal\ncovariate shift. In Proceedings of The 32nd International\nConference on Machine Learning , pp. 448\u2013456, 2015.\nJean, S \u00b4ebastien, Firat, Orhan, Cho, Kyunghyun, Memi-\nsevic, Roland, and Bengio, Yoshua. Montreal Neural\nMachine Translation systems for WMT15. In Proc. of\nWMT , pp. 134\u2013140, 2015.\nKalchbrenner, Nal, Espeholt, Lasse, Simonyan, Karen,\nvan den Oord, Aaron, Graves, Alex, and Kavukcuoglu,\nKoray. Neural Machine Translation in Linear Time.\narXiv , 2016.\nLeCun, Yann and Bengio, Yoshua. Convolutional networks\nfor images, speech, and time series. The handbook of\nbrain theory and neural networks , 3361(10):1995, 1995.\nL\u2019Hostis, Gurvan, Grangier, David, and Auli, Michael. V o-\ncabulary Selection Strategies for Neural Machine Trans-\nlation. arXiv preprint arXiv:1610.00072 , 2016.\nLin, Chin-Yew. Rouge: A package for automatic evalu-\nation of summaries. In Text Summarization Branches\nOut: Proceedings of the ACL-04 Workshop , pp. 74\u201381,\n2004.\nLuong, Minh-Thang, Pham, Hieu, and Manning, Christo-\npher D. Effective approaches to attention-based neural\nmachine translation. In Proc. of EMNLP , 2015.\nMeng, Fandong, Lu, Zhengdong, Wang, Mingxuan, Li,\nHang, Jiang, Wenbin, and Liu, Qun. Encoding Source\nLanguage with Convolutional Neural Network for Ma-\nchine Translation. In Proc. of ACL , 2015.\nMi, Haitao, Wang, Zhiguo, and Ittycheriah, Abe. V ocab-\nulary Manipulation for Neural Machine Translation. In\nProc. of ACL , 2016.\nMiller, Alexander H., Fisch, Adam, Dodge, Jesse, Karimi,\nAmir-Hossein, Bordes, Antoine, and Weston, Jason.\nKey-value memory networks for directly reading docu-\nments. In Proc. of EMNLP , 2016.\nNallapati, Ramesh, Zhou, Bowen, Gulcehre, Caglar, Xi-\nang, Bing, et al. Abstractive text summarization us-\ning sequence-to-sequence rnns and beyond. In Proc. of\nEMNLPConvolutional Sequence to Sequence Learning\nOord, Aaron van den, Kalchbrenner, Nal, and\nKavukcuoglu, Koray. Pixel recurrent neural networks.\narXiv preprint arXiv:1601.06759 , 2016a.\nOord, Aaron van den, Kalchbrenner, Nal, Vinyals, Oriol,\nEspeholt, Lasse, Graves, Alex, and Kavukcuoglu, Koray.\nConditional image generation with pixelcnn decoders.\narXiv preprint arXiv:1606.05328 , 2016b.\nOver, Paul, Dang, Hoa, and Harman, Donna. Duc in con-\ntext. Information Processing & Management , 43(6):\n1506\u20131520, 2007.\nPascanu, Razvan, Mikolov, Tomas, and Bengio, Yoshua.\nOn the dif\ufb01culty of training recurrent neural networks.\nInProceedings of The 30th International Conference on\nMachine Learning , pp. 1310\u20131318, 2013.\nRush, Alexander M, Chopra, Sumit, and Weston, Jason. A\nneural attention model for abstractive sentence summa-\nrization. In Proc. of EMNLP , 2015.\nSalimans, Tim and Kingma, Diederik P. Weight nor-\nmalization: A simple reparameterization to acceler-\nate training of deep neural networks. arXiv preprint\narXiv:1602.07868 , 2016.\nSchuster, Mike and Nakajima, Kaisuke. Japanese and ko-\nrean voice search. In Acoustics, Speech and Signal Pro-\ncessing (ICASSP), 2012 IEEE International Conference\non, pp. 5149\u20135152. IEEE, 2012.\nSennrich, Rico, Haddow, Barry, and Birch, Alexandra.\nNeural Machine Translation of Rare Words with Sub-\nword Units. In Proc. of ACL , 2016a.\nSennrich, Rico, Haddow, Barry, and Birch, Alexandra. Ed-\ninburgh Neural Machine Translation Systems for WMT\n16. In Proc. of WMT , 2016b.\nShazeer, Noam, Mirhoseini, Azalia, Maziarz, Krzysztof,\nDavis, Andy, Le, Quoc, Hinton, Geoffrey, and Dean,\nJeff. Outrageously large neural networks: The sparsely-\ngated mixture-of-experts layer. ArXiv e-prints , January\n2016.\nShen, Shiqi, Zhao, Yu, Liu, Zhiyuan, Sun, Maosong,\net al. Neural headline generation with sentence-wise op-\ntimization. arXiv preprint arXiv:1604.01904 , 2016.\nSrivastava, Nitish, Hinton, Geoffrey E., Krizhevsky, Alex,\nSutskever, Ilya, and Salakhutdinov, Ruslan. Dropout: a\nsimple way to prevent Neural Networks from over\ufb01tting.\nJMLR , 15:1929\u20131958, 2014.\nSukhbaatar, Sainbayar, Weston, Jason, Fergus, Rob, and\nSzlam, Arthur. End-to-end Memory Networks. In Proc.\nof NIPS , pp. 2440\u20132448, 2015.Sutskever, Ilya, Martens, James, Dahl, George E., and Hin-\nton, Geoffrey E. On the importance of initialization and\nmomentum in deep learning. In ICML , 2013.\nSutskever, Ilya, Vinyals, Oriol, and Le, Quoc V . Sequence\nto Sequence Learning with Neural Networks. In Proc. of\nNIPS , pp. 3104\u20133112, 2014.\nSuzuki, Jun and Nagata, Masaaki. Cutting-off redundant\nrepeating generations for neural abstractive summariza-\ntion. arXiv preprint arXiv:1701.00138 , 2017.\nWaibel, Alex, Hanazawa, Toshiyuki, Hinton, Geoffrey,\nShikano, Kiyohiro, and Lang, Kevin J. Phoneme Recog-\nnition using Time-delay Neural Networks. IEEE trans-\nactions on acoustics, speech, and signal processing , 37\n(3):328\u2013339, 1989.\nWu, Yonghui, Schuster, Mike, Chen, Zhifeng, Le, Quoc V ,\nNorouzi, Mohammad, Macherey, Wolfgang, Krikun,\nMaxim, Cao, Yuan, Gao, Qin, Macherey, Klaus, et al.\nGoogle\u2019s Neural Machine Translation System: Bridging\nthe Gap between Human and Machine Translation. arXiv\npreprint arXiv:1609.08144 , 2016.\nYang, Zichao, Hu, Zhiting, Deng, Yuntian, Dyer, Chris,\nand Smola, Alex. Neural Machine Translation\nwith Recurrent Attention Modeling. arXiv preprint\narXiv:1607.05108 , 2016.\nZhou, Jie, Cao, Ying, Wang, Xuguang, Li, Peng, and Xu,\nWei. Deep Recurrent Models with Fast-Forward Con-\nnections for Neural Machine Translation. arXiv preprint\narXiv:1606.04199Convolutional Sequence to Sequence Learning\nA. Weight Initialization\nWe derive a weight initialization scheme tailored to the\nGLU activation function similar to Glorot & Bengio\n(2010); He et al. (2015b) by focusing on the variance of\nactivations within the network for both forward and back-\nward passes. We also detail how we modify the weight\ninitialization for dropout.\nA.1. Forward Pass\nAssuming that the inputs xlof a convolutional layer land\nits weightsWlare independent and identically distributed\n(i.i.d.), the variance of its output, computed as yl=Wlxl+\nbl, is\nVar/bracketleftbig\nyl/bracketrightbig\n=nlVar/bracketleftbig\nwlxl/bracketrightbig\n(3)\nwherenlis the number inputs to the layer. For one-\ndimensional convolutional layers with kernel width kand\ninput dimension c, this iskc. We adopt the notation in (He\net al., 2015b), i.e. yl,wlandxlrepresent the random vari-\nables in yl,Wlandxl. Withwlandxlindependent from\neach other and normally distributed with zero mean, this\namounts to\nVar/bracketleftbig\nyl/bracketrightbig\n=nlVar/bracketleftbig\nwl/bracketrightbig\nVar/bracketleftbig\nxl/bracketrightbig\n. (4)\nxlis the result of the GLU activation function\nya\nl\u22121\u03c3(yb\nl\u22121)with yl\u22121= (ya\nl\u22121,yb\nl\u22121), and ya\nl\u22121,yb\nl\u22121\ni.i.d. Next, we formulate upper and lower bounds in or-\nder to approximate Var[xl]. Ifyl\u22121follows a symmetric\ndistribution with mean 0, then\nVar/bracketleftbig\nxl/bracketrightbig\n=Var/bracketleftbig\nya\nl\u22121\u03c3(yb\nl\u22121)/bracketrightbig\n(5)\n=E/bracketleftbig/parenleftbig\nya\nl\u22121\u03c3(yb\nl\u22121)/parenrightbig2/bracketrightbig\n\u2212E2/bracketleftbig\nya\nl\u22121\u03c3(yb\nl\u22121)/bracketrightbig\n(6)\n=Var[ya\nl\u22121]E/bracketleftbig\n\u03c3(yb\nl\u22121)2/bracketrightbig\n. (7)\nA lower bound is given by (1/4)Var[ya\nl\u22121]when expand-\ning (6) with E2[\u03c3(yb\nl\u22121)] = 1/4:\nVar/bracketleftbig\nxl/bracketrightbig\n=Var/bracketleftbig\nya\nl\u22121\u03c3(yb\nl\u22121)/bracketrightbig\n(8)\n=Var/bracketleftbig\nya\nl\u22121/bracketrightbig\nE2/bracketleftbig\n\u03c3(yb\nl\u22121)/bracketrightbig\n+\nVar/bracketleftbig\nya\nl\u22121/bracketrightbig\nVar/bracketleftbig\n\u03c3(yb\nl\u22121)/bracketrightbig (9)\n=1\n4Var/bracketleftbig\nya\nl\u22121/bracketrightbig\n+Var/bracketleftbig\nya\nl\u22121/bracketrightbig\nVar/bracketleftbig\n\u03c3(yb\nl\u22121)/bracketrightbig\n(10)\nandVar[ya\nl\u22121]Var[\u03c3(yb\nl\u22121)]>0. We utilize the relation\n\u03c3(x)2\u2264(1/16)x2\u22121/4 +\u03c3(x)(Appendix B) to provide\nan upper bound on E[\u03c3(x)2]:\nE[\u03c3(x)2]\u2264E/bracketleftbig1\n16x2\u22121\n4+\u03c3(x)/bracketrightbig\n(11)\n=1\n16E[x2]\u22121\n4+E[\u03c3(x)] (12)Withx\u223cN(0,std(x)), this yields\nE/bracketleftbig\n\u03c3(x)2/bracketrightbig\n\u22641\n16E/bracketleftbig\nx2/bracketrightbig\n\u22121\n4+1\n2(13)\n=1\n16Var/bracketleftbig\nx/bracketrightbig\n+1\n4. (14)\nWith (7) and Var[ya\nl\u22121] =Var[yb\nl\u22121] =Var[yl\u22121], this\nresults in\nVar/bracketleftbig\nxl/bracketrightbig\n\u22641\n16Var/bracketleftbig\nyl\u22121/bracketrightbig2+1\n4Var/bracketleftbig\nyl\u22121/bracketrightbig\n. (15)\nWe initialize the embedding matrices in our network with\nsmall variances (around 0.01), which allows us to dismiss\nthe quadratic term and approximate the GLU output vari-\nance with\nVar[xl]\u22481\n4Var[yl\u22121]. (16)\nIfLnetwork layers of equal size and with GLU activations\nare combined, the variance of the \ufb01nal output yLis given\nby\nVar[yL]\u2248Var[y1]\uf8eb\n\uf8edL/productdisplay\nl=21\n4nlVar[wl]\uf8f6\n\uf8f8. (17)\nFollowing (He et al., 2015b), we aim to satisfy the condi-\ntion1\n4nlVar/bracketleftbig\nwl/bracketrightbig\n= 1,\u2200l (18)\nso that the activations in a network are neither exponen-\ntially magni\ufb01ed nor reduced. This is achieved by initializ-\ningWlfromN(0,/radicalbig\n4/nl).\nA.2. Backward Pass\nThe gradient of a convolutional layer is computed via back-\npropagation as \u2206xl=\u02c6Wlyl. Considering separate gradi-\nents\u2206ya\nland\u2206yb\nlfor GLU, the gradient of xis given by\n\u2206xl=\u02c6Wa\nl\u2206ya\nl+\u02c6Wb\nl\u2206yb\nl. (19)\n\u02c6Wcorresponds to Wwith re-arranged weights to enable\nback-propagation. Analogously to the forward pass, \u2206xl,\n\u02c6wland\u2206ylrepresent the random variables for the values\nin\u2206xl,\u02c6Wland\u2206yl, respectively. Note that Wand \u02c6W\ncontain the same values, i.e. \u02c6w=w. Similar to (3), the\nvariance of \u2206xlis\nVar[\u2206xl] = \u02c6nl/parenleftBig\nVar[wa\nl]Var[\u2206ya\nl] +Var[wb\nl]Var[\u2206yb\nl]/parenrightBig\n.\n(20)\nHere, \u02c6nlis the number of inputs to layer l+1. The gradients\nfor the GLU inputs are:\n\u2206ya\nl= \u2206xl+1\u03c3(yb\nl)and (21)\n\u2206yb\nl= \u2206xl+1ya\nl\u03c3/prime(yb\nl).Convolutional Sequence to Sequence Learning\nThe approximation for the forward pass can be used for\nVar[\u2206ya\nl], and for estimating Var[\u2206yb\nl]we assume an up-\nper bound on E[\u03c3/prime(yb\nl)2]of1/16since\u03c3/prime(yb\nl)\u2208[0,1\n4].\nHence,\nVar[\u2206ya\nl]\u22121\n4Var[\u2206xl+1]\u22641\n16Var[\u2206xl+1]Var[yb\nl)]\n(23)\nVar[\u2206yb\nl]\u22641\n16\u2206Var[\u2206xl+1]Var[ya\nl] (24)\nWe observe relatively small gradients in our network, typ-\nically around 0.001 at the start of training. Therefore, we\napproximate by discarding the quadratic terms above, i.e.\nVar[\u2206ya\nl]\u22481\n4Var[\u2206xl+1] (25)\nVar[\u2206yb\nl]\u22480 (26)\nVar[\u2206xl]\u22481\n4\u02c6nlVar[wa\nl]Var[\u2206xl+1] (27)\nAs for the forward pass, the above result can be general-\nized to backpropagation through many successive layers,\nresulting in\nVar[\u2206x2]\u2248Var[\u2206xL+1]\uf8eb\n\uf8edL/productdisplay\nl=21\n4\u02c6nlVar[wa\nl]\uf8f6\n\uf8f8 (28)\nand a similar condition, i.e. (1/4)\u02c6nlVar[wa\nl] = 1 . In the\nnetworks we consider, successions of convolutional layers\nusually operate on the same number of inputs so that most\ncasesnl= \u02c6nl. Note that Wb\nlis discarded in the approx-\nimation; however, for the sake of consistency we use the\nsame initialization for Wa\nlandWb\nl.\nFor arbitrarily large variances of network inputs and activa-\ntions, our approximations are invalid; in that case, the ini-\ntial values for Wa\nlandWb\nlwould have to be balanced for\nthe input distribution to be retained. Alternatively, meth-\nods that explicitly control the variance in the network, e.g.\nbatch normalization (Ioffe & Szegedy, 2015) or layer nor-\nmalization (Ba et al., 2016) could be employed.\nA.3. Dropout\nDropout retains activations in a neural network with a prob-\nabilitypand sets them to zero otherwise (Srivastava et al.,\n2014). It is common practice to scale the retained activa-\ntions by 1/pduring training so that the weights of the net-\nwork do not have to be modi\ufb01ed at test time when pis set to",
                "subsection": []
            }
        ]
    },
    {
        "id": "1.",
        "section": "Introduction",
        "text": "Sequence to sequence learning has been successful in\nmany tasks such as machine translation, speech recogni-\ntion (Sutskever et al., 2014; Chorowski et al., 2015) and\ntext summarization (Rush et al., 2015; Nallapati et al.,\n2016; Shen et al., 2016) amongst others. The dominant\napproach to date encodes the input sequence with a se-\nries of bi-directional recurrent neural networks (RNN) and\ngenerates a variable length output with another set of de-\ncoder RNNs, both of which interface via a soft-attention\nmechanism (Bahdanau et al., 2014; Luong et al., 2015).\nIn machine translation, this architecture has been demon-\nstrated to outperform traditional phrase-based models by\nlarge margins (Sennrich et al., 2016b; Zhou et al., 2016;\nWu et al., 2016;\u00a72).\n1The source code and models are available at https://\ngithub.com/facebookresearch/fairseq .Convolutional neural networks are less common for se-\nquence modeling, despite several advantages (Waibel et al.,\n1989; LeCun & Bengio, 1995). Compared to recurrent lay-\ners, convolutions create representations for \ufb01xed size con-\ntexts, however, the effective context size of the network can\neasily be made larger by stacking several layers on top of\neach other. This allows to precisely control the maximum\nlength of dependencies to be modeled. Convolutional net-\nworks do not depend on the computations of the previous\ntime step and therefore allow parallelization over every ele-\nment in a sequence. This contrasts with RNNs which main-\ntain a hidden state of the entire past that prevents parallel\ncomputation within a sequence.\nMulti-layer convolutional neural networks create hierarchi-\ncal representations over the input sequence in which nearby\ninput elements interact at lower layers while distant ele-\nments interact at higher layers. Hierarchical structure pro-\nvides a shorter path to capture long-range dependencies\ncompared to the chain structure modeled by recurrent net-\nworks, e.g. we can obtain a feature representation captur-\ning relationships within a window of nwords by applying\nonlyO(n\nk)convolutional operations for kernels of width\nk, compared to a linear number O(n)for recurrent neu-\nral networks. Inputs to a convolutional network are fed\nthrough a constant number of kernels and non-linearities,\nwhereas recurrent networks apply up to noperations and\nnon-linearities to the \ufb01rst word and only a single set of\noperations to the last word. Fixing the number of non-\nlinearities applied to the inputs also eases learning.\nRecent work has applied convolutional neural networks to\nsequence modeling such as Bradbury et al. (2016) who in-\ntroduce recurrent pooling between a succession of convo-\nlutional layers or Kalchbrenner et al. (2016) who tackle\nneural translation without attention. However, none of\nthese approaches has been demonstrated improvements\nover state of the art results on large benchmark datasets.\nGated convolutions have been previously explored for ma-\nchine translation by Meng et al. (2015) but their evaluation\nwas restricted to a small dataset and the model was used\nConvolutional Sequence to Sequence Learning\ntures which are partially convolutional have shown strong\nperformance on larger tasks but their decoder is still recur-\nrent (Gehring et al., 2016).\nIn this paper we propose an architecture for sequence to se-\nquence modeling that is entirely convolutional. Our model\nis equipped with gated linear units (Dauphin et al., 2016)\nand residual connections (He et al., 2015a). We also use\nattention in every decoder layer and demonstrate that each\nattention layer only adds a negligible amount of overhead.\nThe combination of these choices enables us to tackle large\nscale problems (\u00a73).\nWe evaluate our approach on several large datasets for ma-\nchine translation as well as summarization and compare to\nthe current best architectures reported in the literature. On\nWMT\u201916 English-Romanian translation we achieve a new\nstate of the art, outperforming the previous best result by",
        "subsection": []
    },
    {
        "id": "2.",
        "section": "Recurrent sequence to sequence learning",
        "text": "Sequence to sequence modeling has been synonymous\nwith recurrent neural network based encoder-decoder ar-\nchitectures (Sutskever et al., 2014; Bahdanau et al., 2014).\nThe encoder RNN processes an input sequence x=\n(x1,...,xm)ofmelements and returns state representa-\ntions z= (z1....,zm). The decoder RNN takes zand\ngenerates the output sequence y= (y1,...,yn)left to\nright, one element at a time. To generate output yi+1, the\ndecoder computes a new hidden state hi+1based on the\nprevious state hi, an embedding giof the previous target\nlanguage word yi, as well as a conditional input ciderived\nfrom the encoder output z. Based on this generic formula-\ntion, various encoder-decoder architectures have been pro-\nposed, which differ mainly in the conditional input and the\ntype of RNN.\nModels without attention consider only the \ufb01nal encoder\nstatezmby settingci=zmfor alli(Cho et al., 2014), or\nsimply initialize the \ufb01rst decoder state with zm(Sutskever\net al., 2014), in which case ciis not used. Architectures\nwith attention (Bahdanau et al., 2014; Luong et al., 2015)\ncomputecias a weighted sum of (z1....,zm)at each time\nstep. The weights of the sum are referred to as attention\nscores and allow the network to focus on different parts of\nthe input sequence as it generates the output sequences. At-\ntention scores are computed by essentially comparing each\nencoder state zjto a combination of the previous decoderstatehiand the last prediction yi; the result is normalized\nto be a distribution over input elements.\nPopular choices for recurrent networks in encoder-decoder\nmodels are long short term memory networks (LSTM;\nHochreiter & Schmidhuber, 1997) and gated recurrent units\n(GRU; Cho et al., 2014). Both extend Elman RNNs (El-\nman, 1990) with a gating mechanism that allows the mem-\norization of information from previous time steps in order\nto model long-term dependencies. Most recent approaches\nalso rely on bi-directional encoders to build representations\nof both past and future contexts (Bahdanau et al., 2014;\nZhou et al., 2016; Wu et al., 2016). Models with many lay-\ners often rely on shortcut or residual connections (He et al.,\n2015a; Zhou et al., 2016; Wu et al., 2016).",
        "subsection": [
            {
                "id": "3.1.",
                "section": "Position embeddings",
                "text": "First, we embed input elements x= (x1,...,xm)in dis-\ntributional space as w= (w1,...,wm), wherewj\u2208Rf\nis a column in an embedding matrix D\u2208RV\u00d7f. We also\nequip our model with a sense of order by embedding the ab-\nsolute position of input elements p= (p1,...,pm)where\npj\u2208Rf. Both are combined to obtain input element rep-\nresentations e= (w1+p1,...,wm+pm). We proceed\nsimilarly for output elements that were already generated\nby the decoder network to yield output element represen-\ntations that are being fed back into the decoder network\ng= (g1,...,gn). Position embeddings are useful in our\narchitecture since they give our model a sense of which\nportion of the sequence in the input or output it is currently\ndealing with (\u00a75.4).",
                "subsection": []
            },
            {
                "id": "3.2.",
                "section": "Convolutional block structure",
                "text": "Both encoder and decoder networks share a simple block\nstructure that computes intermediate states based on a \ufb01xed\nnumber of input elements. We denote the output of the l-\nth block as hl= (hl\n1,...,hl\nn)for the decoder network,\nandzl= (zl\n1,...,zl\nm)for the encoder network; we refer\nto blocks and layers interchangeably. Each block contains\na one dimensional convolution followed by a non-linearity.\nFor a decoder network with a single block and kernel width\nk, each resulting state h1\nicontains information over kinput\nelements. Stacking several blocks on top of each other in-\ncreases the number of input elements represented in a state.\nFor instance, stacking 6blocks with k= 5results in an in-\nput \ufb01eld of 25elements, i.e. each output depends onConvolutional Sequence to Sequence Learning\ninputs. Non-linearities allow the networks to exploit the\nfull input \ufb01eld, or to focus on fewer elements if needed.\nEach convolution kernel is parameterized as W\u2208R2d\u00d7kd,\nbw\u2208R2dand takes as input X\u2208Rk\u00d7dwhich is a\nconcatenation of kinput elements embedded in ddimen-\nsions and maps them to a single output element Y\u2208R2d\nthat has twice the dimensionality of the input elements;\nsubsequent layers operate over the koutput elements of\nthe previous layer. We choose gated linear units (GLU;\nDauphin et al., 2016) as non-linearity which implement a\nsimple gating mechanism over the output of the convolu-\ntionY= [AB]\u2208R2d:\nv([AB]) =A\u2297\u03c3(B)\nwhereA,B\u2208Rdare the inputs to the non-linearity, \u2297is\nthe point-wise multiplication and the output v([A B])\u2208\nRdis half the size of Y. The gates \u03c3(B)control which\ninputsAof the current context are relevant. A similar non-\nlinearity has been introduced in Oord et al. (2016b) who\napply tanh toAbut Dauphin et al. (2016) shows that GLUs\nperform better in the context of language modelling.\nTo enable deep convolutional networks, we add residual\nconnections from the input of each convolution to the out-\nput of the block (He et al., 2015a).\nhl\ni=v(Wl[hl\u22121\ni\u2212k/2,...,hl\u22121\ni+k/2] +bl\nw) +hl\u22121\ni\nFor encoder networks we ensure that the output of the con-\nvolutional layers matches the input length by padding the\ninput at each layer. However, for decoder networks we have\nto take care that no future information is available to the de-\ncoder (Oord et al., 2016a). Speci\ufb01cally, we pad the input\nbyk\u22121elements on both the left and right side by zero\nvectors, and then remove kelements from the end of the\nconvolution output.\nWe also add linear mappings to project between the embed-\nding sizefand the convolution outputs that are of size 2d.\nWe apply such a transform to wwhen feeding embeddings\nto the encoder network, to the encoder output zu\nj, to the \ufb01-\nnal layer of the decoder just before the softmax hL, and to\nall decoder layers hlbefore computing attention scores (1).\nFinally, we compute a distribution over the Tpossible next\ntarget elements yi+1by transforming the top decoder out-\nputhL\nivia a linear layer with weights Woand biasbo:\np(yi+1|y1,...,yi,x) = softmax( WohL\ni+bo)\u2208RT\n3.3. Multi-step Attention\nWe introduce a separate attention mechanism for each de-\ncoder layer. To compute the attention, we combine the cur-\nrent decoder state hl\niwith an embedding of the previous\nFigure 1. Illustration of batching during training. The English\nsource sentence is encoded (top) and we compute all attention\nvalues for the four German target words (center) simultaneously.\nOur attentions are just dot products between decoder context rep-\nresentations (bottom left) and encoder representations. We add\nthe conditional inputs computed by the attention (center right) to\nthe decoder states which then predict the target words (bottom\nright). The sigmoid and multiplicative boxes illustrate Gated Lin-\near Units.\ntarget element gi:\ndl\ni=Wl\ndhl\ni+bl\nd+gi (1)\nFor decoder layer lthe attention al\nijof stateiand source el-\nementjis computed as a dot-product between the decoder\nstate summary dl\niand each output zu\njof the last encoder\nblocku:\nal\nij=exp/parenleftbig\ndl\ni\u00b7zu\nj/parenrightbig\n/summationtextm\nt=1exp/parenleftbig\ndl\ni\u00b7zu\nt/parenrightbig\nThe conditional input cl\nito the current decoder layer is a\nweighted sum of the encoder outputs as well as the input\nelement embeddings ej(Figure 1, center right):\ncl\ni=m/summationdisplay\nj=1al\nij(zu\nj+ej) (2)\nThis is slightly different to recurrent approaches which\ncompute both the attention and the weighted sum over zu\nConvolutional Sequence to Sequence Learning\nonly. We found adding ejto be bene\ufb01cial and it resem-\nbles key-value memory networks where the keys are the zu\nj\nand the values are the zu\nj+ej(Miller et al., 2016). En-\ncoder outputs zu\njrepresent potentially large input contexts\nandejprovides point information about a speci\ufb01c input el-\nement that is useful when making a prediction. Once cl\ni\nhas been computed, it is simply added to the output of the\ncorresponding decoder layer hl\ni.\nThis can be seen as attention with multiple \u2019hops\u2019\n(Sukhbaatar et al., 2015) compared to single step attention\n(Bahdanau et al., 2014; Luong et al., 2015; Zhou et al.,\n2016; Wu et al., 2016). In particular, the attention of\nthe \ufb01rst layer determines a useful source context which\nis then fed to the second layer that takes this information\ninto account when computing attention etc. The decoder\nalso has immediate access to the attention history of the\nk\u22121previous time steps because the conditional inputs\ncl\u22121\ni\u2212k,...,cl\u22121\niare part ofhl\u22121\ni\u2212k,...,hl\u22121\niwhich are input\ntohl\ni. This makes it easier for the model to take into ac-\ncount which previous inputs have been attended to already\ncompared to recurrent nets where this information is in the\nrecurrent state and needs to survive several non-linearities.\nOverall, our attention mechanism considers which words\nwe previously attended to (Yang et al., 2016) andperforms\nmultiple attention \u2019hops\u2019 per time step. In Appendix \u00a7C,\nwe plot attention scores for a deep decoder and show that\nat different layers, different portions of the source are at-\ntended to.\nOur convolutional architecture also allows to batch the at-\ntention computation across all elements of a sequence com-\npared to RNNs (Figure 1, middle). We batch the computa-\ntions of each decoder layer individually.",
                "subsection": []
            },
            {
                "id": "3.4.",
                "section": "Normalization strategy",
                "text": "We stabilize learning through careful weight initialization\n(\u00a73.5) and by scaling parts of the network to ensure that the\nvariance throughout the network does not change dramati-\ncally. In particular, we scale the output of residual blocks\nas well as the attention to preserve the variance of activa-\ntions. We multiply the sum of the input and output of a\nresidual block by\u221a\n0.5to halve the variance of the sum.\nThis assumes that both summands have the same variance\nwhich is not always true but effective in practice.\nThe conditional input cl\nigenerated by the attention is a\nweighted sum of mvectors (2) and we counteract a change\nin variance through scaling by m/radicalbig\n1/m; we multiply by\nmto scale up the inputs to their original size, assuming the\nattention scores are uniformly distributed. This is generally\nnot the case but we found it to work well in practice.\nFor convolutional decoders with multiple attention, we\nscale the gradients for the encoder layers by the numberof attention mechanisms we use; we exclude source word\nembeddings. We found this to stabilize learning since the\nencoder received too much gradient otherwise.",
                "subsection": []
            },
            {
                "id": "3.5.",
                "section": "Initialization",
                "text": "Normalizing activations when adding the output of dif-\nferent layers, e.g. residual connections, requires careful\nweight initialization. The motivation for our initialization\nis the same as for the normalization: maintain the variance\nof activations throughout the forward and backward passes.\nAll embeddings are initialized from a normal distribution\nwith mean 0and standard deviation 0.1. For layers whose\noutput is not directly fed to a gated linear unit, we initial-\nize weights fromN(0,/radicalbig\n1/nl)wherenlis the number of\ninput connections to each neuron. This ensures that the\nvariance of a normally distributed input is retained.\nFor layers which are followed by a GLU activation, we pro-\npose a weight initialization scheme by adapting the deriva-\ntions in (He et al., 2015b; Glorot & Bengio, 2010; Ap-\npendix A). If the GLU inputs are distributed with mean 0\nand have suf\ufb01ciently small variance, then we can approx-\nimate the output variance with 1/4of the input variance\n(Appendix A.1). Hence, we initialize the weights so that\nthe input to the GLU activations have 4times the variance\nof the layer input. This is achieved by drawing their initial\nvalues fromN(0,/radicalbig\n4/nl). Biases are uniformly set to zero\nwhen the network is constructed.\nWe apply dropout to the input of some layers so that in-\nputs are retained with a probability of p. This can be seen\nas multiplication with a Bernoulli random variable taking\nvalue 1/pwith probability pand0otherwise (Srivastava\net al., 2014). The application of dropout will then cause\nthe variance to be scaled by 1/p. We aim to restore the\nincoming variance by initializing the respective layers with\nlarger weights. Speci\ufb01cally, we use N(0,/radicalbig\n4p/nl)for lay-\ners whose output is subject to a GLU and N(0,/radicalbig\np/nl)\notherwise (Appendix A.3).",
                "subsection": []
            }
        ]
    },
    {
        "id": "3.",
        "section": "Aconvolutional architecture",
        "text": "Next we introduce a fully convolutional architecture for se-\nquence to sequence modeling. Instead of relying on RNNs\nto compute intermediate encoder states zand decoder states\nhwe use convolutional neural networks (CNN).",
        "subsection": [
            {
                "id": "4.1.",
                "section": "Datasets",
                "text": "We consider three major WMT translation tasks as well as\na text summarization task.\nWMT\u201916 English-Romanian. We use the same data and\npre-processing as Sennrich et al. (2016b) but remove sen-\ntences with more than 175words. This results in 2.8M sen-\ntence pairs for training and we evaluate on newstest2016.2\n2We followed the pre-processing of https://github.\ncom/rsennrich/wmt16-scripts/blob/80e21e5/\nsample/preprocess.sh and added the back-translated data\nfromConvolutional Sequence to Sequence Learning\nWe experiment with word-based models using a source vo-\ncabulary of 200K types and a target vocabulary of 80K\ntypes. We also consider a joint source and target byte-pair\nencoding (BPE) with 40K types (Sennrich et al., 2016a;b).\nWMT\u201914 English-German. We use the same setup as Lu-\nong et al. (2015) which comprises 4.5M sentence pairs for\ntraining and we test on newstest2014.3As vocabulary we\nuse 40K sub-word types based on BPE.\nWMT\u201914 English-French. We use the full training set of\n36M sentence pairs, and remove sentences longer than 175\nwords as well as pairs with a source/target length ratio ex-\nceeding 1.5. This results in 35.5M sentence-pairs for train-\ning. Results are reported on newstest2014 . We use a source\nand target vocabulary with 40K BPE types.\nIn all setups a small subset of the training data serves as val-\nidation set (about 0.5-1% for each dataset) for early stop-\nping and learning rate annealing.\nAbstractive summarization. We train on the Gigaword\ncorpus (Graff et al., 2003) and pre-process it identically\nto Rush et al. (2015) resulting in 3.8M training examples\nand 190K for validation. We evaluate on the DUC-2004\ntest data comprising 500 article-title pairs (Over et al.,\n2007) and report three variants of recall-based ROUGE\n(Lin, 2004), namely, ROUGE-1 (unigrams), ROUGE-2 (bi-\ngrams), and ROUGE-L (longest-common substring). We\nalso evaluate on a Gigaword test set of 2000 pairs which\nis identical to the one used by Rush et al. (2015) and we\nreport F1 ROUGE similar to prior work. Similar to Shen\net al. (2016) we use a source and target vocabulary of 30K\nwords and require outputs to be at least 14 words long.",
                "subsection": []
            },
            {
                "id": "4.2.",
                "section": "Model parameters and optimization",
                "text": "We use 512hidden units for both encoders and decoders,\nunless otherwise stated. All embeddings, including the out-\nput produced by the decoder before the \ufb01nal linear layer,\nhave dimensionality 512; we use the same dimensionalities\nfor linear layers mapping between the hidden and embed-\nding sizes (\u00a73.2).\nWe train our convolutional models with Nesterov\u2019s accel-\nerated gradient method (Sutskever et al., 2013) using a mo-\nmentum value of 0.99and renormalize gradients if their\nnorm exceeds 0.1(Pascanu et al., 2013). We use a learn-\ning rate of 0.25and once the validation perplexity stops\nimproving, we reduce the learning rate by an order of mag-\nnitude after each epoch until it falls below 10\u22124.\nUnless otherwise stated, we use mini-batches of 64sen-\ntences. We restrict the maximum number of words in a\nmini-batch to make sure that batches with long sentences\nbacktranslations/en-ro .\n3http://nlp.stanford.edu/projects/nmtstill \ufb01t in GPU memory. If the threshold is exceeded, we\nsimply split the batch until the threshold is met and pro-\ncess the parts separatedly. Gradients are normalized by the\nnumber of non-padding tokens per mini-batch. We also use\nweight normalization for all layers except for lookup tables\n(Salimans & Kingma, 2016).\nBesides dropout on the embeddings and the decoder out-\nput, we also apply dropout to the input of the convolu-\ntional blocks (Srivastava et al., 2014). All models are im-\nplemented in Torch (Collobert et al., 2011) and trained on\na single Nvidia M40 GPU except for WMT\u201914 English-\nFrench for which we use a multi-GPU setup on a single\nmachine. We train on up to eight GPUs synchronously by\nmaintaining copies of the model on each card and split the\nbatch so that each worker computes 1/8-th of the gradients;\nat the end we sum the gradients via Nvidia NCCL.",
                "subsection": []
            },
            {
                "id": "4.3.",
                "section": "Evaluation",
                "text": "We report average results over three runs of each model,\nwhere each differs only in the initial random seed. Trans-\nlations are generated by a beam search and we normalize\nlog-likelihood scores by sentence length. We use a beam\nof width 5. We divide the log-likelihoods of the \ufb01nal hy-\npothesis in beam search by their length |y|. For WMT\u201914\nEnglish-German we tune a length normalization constant\non a separate development set ( newstest2015 ) and we nor-\nmalize log-likelihoods by |y|\u03b1(Wu et al., 2016). On other\ndatasets we did not \ufb01nd any bene\ufb01t with length normaliza-\ntion.\nFor word-based models, we perform unknown word re-\nplacement based on attention scores after generation (Jean\net al., 2015). Unknown words are replaced by looking up\nthe source word with the maximum attention score in a pre-\ncomputed dictionary. If the dictionary contains no trans-\nlation, then we simply copy the source word. Dictionar-\nies were extracted from the word aligned training data that\nwe obtained with fast align (Dyer et al., 2013). Each\nsource word is mapped to the target word it is most fre-\nquently aligned to. In our multi-step attention ( \u00a73.3) we\nsimply average the attention scores over all layers. Fi-\nnally, we compute case-sensitive tokenized BLEU, except\nfor WMT\u201916 English-Romanian where we use detokenized\nBLEU to be comparable with Sennrich et al. (2016b).4\n4https://github.com/moses-smt/\nmosesdecoder/blob/617e8c8/scripts/generic/\n{multi-bleu.perl ,Convolutional Sequence to Sequence Learning\n5. Results",
                "subsection": []
            }
        ]
    },
    {
        "id": "4.",
        "section": "Experimental setup",
        "text": "",
        "subsection": [
            {
                "id": "5.1.",
                "section": "Recurrent vs",
                "text": "We \ufb01rst evaluate our convolutional model on three transla-\ntion tasks. On WMT\u201916 English-Romanian translation we\ncompare to Sennrich et al. (2016b) which is the winning\nentry on this language pair at WMT\u201916 (Bojar et al., 2016).\nTheir model implements the attention-based sequence to\nsequence architecture of Bahdanau et al. (2014) and uses\nGRU cells both in the encoder and decoder. We test both\nword-based and BPE vocabularies ( \u00a74).\nTable 1 shows that our fully convolutional sequence to se-\nquence model (ConvS2S) outperforms the WMT\u201916 win-\nning entry for English-Romanian by 1.9 BLEU with a BPE\nencoding and by 1.3 BLEU with a word factored vocabu-\nlary. This instance of our architecture has 20 layes in the\nencoder and 20 layers in the decoder, both using kernels\nof width 3 and hidden size 512 throughout. Training took\nbetween 6 and 7.5 days on a single GPU.\nOn WMT\u201914 English to German translation we compare to\nthe following prior work: Luong et al. (2015) is based on a\nfour layer LSTM attention model, ByteNet (Kalchbrenner\net al., 2016) propose a convolutional model based on char-\nacters without attention, with 30 layers in the encoder and\n30 layers in the decoder, GNMT (Wu et al., 2016) repre-\nsents the state of the art on this dataset and they use eight\nencoder LSTMs as well as eight decoder LSTMs, we quote\ntheir result for a word-based model, such as ours, as well\nas a word-piece model (Schuster & Nakajima, 2012).5\nThe results (Table 1) show that our convolutional model\noutpeforms GNMT by 0.5 BLEU. Our encoder has 15 lay-\ners and the decoder has 15 layers, both with 512 hidden\nunits in the \ufb01rst ten layers and 768 units in the subsequent\nthree layers, all using kernel width 3. The \ufb01nal two layers\nhave 2048 units which are just linear mappings with a sin-\ngle input. We trained this model on a single GPU over a\nperiod of 18.5 days with a batch size of 48. LSTM sparse\nmixtures have shown strong accuracy at 26.03 BLEU for a\nsingle run (Shazeer et al., 2016) which compares to 25.39\nBLEU for our best run. This mixture sums the output of\nfour experts, not unlike an ensemble which sums the output\nof multiple networks. ConvS2S also bene\ufb01ts from ensem-\nbling (\u00a75.2), therefore mixtures are a promising direction.\nFinally, we train on the much larger WMT\u201914 English-\nFrench task where we compare to the state of the art re-\nsult of GNMT (Wu et al., 2016). Our model is trained with\na simple token-level likelihood objective and we improve\nover GNMT in the same setting by 1.6 BLEU on average.\nWe also outperform their reinforcement (RL) models by 0.5\n5We did not use the exact same vocabulary size because word\npieces and BPE estimate the vocabulary differently.WMT\u201916 English-Romanian BLEU\nSennrich et al. (2016b) GRU (BPE 90K) 28.1\nConvS2S (Word 80K) 29.45\nConvS2S (BPE 40K) 30.02\nWMT\u201914 English-German BLEU\nLuong et al. (2015) LSTM (Word 50K) 20.9\nKalchbrenner et al. (2016) ByteNet (Char) 23.75\nWu et al. (2016) GNMT (Word 80K) 23.12\nWu et al. (2016) GNMT (Word pieces) 24.61\nConvS2S (BPE 40K) 25.16\nWMT\u201914 English-French BLEU\nWu et al. (2016) GNMT (Word 80K) 37.90\nWu et al. (2016) GNMT (Word pieces) 38.95\nWu et al. (2016) GNMT (Word pieces) + RL 39.92\nConvS2S (BPE 40K) 40.51\nTable 1. Accuracy on WMT tasks comapred to previous work.\nConvS2S and GNMT results are averaged over several runs.\nBLEU. Reinforcement learning is equally applicable to our\narchitecture and we believe that it would further improve\nour results.\nThe ConvS2S model for this experiment uses 15 layers in\nthe encoder and 15 layers in the decoder, both with 512\nhidden units in the \ufb01rst \ufb01ve layers, 768 units in the subse-\nquent four layers, 1024 units in the next 3 layers, all using\nkernel width 3; the \ufb01nal two layers have 2048 units and\n4096 units each but the they are linear mappings with ker-\nnel width 1. This model has an effective context size of\nonly 25 words, beyond which it cannot access any infor-\nmation on the target size. Our results are based on training\nwith 8 GPUs for about 37 days and batch size 32 on each\nworker.6The same con\ufb01guration as for WMT\u201914 English-\nGerman achieves 39.41 BLEU in two weeks on this dataset\nin an eight GPU setup.\nZhou et al. (2016) report a non-averaged result of 39.2\nBLEU. More recently, Ha et al. (2016) showed that one\ncan generate weights with one LSTM for another LSTM.\nThis approach achieves 40.03 BLEU but the result is not\naveraged. Shazeer et al. (2016) compares at 40.56 BLEU\nto our best single run of 40.70 BLEU.\n6This is half of the GPU time consumed by a basic model of\nWu et al. (2016) who use 96 GPUs for 6 days. We expect the time\nto train our model to decrease substantially in a multi-machine\nConvolutional Sequence to Sequence Learning\nWMT\u201914 English-German BLEU\nWu et al. (2016) GNMT 26.20\nWu et al. (2016) GNMT + RL 26.30\nConvS2S 26.43\nWMT\u201914 English-French BLEU\nZhou et al. (2016) 40.4\nWu et al. (2016) GNMT 40.35\nWu et al. (2016) GNMT + RL 41.16\nConvS2S 41.44\nConvS2S (10 models) 41.62\nTable 2. Accuracy of ensembles with eight models. We show\nboth likelihood and Reinforce (RL) results for GNMT; Zhou et al.\n(2016) and ConvS2S use simple likelihood training.\nThe translations produced by our models often match the\nlength of the references, particularly for the large WMT\u201914\nEnglish-French task, or are very close for small to medium\ndata sets such as WMT\u201914 English-German or WMT\u201916\nEnglish-Romanian.",
                "subsection": []
            },
            {
                "id": "5.2.",
                "section": "Ensemble results",
                "text": "Next, we ensemble eight likelihood-trained models for both\nWMT\u201914 English-German and WMT\u201914 English-French\nand compare to previous work which also reported ensem-\nble results. For the former, we also show the result when\nensembling 10 models. Table 2 shows that we outperform\nthe best current ensembles on both datasets.",
                "subsection": []
            },
            {
                "id": "5.3.",
                "section": "Generation speed",
                "text": "Next, we evaluate the inference speed of our architecture\non the development set of the WMT\u201914 English-French\ntask which is the concatenation of newstest2012 and new-\nstest2013; it comprises 6003 sentences. We measure gener-\nation speed both on GPU and CPU hardware. Speci\ufb01cally,\nwe measure GPU speed on three generations of Nvidia\ncards: a GTX-1080ti, an M40 as well as an older K40\ncard. CPU timings are measured on one host with 48 hyper-\nthreaded cores (Intel Xeon E5-2680 @ 2.50GHz) with 40\nworkers. In all settings, we batch up to 128 sentences, com-\nposing batches with sentences of equal length. Note that\nthe majority of batches is smaller because of the small size\nof the development set. We experiment with beams of size\n5 as well as greedy search, i.e beam of size 1. To make gen-\neration fast, we do not recompute convolution states that\nhave not changed compared to the previous time step but\nrather copy (shift) these activations.\nWe compare to results reported in Wu et al. (2016) whoBLEU Time (s)\nGNMT GPU (K80) 31.20 3,028\nGNMT CPU 88 cores 31.20 1,322\nGNMT TPU 31.21 384\nConvS2S GPU (K40) b= 1 33.45 327\nConvS2S GPU (M40) b= 1 33.45 221\nConvS2S GPU (GTX-1080ti) b= 1 33.45 142\nConvS2S CPU 48 cores b= 1 33.45 142\nConvS2S GPU (K40) b= 5 34.10 587\nConvS2S CPU 48 cores b= 5 34.10 482\nConvS2S GPU (M40) b= 5 34.10 406\nConvS2S GPU (GTX-1080ti) b= 5 34.10 256\nTable 3. CPU and GPU generation speed in seconds on the de-\nvelopment set of WMT\u201914 English-French. We show results for\ndifferent beam sizes b. GNMT \ufb01gures are taken from Wu et al.\n(2016). CPU speeds are not directly comparable because Wu et al.\n(2016) use a 88 core machine versus our 48 core setup.\nuse Nvidia K80 GPUs which are essentially two K40s. We\ndid not have such a GPU available and therefore run ex-\nperiments on an older K40 card which is inferior to a K80,\nin addition to the newer M40 and GTX-1080ti cards. The\nresults (Table 3) show that our model can generate transla-\ntions on a K40 GPU at 9.3 times the speed and 2.25 higher\nBLEU; on an M40 the speed-up is up to 13.7 times and on\na GTX-1080ti card the speed is 21.3 times faster. A larger\nbeam of size 5 decreases speed but gives better BLEU.\nOn CPU, our model is up to 9.3 times faster, however, the\nGNMT CPU results were obtained with an 88 core machine\nwhereas our results were obtained with just over half the\nnumber of cores. On a per CPU core basis, our model is\n17 times faster at a better BLEU. Finally, our CPU speed is\n2.7 times higher than GNMT on a custom TPU chip which\nshows that high speed can be achieved on commodity hard-\nware. We do no report TPU \ufb01gures as we do not have ac-\ncess to this hardware.",
                "subsection": []
            },
            {
                "id": "5.4.",
                "section": "Position embeddings",
                "text": "In the following sections, we analyze the design choices in\nour architecture. The remaining results in this paper are\nbased on the WMT\u201914 English-German task with 13 en-\ncoder layers at kernel size 3 and 5 decoder layers at kernel\nsize 5. We use a target vocabulary of 160K words as well as\nvocabulary selection (Mi et al., 2016; L\u2019Hostis et al., 2016)\nto decrease the size of the output layer which speeds up\ntraining and testing. The average vocabulary size for each\ntraining batch is about 20K target words. All \ufb01gures are av-\neraged over three runs ( \u00a74) and BLEU is reported on new-\nstest2014 before unknown word replacement.\nConvolutional Sequence to Sequence Learning\nPPL BLEU\nConvS2S 6.64 21.7\n-source position 6.69 21.3\n-target position 6.63 21.5\n-source & target position 6.68 21.2\nTable 4. Effect of removing position embeddings from our model\nin terms of validation perplexity (valid PPL) and BLEU.\nbeddings from the encoder and decoder ( \u00a73.1). These em-\nbeddings allow our model to identify which portion of the\nsource and target sequence it is dealing with but also im-\npose a restriction on the maximum sentence length. Ta-\nble 4 shows that position embeddings are helpful but that\nour model still performs well without them. Removing\nthe source position embeddings results in a larger accuracy\ndecrease than target position embeddings. However, re-\nmoving both source and target positions decreases accuracy\nonly by 0.5BLEU. We had assumed that the model would\nnot be able to calibrate the length of the output sequences\nvery well without explicit position information, however,\nthe output lengths of models without position embeddings\nclosely matches models with position information. This in-\ndicates that the models can learn relative position informa-\ntion within the contexts visible to the encoder and decoder\nnetworks which can observe up to 27 and 25 words respec-\ntively.\nRecurrent models typically do not use explicit position em-\nbeddings since they can learn where they are in the se-\nquence through the recurrent hidden state computation. In\nour setting, the use of position embeddings requires only a\nsimple addition to the input word embeddings which is a\nnegligible overhead.",
                "subsection": []
            },
            {
                "id": "5.5.",
                "section": "Multi-step attention",
                "text": "The multiple attention mechanism ( \u00a73.3) computes a sep-\narate source context vector for each decoder layer. The\ncomputation also takes into account contexts computed for\npreceding decoder layers of the current time step as well\nas previous time steps that are within the receptive \ufb01eld of\nthe decoder. How does multiple attention compare to at-\ntention in fewer layers or even only in a single layer as is\nusual? Table 5 shows that attention in all decoder layers\nachieves the best validation perplexity (PPL). Furthermore,\nremoving more and more attention layers decreases accu-\nracy, both in terms of BLEU as well as PPL.\nThe computational overhead for attention is very small\ncompared to the rest of the network. Training with atten-\ntion in all \ufb01ve decoder layers processes 3624 target words\nper second on average on a single GPU, compared to 3772\nwords per second for attention in a single layer. This is onlyAttn Layers PPL BLEU\n1,2,3,4,5 6.65 21.63\n1,2,3,4 6.70 21.54\n1,2,3 6.95 21.36\n1,2 6.92 21.47\n1,3,5 6.97 21.10\n1 7.15 21.26\n2 7.09 21.30\n3 7.11 21.19\n4 7.19 21.31\n5 7.66 20.24\nTable 5. Multi-step attention in all \ufb01ve decoder layers or fewer\nlayers in terms of validation perplexity (PPL) and test BLEU.\n 19 19.5 20 20.5 21 21.5 22Encoder\nDecoder 19 19.5 20 20.5 21 21.5 22\n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25BLEU\nLayersEncoder\nDecoder\nFigure 2. Encoder and decoder with different number of layers.\na 4% slow down when adding 4 attention modules. Most\nneural machine translation systems only use a single mod-\nule. This demonstrates that attention is not the bottleneck\nin neural machine translation, even though it is quadratic in\nthe sequence length (cf. Kalchbrenner et al., 2016). Part of\nthe reason for the low impact on speed is that we batch the\ncomputation of an attention module over all target words,\nsimilar to Kalchbrenner et al. (2016). However, for RNNs\nbatching of the attention may be less effective because of\nthe dependence on the previous time step.",
                "subsection": []
            },
            {
                "id": "5.6.",
                "section": "Kernel size and depth",
                "text": "Figure 2 shows accuracy when we change the number of\nlayers in the encoder or decoder. The kernel width for lay-\ners in the encoder is 3and for the decoder it is 5. Deeper\narchitectures are particularly bene\ufb01cial for the encoder but\nless so for the decoder. Decoder setups with two layers al-\nready perform well whereas for the encoder accuracy keeps\nincreasing steadily with more layers until up to 9 layers\nConvolutional Sequence to Sequence Learning\nDUC-2004 Gigaword\nRG-1 (R) RG-2 (R) RG-L (R) RG-1 (F) RG-2 (F) RG-L (F)\nRNN MLE (Shen et al., 2016) 24.92 8.60 22.25 32.67 15.23 30.56\nRNN MRT (Shen et al., 2016) 30.41 10.87 26.79 36.54 16.59 33.44\nWFE (Suzuki & Nagata, 2017) 32.28 10.54 27.80 36.30 17.31 33.88\nConvS2S 30.44 10.84 26.90 35.88 17.48 33.29\nTable 6. Accuracy on two summarization tasks in terms of Rouge-1 (RG-1), Rouge-2 (RG-2), and Rouge-L (RG-L).\nKernel width Encoder layers\n5 9 13\n3 20.61 21.17 21.63\n5 20.80 21.02 21.42\n7 20.81 21.30 21.09\nTable 7. Encoder with different kernel width in terms of BLEU.\nKernel width Decoder layers\n3 5 7\n3 21.10 21.71 21.62\n5 21.09 21.63 21.24\n7 21.40 21.31 21.33\nTable 8. Decoder with different kernel width in terms of BLEU.\nAside from increasing the depth of the networks, we can\nalso change the kernel width. Table 7 shows that encoders\nwith narrow kernels and many layers perform better than\nwider kernels. These networks can also be faster since the\namount of work to compute a kernel operating over 3 input\nelements is less than half compared to kernels over 7 ele-\nments. We see a similar picture for decoder networks with\nlarge kernel sizes (Table 8). Dauphin et al. (2016) shows\nthat context sizes of 20 words are often suf\ufb01cient to achieve\nvery good accuracy on language modeling for English.",
                "subsection": []
            },
            {
                "id": "5.7.",
                "section": "Summarization",
                "text": "Finally, we evaluate our model on abstractive sentence\nsummarization which takes a long sentence as input and\noutputs a shortened version. The current best models on\nthis task are recurrent neural networks which either opti-\nmize the evaluation metric (Shen et al., 2016) or address\nspeci\ufb01c problems of summarization such as avoiding re-\npeated generations (Suzuki & Nagata, 2017). We use stan-\ndard likelhood training for our model and a simple model\nwith six layers in the encoder and decoder each, hidden\nsize256, batch size 128, and we trained on a single GPU in\none night. Table 6 shows that our likelhood trained model\noutperforms the likelihood trained model (RNN MLE) of\nShen et al. (2016) and is not far behind the best models on\nthis task which bene\ufb01t from task-speci\ufb01c optimization andmodel structure. We expect our model to bene\ufb01t from these\nimprovements as well.",
                "subsection": []
            }
        ]
    },
    {
        "id": "6.",
        "section": "Conclusion and future work",
        "text": "We introduce the \ufb01rst fully convolutional model for se-\nquence to sequence learning that outperforms strong re-\ncurrent models on very large benchmark datasets at an or-\nder of magnitude faster speed. Compared to recurrent net-\nworks, our convolutional approach allows to discover com-\npositional structure in the sequences more easily since rep-\nresentations are built hierarchically. Our model relies on\ngating and performs multiple attention steps.\nWe achieve a new state of the art on several public trans-\nlation benchmark data sets. On the WMT\u201916 English-\nRomanian task we outperform the previous best result by",
        "subsection": []
    },
    {
        "id": "1.",
        "section": "In this case",
        "text": "xby a Bernoulli random variable rwhere Pr[r= 1/p] =p\nandPr[r= 0] = 1\u2212p(Srivastava et al., 2014). It holds\nthatE[r] = 1 andVar[r] = (1\u2212p)/p. Ifxis independentofrandE[x] = 0 , the variance after dropout is\nVar[xr] =E[r]2Var[x] +Var[r]Var[x] (29)\n=/parenleftbigg\n1 +1\u2212p\np/parenrightbigg\nVar[x] (30)\n=1\npVar[x] (31)\nAssuming that a the input of a convolutional layer has been\nsubject to dropout with a retain probability p, the varia-\ntions of the forward and backward activations from \u00a7A.1\nand\u00a7A.2 can now be approximated with\nVar[xl+1]\u22481\n4pnlVar[wl]Var[xl]and (32)\nVar[\u2206xl]\u22481\n4pnlVar[wa\nl]Var[\u2206xl+1]. (33)\nThis amounts to a modi\ufb01ed initialization of Wlfrom a nor-\nmal distribution with zero mean and a standard deviation of/radicalbig\n4p/n. For layers without a succeeding GLU activation\nfunction, we initialize weights from N(0,/radicalbig\np/n)to cali-\nbrate for any immediately preceding dropout application.\nB. Upper Bound on Squared Sigmoid\nThe sigmoid function \u03c3(x)can be expressed as a hyper-\nbolic tangent by using the identity tanh(x) = 2\u03c3(2x)\u22121.\nThe derivative of tanh istanh/prime(x) = 1\u2212tanh2(x), and\nwithtanh(x)\u2208[0,1],x\u22650it holds that\ntanh/prime(x)\u22641,x\u22650 (34)\n/integraldisplayx\n0tanh/prime(x) dx\u2264/integraldisplayx\n01 dx (35)\ntanh(x)\u2264x,x\u22650 (36)\nWe can express this relation with \u03c3(x)as follows:\n2\u03c3(x)\u22121\u22641\n2x,x\u22650 (37)\nBoth terms of this inequality have rotational symmetry w.r.t\n0, and thus\n/parenleftbig\n2\u03c3(x)\u22121/parenrightbig2\u2264/parenleftbigg1\n2x/parenrightbigg2\n\u2200x (38)\n\u21d4\u03c3(x)2\u22641\n16x2\u22121\n4+\u03c3(x). (39)\nC. Attention Visualization\nFigure 3 shows attention scores for a generated sentence\nfrom the WMT\u201914 English-German task. The model used\nfor this plot has 8 decoder layers and a 80K BPE vocabu-\nlary. The attention passes in different decoder layers cap-\nConvolutional Sequence to Sequence Learning\nand 6 exhibit a linear alignment. The \ufb01rst layer shows the\nclearest alignment, although it is slightly off and frequently\nattends to the corresponding source word of the previously\ngenerated target word. Layer 2 and 8 lack a clear struc-\nture and are presumably collecting information about the\nwhole source sentence. The fourth layer shows high align-\nment scores on nouns such as \u201cfestival\u201d, \u201cway\u201d and \u201cwork\u201d\nfor both the generated target nouns as well as their preced-\ning words. Note that in German, those preceding words\ndepend on gender and object relationship of the respec-\ntive noun. Finally, the attention scores in layer 5 and 7\nfocus on \u201cbuilt\u201d, which is reordered in the German trans-\nlation and is moved from the beginning to the very end of\nthe sentence. One interpretation for this is that as genera-\ntion progresses, the model repeatedly tries to perform the\nre-ordering. \u201caufgebaut\u201d can be generated after a noun or\npronoun only, which is re\ufb02ected in the higher scores at po-\nConvolutional Sequence to Sequence Learning\nLayer 1\n Layer 2\n Layer 3\nLayer 4\n Layer 5\n Layer 6\nLayer 7\n Layer 8\nFigure 3. Attention scores for different decoder layers for a sentence translated from English (y-axis) to German (x-axis). This model",
        "subsection": []
    },
    {
        "missing": []
    },
    {
        "references": []
    },
    {
        "title": "Convolutional Sequence to Sequence Learning",
        "arxiv_id": "1705.03122"
    }
]