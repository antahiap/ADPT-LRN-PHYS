[
    {
        "id": "",
        "section": "Abstract",
        "text": "Neural Machine Translation (NMT) is an end-to-end learning approach for automated translation,\nwith the potential to overcome many of the weaknesses of conventional phrase-based translation systems.\nUnfortunately, NMTsystemsareknowntobecomputationallyexpensivebothintrainingandintranslation\ninference \u2013 sometimes prohibitively so in the case of very large data sets and large models. Several authors\nhave also charged that NMT systems lack robustness, particularly when input sentences contain rare words.\nThese issues have hindered NMT\u2019s use in practical deployments and services, where both accuracy and\nspeed are essential. In this work, we present GNMT, Google\u2019s Neural Machine Translation system, which\nattempts to address many of these issues. Our model consists of a deep LSTM network with 8 encoder\nand 8 decoder layers using residual connections as well as attention connections from the decoder network\nto the encoder. To improve parallelism and therefore decrease training time, our attention mechanism\nconnects the bottom layer of the decoder to the top layer of the encoder. To accelerate the \ufb01nal translation\nspeed, we employ low-precision arithmetic during inference computations. To improve handling of rare\nwords, we divide words into a limited set of common sub-word units (\u201cwordpieces\u201d) for both input and\noutput. This method provides a good balance between the \ufb02exibility of \u201ccharacter\u201d-delimited models and\nthe e\ufb03ciency of \u201cword\u201d-delimited models, naturally handles translation of rare words, and ultimately\nimproves the overall accuracy of the system. Our beam search technique employs a length-normalization\nprocedure and uses a coverage penalty, which encourages generation of an output sentence that is most\nlikely to cover all the words in the source sentence. To directly optimize the translation BLEU scores,\nwe consider re\ufb01ning the models by using reinforcement learning, but we found that the improvement\nin the BLEU scores did not re\ufb02ect in the human evaluation. On the WMT\u201914 English-to-French and\nEnglish-to-German benchmarks, GNMT achieves competitive results to state-of-the-art. Using a human\nside-by-side evaluation on a set of isolated simple sentences, it reduces translation errors by an average of\n60% compared to Google\u2019s phrase-based production system.",
        "subsection": []
    },
    {
        "id": "1",
        "section": "Introduction",
        "text": "Neural Machine Translation (NMT) [ 41,2] has recently been introduced as a promising approach with the\npotential of addressing many shortcomings of traditional machine translation systems. The strength of NMT\nlies in its ability to learn directly, in an end-to-end fashion, the mapping from input text to associated\noutput text. Its architecture typically consists of two recurrent neural networks (RNNs), one to consume the\ninput text sequence and one to generate translated output text. NMT is often accompanied by an attention\nmechanism [2] which helps it cope e\ufb00ectively with long input sequences.\nAn advantage of Neural Machine Translation is that it sidesteps many brittle design choices in traditional\nphrase-based machine translation [ 26]. In practice, however, NMT systems used to be worse in accuracy than\nphrase-based translation systems, especially when training on very large-scale datasets as used for the very\nbest publicly available translation systems. Three inherent weaknesses of Neural Machine Translation are\nresponsible for this gap: its slower training and inference speed, ine\ufb00ectiveness in dealing with rare words,\nand sometimes failure to translate all words in the source sentence. Firstly, it generally takes a considerable\namount of time and computational resources to train an NMT system on a large-scale translation dataset,\nthus slowing the rate of experimental turnaround time and innovation. For inference they are generally\nmuch slower than phrase-based systems due to the large number of parameters used. Secondly, NMT lacks\nrobustness in translating rare words. Though this can be addressed in principle by training a \u201ccopy model\u201d to\nmimic a traditional alignment model [ 31], or by using the attention mechanism to copy rare words [ 37], these\napproaches are both unreliable at scale, since the quality of the alignments varies across languages, and the\nlatent alignments produced by the attention mechanism are unstable when the network is deep. Also, simple\ncopying may not always be the best strategy to cope with rare words, for example when a transliteration\nis more appropriate. Finally, NMT systems sometimes produce output sentences that do not translate all\nparts of the input sentence \u2013 in other words, they fail to completely \u201ccover\u201d the input, which can result in\nsurprising translations.\nThis work presents the design and implementation of GNMT, a production NMT system at Google, that\naims to provide solutions to the above problems. In our implementation, the recurrent networks are Long\nShort-Term Memory (LSTM) RNNs [ 23,17]. Our LSTM RNNs have 8 layers, with residual connections\nbetween layers to encourage gradient \ufb02ow [ 21]. For parallelism, we connect the attention from the bottom\nlayer of the decoder network to the top layer of the encoder network. To improve inference time, we\nemploy low-precision arithmetic for inference, which is further accelerated by special hardware (Google\u2019s\nTensor Processing Unit, or TPU). To e\ufb00ectively deal with rare words, we use sub-word units (also known as\n\u201cwordpieces\u201d) [ 35] for inputs and outputs in our system. Using wordpieces gives a good balance between the\n\ufb02exibility of single characters and the e\ufb03ciency of full words for decoding, and also sidesteps the need for\nspecial treatment of unknown words. Our beam search technique includes a length normalization procedure\nto deal e\ufb03ciently with the problem of comparing hypotheses of di\ufb00erent lengths during decoding, and a\ncoverage penalty to encourage the model to translate all of the provided input.\nOur implementation is robust, and performs well on a range of datasets across many pairs of languages\nwithout the need for language-speci\ufb01c adjustments. Using the same implementation, we are able to achieve\nresults comparable to or better than previous state-of-the-art systems on standard benchmarks, while\ndelivering great improvements over Google\u2019s phrase-based production translation system. Speci\ufb01cally, on\nWMT\u201914 English-to-French, our single model scores 38.95 BLEU, an improvement of 7.5 BLEU from a\nsingle model without an external alignment model reported in [ 31] and an improvement of 1.2 BLEU from\na single model without an external alignment model reported in [ 45]. Our single model is also comparable\nto a single model in [ 45], while not making use of any alignment model as being used in [ 45]. Likewise on\nWMT\u201914 English-to-German, our single model scores 24.17 BLEU, which is 3.4 BLEU better than a previous\ncompetitive baseline [ 6]. On production data, our implementation is even more e\ufb00ective. Human evaluations\nshow that GNMT has reduced translation errors by 60% compared to our previous phrase-based system\non many pairs of languages: English \u2194French, English\u2194Spanish, and English \u2194Chinese. Additional\nexperiments suggest the quality of the resulting translation system gets closer to that of average human\ntranslators.",
        "subsection": []
    },
    {
        "id": "2",
        "section": "Related work",
        "text": "Statistical Machine Translation (SMT) has been the dominant translation paradigm for decades [ 3,4,5].\nPractical implementations of SMT are generally phrase-based systems (PBMT) which translate sequences of\nwords or phrases where the lengths may di\ufb00er [26].\nEven prior to the advent of direct Neural Machine Translation, neural networks have been used as a\ncomponent within SMT systems with some success. Perhaps one of the most notable attempts involved the use\nof a joint language model to learn phrase representations [ 13] which yielded an impressive improvement when\ncombined with phrase-based translation. This approach, however, still makes use of phrase-based translation\nsystems at its core, and therefore inherits their shortcomings. Other proposed approaches for learning phrase\nrepresentations [ 7] or learning end-to-end translation with neural networks [ 24] o\ufb00ered encouraging hints, but\nultimately delivered worse overall accuracy compared to standard phrase-based systems.\nThe concept of end-to-end learning for machine translation has been attempted in the past (e.g., [ 8]) with\nlimited success. Following seminal papers in the area [ 41,2], NMT translation quality has crept closer to\nthe level of phrase-based translation systems for common research benchmarks. Perhaps the \ufb01rst successful\nattempt at surpassing phrase-based translation was described in [ 31]. On WMT\u201914 English-to-French, this\nsystem achieved a 0.5 BLEU improvement compared to a state-of-the-art phrase-based system.\nSince then, many novel techniques have been proposed to further improve NMT: using an attention\nmechanism to deal with rare words [ 37], a mechanism to model translation coverage [ 42], multi-task and\nsemi-supervised training to incorporate more data [ 14,29], a character decoder [ 9], a character encoder [ 11],\nsubword units [ 38] also to deal with rare word outputs, di\ufb00erent kinds of attention mechanisms [ 30], and\nsentence-level loss minimization [ 39,34]. While the translation accuracy of these systems has been encouraging,\nsystematic comparison with large scale, production quality phrase-based translation systems has been lacking.",
        "subsection": [
            {
                "id": "3.1",
                "section": "Residual connections",
                "text": "As mentioned above, deep stacked LSTMs often give better accuracy over shallower models. However, simply\nstacking more layers of LSTM works only to a certain number of layers, beyond which the network becomes\ntoo slow and di\ufb03cult to train, likely due to exploding and vanishing gradient problems [ 33,22]. In our\nexperience with large-scale translation tasks, simple stacked LSTM layers work well up to 4 layers, barely\nwith 6 layers, and very poorly beyond 8 layers.\nFigure 2: The di\ufb00erence between normal stacked LSTM and our stacked LSTM with residual connections.\nOn the left: simple stacked LSTM layers [ 41]. On the right: our implementation of stacked LSTM layers\nwith residual connections. With residual connections, input to the bottom LSTM layer ( x0\ni\u2019s to LSTM 1) is\nelement-wise added to the output from the bottom layer ( x1\ni\u2019s). This sum is then fed to the top LSTM layer\n(LSTM 2) as the new input.\nMotivated by the idea of modeling di\ufb00erences between an intermediate layer\u2019s output and the targets,\nwhich has shown to work well for many projects in the past [ 16,21,40], we introduce residual connections\namong the LSTM layers in a stack (see Figure 2). More concretely, let LSTMiandLSTMi+1be thei-th and\n(i+ 1)-th LSTM layers in a stack, whose parameters are WiandWi+1respectively. At the t-th time step,\nfor the stacked LSTM without residual connections, we have:\nci\nt,mi\nt= LSTMi(ci\nt\u22121,mi\nt\u22121,xi\u22121\nt;Wi)\nxi\nt=mi\nt\nci+1\nt,mi+1\nt= LSTMi+1(ci+1\nt\u22121,mi+1\nt\u22121,xi\nt;Wi+1)(5)\nwhere xi\ntis the input to LSTMiat time step t, and mi\ntandci\ntare the hidden states and memory states of\nLSTMiat time step t, respectively.\nWith residual connections between LSTMiandLSTMi+1, the above equations become:\nci\nt,mi\nt= LSTMi(ci\nt\u22121,mi\nt\u22121,xi\u22121\nt;Wi)\nxi\nt=mi\nt+xi\u22121\nt\nci+1\nt,mi+1\nt= LSTMi+1(ci+1\nt\u22121,mi+1\nt\u22121,xi\nt;Wi+1)(6)\nResidual connections greatly improve the gradient \ufb02ow in the backward pass, which allows us to train very\ndeep encoder and decoder networks. In most of our experiments, we use 8 LSTM layers for the encoder and\ndecoder, though residual connections can allow us to train substantially deeper networks (similar to what\nwas observed in [45]).",
                "subsection": []
            },
            {
                "id": "3.2",
                "section": "Bi-directional encoder for first layer",
                "text": "For translation systems, the information required to translate certain words on the output side can appear\nanywhere on the source side. Often the source side information is approximately left-to-right, similar to\nthe target side, but depending on the language pair the information for a particular output word can be\ndistributed and even be split up in certain regions of the input side.\nTo have the best possible context at each point in the encoder network it makes sense to use a bi-directional\nRNN [36] for the encoder, which was also used in [ 2]. To allow for maximum possible parallelization during\ncomputation (to be discussed in more detail in section 3.3), bi-directional connections are only used for the\nbottom encoder layer \u2013 all other encoder layers are uni-directional. Figure 3 illustrates our use of bi-directional\nLSTMs at the bottom encoder layer. The layer LSTMfprocesses the source sentence from left to right, while\nthe layer LSTMbprocesses the source sentence from right to left. Outputs from LSTMf(\u2212 \u2192\nxf\nt) and LSTMb\n(\u2190 \u2212\nxb\nt) are \ufb01rst concatenated and then fed to the next layer LSTM 1.\nFigure 3: The structure of bi-directional connections in the \ufb01rst layer of the encoder. LSTM layer LSTMf\nprocesses information from left to right, while LSTM layer LSTMbprocesses information from right to left.\nOutput from LSTMfandLSTMbare \ufb01rst concatenated and then fed to the next LSTM layer LSTM 1.",
                "subsection": []
            },
            {
                "id": "3.3",
                "section": "Model parallelism",
                "text": "Due to the complexity of our model, we make use of both model parallelism and data parallelism to speed\nup training. Data parallelism is straightforward: we train nmodel replicas concurrently using a Downpour\nSGD algorithm [ 12]. Thenreplicas all share one copy of model parameters, with each replica asynchronously\nupdating the parameters using a combination of Adam [ 25] and SGD algorithms. In our experiments, nis\noften around 10. Each replica works on a mini-batch of msentence pairs at a time, which is often 128 in our\nexperiments.\nIn addition to data parallelism, model parallelism is used to improve the speed of the gradient computation\non each replica. The encoder and decoder networks are partitioned along the depth dimension and are placed\non multiple GPUs, e\ufb00ectively running each layer on a di\ufb00erent GPU. Since all but the \ufb01rst encoder layer are\nuni-directional, layer i+ 1can start its computation before layer iis fully \ufb01nished, which improves training\nspeed. The softmax layer is also partitioned, with each partition responsible for a subset of symbols in the\noutput vocabulary. Figure 1 shows more details of how partitioning is done.\nModel parallelism places certain constraints on the model architectures we can use. For example, we\ncannot a\ufb00ord to have bi-directional LSTM layers for all the encoder layers, since doing so would reduce\nparallelism among subsequent layers, as each layer would have to wait until both forward and backward\ndirections of the previous layer have \ufb01nished. This would e\ufb00ectively constrain us to make use of only 2 GPUs\nin parallel (one for the forward direction and one for the backward direction). For the attention portion of\nthe model, we chose to align the bottom decoder output to the top encoder output to maximize parallelism\nwhen running the decoder network. Had we aligned the top decoder layer to the top encoder layer, we would\nhave removed all parallelism in the decoder network and would not bene\ufb01t from using more than one GPU\nfor decoding.",
                "subsection": []
            }
        ]
    },
    {
        "id": "3",
        "section": "Model architecture",
        "text": "Our model (see Figure 1) follows the common sequence-to-sequence learning framework [ 41] with attention [ 2].\nIt has three components: an encoder network, a decoder network, and an attention network. The encoder\ntransforms a source sentence into a list of vectors, one vector per input symbol. Given this list of vectors,\nthe decoder produces one symbol at a time, until the special end-of-sentence symbol (EOS) is produced.\nThe encoder and decoder are connected through an attention module which allows the decoder to focus on\ndi\ufb00erent regions of the source sentence during the course of decoding.\nFor notation, we use bold lower case to denote vectors (e.g., v,oi), bold upper case to represent matrices\n(e.g., U,W), cursive upper case to represent sets (e.g., V,T), capital letters to represent sequences (e.g. X,\nY), and lower case to represent individual symbols in a sequence, (e.g., x1,x2).\nLet(X,Y )be a source and target sentence pair. Let X=x1,x2,x3,...,xMbe the sequence of Msymbols\nin the source sentence and let Y=y1,y2,y3,...,yNbe the sequence of Nsymbols in the target sentence. The\nencoder is simply a function of the following form:\nx1,x2,...,xM=EncoderRNN (x1,x2,x3,...,xM) (1)\nIn this equation, x1,x2,...,xMis a list of \ufb01xed size vectors. The number of members in the list is the same\nas the number of symbols in the source sentence ( Min this example). Using the chain rule the conditional\nprobability of the sequence P(Y|X)can be decomposed as:\nP(Y|X) =P(Y|x1,x2,x3,...,xM)\n=N/productdisplay\ni=1P(yi|y0,y1,y2,...,yi\u22121;x1,x2,x3,...,xM)(2)\nwherey0is a special \u201cbeginning of sentence\u201d symbol that is prepended to every target sentence.\nDuring inference we calculate the probability of the next symbol given the source sentence encoding and\nthe decoded target sequence so far:\nP(yi|y0,y1,y2,y3,...,yi\u22121;x1,x2,x3,...,xM) (3)\nOur decoder is implemented as a combination of an RNN network and a softmax layer. The decoder RNN\nnetwork produces a hidden state yifor the next symbol to be predicted, which then goes through the softmax\nlayer to generate a probability distribution over candidate output symbols.\nIn our experiments we found that for NMT systems to achieve good accuracy, both the encoder and\ndecoder RNNs have to be deep enough to capture subtle irregularities in the source and target languages. This\nobservation is similar to previous observations that deep LSTMs signi\ufb01cantly outperform shallow LSTMs [ 41].\nIn that work, each additional layer reduced perplexity by nearly 10%. Similar to [ 31], we use a deep stacked\nLong Short Term Memory (LSTM) [23] network for both the encoder RNN and the decoder RNN.\nOur attention module is similar to [ 2]. More speci\ufb01cally, let yi\u22121be the decoder-RNN output from the\npast decoding time step (in our implementation, we use the output from the bottom decoder layer). Attention\nFigure 1: The model architecture of GNMT, Google\u2019s Neural Machine Translation system. On the left\nis the encoder network, on the right is the decoder network, in the middle is the attention module. The\nbottom encoder layer is bi-directional: the pink nodes gather information from left to right while the green\nnodes gather information from right to left. The other layers of the encoder are uni-directional. Residual\nconnections start from the layer third from the bottom in the encoder and decoder. The model is partitioned\ninto multiple GPUs to speed up training. In our setup, we have 8 encoder LSTM layers (1 bi-directional layer\nand 7 uni-directional layers), and 8 decoder layers. With this setting, one model replica is partitioned 8-ways\nand is placed on 8 di\ufb00erent GPUs typically belonging to one host machine. During training, the bottom\nbi-directional encoder layers compute in parallel \ufb01rst. Once both \ufb01nish, the uni-directional encoder layers\ncan start computing, each on a separate GPU. To retain as much parallelism as possible during running\nthe decoder layers, we use the bottom decoder layer output only for obtaining recurrent attention context,\nwhich is sent directly to all the remaining decoder layers. The softmax layer is also partitioned and placed on\nmultiple GPUs. Depending on the output vocabulary size we either have them run on the same GPUs as the\nencoder and decoder networks, or have them run on a separate set of dedicated GPUs.\ncontext aifor the current time step is computed according to the following formulas:\nst=AttentionFunction (yi\u22121,xt)\u2200t,1\u2264t\u2264M\npt= exp(st)/M/summationdisplay\nt=1exp(st)\u2200t,1\u2264t\u2264M\nai=M/summationdisplay\nt=1pt.xt(4)\nwhereAttentionFunction in our implementation is a feed forward network with one hidden layer.",
        "subsection": [
            {
                "id": "4.1",
                "section": "Wordpiece model",
                "text": "Our most successful approach falls into the second category (sub-word units), and we adopt the wordpiece\nmodel (WPM) implementation initially developed to solve a Japanese/Korean segmentation problem for the\nGoogle speech recognition system [ 35]. This approach is completely data-driven and guaranteed to generate\na deterministic segmentation for any possible sequence of characters. It is similar to the method used in [ 38]\nto deal with rare words in Neural Machine Translation.\nFor processing arbitrary words, we \ufb01rst break words into wordpieces given a trained wordpiece model.\nSpecial word boundary symbols are added before training of the model such that the original word sequence\ncan be recovered from the wordpiece sequence without ambiguity. At decoding time, the model \ufb01rst produces\na wordpiece sequence, which is then converted into the corresponding word sequence.\nHere is an example of a word sequence and the corresponding wordpiece sequence:\n\u2022Word: Jet makers feud over seat width with big orders at stake\n\u2022wordpieces : _J et _makers _fe ud _over _seat _width _with _big _orders _at _stake\nIn the above example, the word \u201cJet\u201d is broken into two wordpieces \u201c_J\u201d and \u201cet\u201d, and the word \u201cfeud\u201d\nis broken into two wordpieces \u201c_fe\u201d and \u201cud\u201d. The other words remain as single wordpieces. \u201c_\u201d is a special\ncharacter added to mark the beginning of a word.\nThe wordpiece model is generated using a data-driven approach to maximize the language-model likelihood\nof the training data, given an evolving word de\ufb01nition. Given a training corpus and a number of desired\ntokensD, the optimization problem is to select Dwordpieces such that the resulting corpus is minimal in the\nnumber of wordpieces when segmented according to the chosen wordpiece model. Our greedy algorithm to\nthis optimization problem is similar to [ 38] and is described in more detail in [ 35]. Compared to the original\nimplementation used in [ 35], we use a special symbol only at the beginning of the words and not at both ends.\nWe also cut the number of basic characters to a manageable number depending on the data (roughly 500 for\nWestern languages, more for Asian languages) and map the rest to a special unknown character to avoid\npolluting the given wordpiece vocabulary with very rare characters. We \ufb01nd that using a total vocabulary of\nbetween 8k and 32k wordpieces achieves both good accuracy (BLEU scores) and fast decoding speed across\nall pairs of language pairs we have tried.\nAs mentioned above, in translation it often makes sense to copy rare entity names or numbers directly\nfrom the source to the target. To facilitate this type of direct copying, we always use a shared wordpiece\nmodel for both the source language and target language. Using this approach, it is guaranteed that the same\nstring in source and target sentence will be segmented in exactly the same way, making it easier for the\nsystem to learn to copy these tokens.\nWordpieces achieve a balance between the \ufb02exibility of characters and e\ufb03ciency of words. We also \ufb01nd\nthat our models get better overall BLEU scores when using wordpieces \u2013 possibly due to the fact that our\nmodels now deal e\ufb03ciently with an essentially in\ufb01nite vocabulary without resorting to characters only. The\nlatter would make the average lengths of the input and output sequences much longer, and therefore would\nrequire more computation.",
                "subsection": []
            },
            {
                "id": "4.2",
                "section": "Mixed word",
                "text": "A second approach we use is the mixed word/character model. As in a word model, we keep a \ufb01xed-size\nword vocabulary. However, unlike in a conventional word model where OOV words are collapsed into a single\nUNK symbol, we convert OOV words into the sequence of its constituent characters. Special pre\ufb01xes are\nprepended to the characters, to 1) show the location of the characters in a word, and 2) to distinguish them\nfrom normal in-vocabulary characters. There are three pre\ufb01xes: <B>,<M>, and <E> , indicating beginning of\nthe word, middle of the word and end of the word, respectively. For example, let\u2019s assume the word Mikiis\nnot in the vocabulary. It will be preprocessed into a sequence of special tokens: <B>M <M>i <M>k <E>i . The\nprocess is done on both the source and the target sentences. During decoding, the output may also contain\nsequences of special tokens. With the pre\ufb01xes, it is trivial to reverse the tokenization to the original words as\npart of a post-processing step.",
                "subsection": []
            }
        ]
    },
    {
        "id": "4",
        "section": "Segmentation approaches",
        "text": "Neural Machine Translation models often operate with \ufb01xed word vocabularies even though translation is\nfundamentally an open vocabulary problem (names, numbers, dates etc.). There are two broad categories of\napproaches to address the translation of out-of-vocabulary (OOV) words. One approach is to simply copy\nrare words from source to target (as most rare words are names or numbers where the correct translation is\njust a copy), either based on the attention model [ 37], using an external alignment model [ 31], or even using\na more complicated special purpose pointing network [ 18]. Another broad category of approaches is to use\nsub-word units , e.g., chararacters [10], mixed word/characters [28], or more intelligent sub-words [38].",
        "subsection": []
    },
    {
        "id": "5",
        "section": "Training criteria",
        "text": "Given a dataset of parallel text containing Ninput-output sequence pairs, denoted D\u2261/braceleftbig\n(X(i),Y\u2217(i))/bracerightbigN\ni=1,\nstandard maximum-likelihood training aims at maximizing the sum of log probabilities of the ground-truth\noutputs given the corresponding inputs,\nOML(\u03b8) =N/summationdisplay\ni=1logP\u03b8(Y\u2217(i)|X(i)). (7)\nThe main problem with this objective is that it does not re\ufb02ect the task reward function as measured by the\nBLEU score in translation. Further, this objective does not explicitly encourage a ranking among incorrect\noutput sequences \u2013 where outputs with higher BLEU scores should still obtain higher probabilities under the\nmodel \u2013 since incorrect outputs are never observed during training. In other words, using maximum-likelihood\ntraining only, the model will not learn to be robust to errors made during decoding since they are never\nobserved, which is quite a mismatch between the training and testing procedure.\nSeveral recent papers [ 34,39,32] have considered di\ufb00erent ways of incorporating the task reward into\noptimization of neural sequence-to-sequence models. In this work, we also attempt to re\ufb01ne a model pre-\ntrained on the maximum likelihood objective to directly optimize for the task reward. We show that, even on\nlarge datasets, re\ufb01nement of state-of-the-art maximum-likelihood models using task reward improves the\nresults considerably.\nWe consider model re\ufb01nement using the expected reward objective (also used in [ 34]), which can be\nexpressed as\nORL(\u03b8) =N/summationdisplay\ni=1/summationdisplay\nY\u2208YP\u03b8(Y|X(i))r(Y,Y\u2217(i)). (8)\nHere,r(Y,Y\u2217(i))denotes the per-sentence score, and we are computing an expectation over all of the output\nsentencesY, up to a certain length.\nThe BLEU score has some undesirable properties when used for single sentences, as it was designed to\nbe a corpus measure. We therefore use a slightly di\ufb00erent score for our RL experiments which we call the\n\u201cGLEU score\u201d. For the GLEU score, we record all sub-sequences of 1, 2, 3 or 4 tokens in output and target\nsequence (n-grams). We then compute a recall, which is the ratio of the number of matching n-grams to\nthe number of total n-grams in the target (ground truth) sequence, and a precision, which is the ratio of\nthe number of matching n-grams to the number of total n-grams in the generated output sequence. Then\nGLEU score is simply the minimum of recall and precision. This GLEU score\u2019s range is always between 0\n(no matches) and 1 (all match) and it is symmetrical when switching output and target. According to our\nexperiments, GLEU score correlates quite well with the BLEU metric on a corpus level but does not have its\ndrawbacks for our per sentence reward objective.\nAs is common practice in reinforcement learning, we subtract the mean reward from r(Y,Y\u2217(i))in equation",
        "subsection": []
    },
    {
        "id": "8.",
        "section": "The mean is estimated to be the sample mean of msequences drawn independently from distribution",
        "text": "P\u03b8(Y|X(i)). In our implementation, mis set to be 15. To further stabilize training, we optimize a linear\ncombination of ML (equation 7) and RL (equation 8) objectives as follows:\nOMixed (\u03b8) =\u03b1\u2217OML(\u03b8) +ORL(\u03b8) (9)\n\u03b1in our implementation is typically set to be 0.017.\nIn our setup, we \ufb01rst train a model using the maximum likelihood objective (equation 7) until convergence.\nWe then re\ufb01ne this model using a mixed maximum likelihood and expected reward objective (equation 9),\nuntil BLEU score on a development set is no longer improving. The second step is optional.",
        "subsection": []
    },
    {
        "id": "6",
        "section": "Quantizable model and quantized inference",
        "text": "One of the main challenges in deploying our Neural Machine Translation model to our interactive production\ntranslation service is that it is computationally intensive at inference, making low latency translation di\ufb03cult,\nand high volume deployment computationally expensive. Quantized inference using reduced precision\narithmetic is one technique that can signi\ufb01cantly reduce the cost of inference for these models, often providing\ne\ufb03ciency improvements on the same computational devices. For example, in [ 43], it is demonstrated that a\nconvolutional neural network model can be sped up by a factor of 4-6 with minimal loss on classi\ufb01cation\naccuracy on the ILSVRC-12 benchmark. In [ 27], it is demonstrated that neural network model weights can\nbe quantized to only three states, -1, 0, and +1.\nMany of those previous studies [ 19,20,43,27] however mostly focus on CNN models with relatively few\nlayers. Deep LSTMs with long sequences pose a novel challenge in that quantization errors can be signi\ufb01cantly\nampli\ufb01ed after many unrolled steps or after going through a deep LSTM stack.\nIn this section, we present our approach to speed up inference with quantized arithmetic. Our solution\nis tailored towards the hardware options available at Google. To reduce quantization errors, additional\nconstraints are added to our model during training so that it is quantizable with minimal impact on\nthe output of the model. That is, once a model is trained with these additional constraints, it can be\nsubsequently quantized without loss to translation quality. Our experimental results suggest that those\nadditional constraints do not hurt model convergence nor the quality of a model once it has converged.\nRecall from equation 6 that in an LSTM stack with residual connections there are two accumulators: ci\nt\nalong the time axis and xi\ntalong the depth axis. In theory, both of the accumulators are unbounded, but\nin practice, we noticed their values remain quite small. For quantized inference, we explicitly constrain the\nvalues of these accumulators to be within [- \u03b4,\u03b4] to guarantee a certain range that can be used for quantization\nlater. The forward computation of an LSTM stack with residual connections is modi\ufb01ed to the following:\nc/primei\nt,mi\nt= LSTMi(ci\nt\u22121,mi\nt\u22121,xi\u22121\nt;Wi)\nci\nt= max(\u2212\u03b4,min(\u03b4,c/primei\nt))\nx/primei\nt=mi\nt+xi\u22121\nt\nxi\nt= max(\u2212\u03b4,min(\u03b4,x/primei\nt))\nc/primei+1\nt,mi+1\nt= LSTMi+1(ci+1\nt\u22121,mi+1\nt\u22121,xi\nt;Wi+1)\nci+1\nt= max(\u2212\u03b4,min(\u03b4,c/primei+1\nt))(10)\nLet us expand LSTMiin equation 10 to include the internal gating logic. For brevity, we drop all the\nsuperscripts i.\nW= [W1,W2,W3,W4,W5,W6,W7,W8]\nit=sigmoid (W1xt+W2mt)\ni/prime\nt= tanh( W3xt+W4mt)\nft=sigmoid (W5xt+W6mt)\not=sigmoid (W7xt+W8mt)\nct=ct\u22121\u2299ft+i/prime\nt\u2299it\nmt=ct\u2299ot(11)\nWhen doing quantized inference, we replace all the \ufb02oating point operations in equations 10 and 11 with\n\ufb01xed-point integer operations with either 8-bit or 16-bit resolution. The weight matrix Wabove is represented\nusing an 8-bit integer matrix WQand a \ufb02oat vector s, as shown below:\nsi= max(abs(W[i,:]))\nWQ[i,j] =round (W[i,j]/si\u00d7127.0)(12)\nAll accumulator values ( ci\ntandxi\nt) are represented using 16-bit integers representing the range [\u2212\u03b4,\u03b4]. All\nmatrix multiplications (e.g., W1xt,W2mt, etc.) in equation 11 are done using 8-bit integer multiplication\naccumulated into larger accumulators. All other operations, including all the activations ( sigmoid,tanh) and\nelementwise operations ( \u2299,+) are done using 16-bit integer operations.\nWe now turn our attention to the log-linear softmax layer. During training, given the decoder RNN\nnetwork output yt, we compute the probability vector ptover all candidate output symbols as follows:\nvt=Ws\u2217yt\nv/prime\nt= max(\u2212\u03b3,min(\u03b3,vt))\npt=softmax (v/prime\nt)(13)\nIn equation 13, Wsis the weight matrix for the linear layer, which has the same number of rows as the\nnumber of symbols in the target vocabulary with each row corresponding to one unique target symbol.\nvrepresents the raw logits, which are \ufb01rst clipped to be between \u2212\u03b3and\u03b3and then normalized into a\nprobability vector p. Input ytis guaranteed to be between \u2212\u03b4and\u03b4due to the quantization scheme we\napplied to the decoder RNN. The clipping range \u03b3for the logits vis determined empirically, and in our case,\nit is set to 25. In quantized inference, the weight matrix Wsis quantized into 8 bits as in equation 12, and\nthe matrix multiplication is done using 8 bit arithmetic. The calculations within the softmax function and\nthe attention model are not quantized during inference.\nIt is worth emphasizing that during training of the model we use full-precision \ufb02oating point numbers.\nThe only constraints we add to the model during training are the clipping of the RNN accumulator values\ninto[\u2212\u03b4,\u03b4]and softmax logits into [\u2212\u03b3,\u03b3].\u03b3is \ufb01xed to be at 25.0, while the value for \u03b4is gradually annealed\nfrom a generous bound of \u03b4= 8.0at the beginning of training, to a rather stringent bound of \u03b4= 1.0towards\nthe end of training. At inference time, \u03b4is \ufb01xed at 1.0. Those additional constraints do not degrade model\nconvergence nor the decoding quality of the model when it has converged. In Figure 4, we compare the loss\nvs. steps for an unconstrained model (the blue curve) and a constrained model (the red curve) on WMT\u201914\nEnglish-to-French. We can see that the loss for the constrained model is slightly better, possibly due to\nregularization roles those constraints play.\nOur solution strikes a good balance between e\ufb03ciency and accuracy. Since the computationally expensive\noperations (the matrix multiplications) are done using 8-bit integer operations, our quantized inference is\nquite e\ufb03cient. Also, since error-sensitive accumulator values are stored using 16-bit integers, our solution is\nvery accurate and is robust to quantization errors.\nIn Table 1 we compare the inference speed and quality when decoding the WMT\u201914 English-to-French\ndevelopment set (a concatenation of newstest2012 and newstest2013 test sets for a total of 6003 sentences) on\n0.511.522.533.544.55  \nNormal training\nQuantized trainingFigure 4: Log perplexity vs. steps for normal (non-quantized) training and quantization-aware training on\nWMT\u201914 English to French during maximum likelihood training. Notice the training losses are similar, with\nthe quantization-aware loss being slightly better. Our conjecture for quantization-aware training being slightly\nbetter is that the clipping constraints act as additional regularization which improves the model quality.\nCPU, GPU and Google\u2019s Tensor Processing Unit (TPU) respectively.1The model used here for comparison\nis trained with quantization constraints on the ML objective only (i.e., without reinforcement learning based\nmodel re\ufb01nement). When the model is decoded on CPU and GPU, it is not quantized and all operations are\ndone using full-precision \ufb02oats. When it is decoded on TPU, certain operations, such as embedding lookup\nand attention module, remain on the CPU, and all other quantized operations are o\ufb00-loaded to the TPU. In\nall cases, decoding is done on a single machine with two Intel Haswell CPUs, which consists in total of 88\nCPU cores (hyperthreads). The machine is equipped with an NVIDIA GPU (Tesla k80) for the experiment\nwith GPU or a single Google TPU for the experiment with TPU.\nTable 1 shows that decoding using reduced precision arithmetics on the TPU su\ufb00ers a very minimal loss\nof 0.0072 on log perplexity, and no loss on BLEU at all. This result matches previous work reporting that\nquantizing convolutional neural network models can retain most of the model quality.\nTable 1 also shows that decoding our model on CPU is actually 2.3 times faster than on GPU. Firstly,\nour dual-CPUs host machine o\ufb00ers a theoretical peak FLOP performance which is more than two thirds that\nof the GPU. Secondly, the beam search algorithm forces the decoder to incur a non-trivial amount of data\ntransfer between the host and the GPU at every decoding step. Hence, our current decoder implementation\n1https://cloudplatform.googleblog.com/2016/05/Google-supercharges-machine-learning-tasks-with-custom-chip.html\nis not fully utilizing the computation capacities that a GPU can theoretically o\ufb00er during inference.\nFinally, Table 1 shows that decoding on TPUs is 3.4 times faster than decoding on CPUs, demonstrating\nthat quantized arithmetics is much faster on TPUs than both CPUs or GPUs.\nTable 1: Model inference on CPU, GPU and TPU. The model used here for comparison is trained with\nthe ML objective only with quantization constraints. Results are obtained by decoding the WMT En \u2192Fr\ndevelopment set on CPU, GPU and TPU respectively.\nBLEU Log Perplexity Decoding time (s)\nCPU 31.20 1.4553 1322\nGPU 31.20 1.4553 3028\nTPU 31.21 1.4626 384\nUnless otherwise noted, we always train and evaluate quantized models in our experiments. Because there\nis little di\ufb00erence from a quality perspective between a model decoded on CPUs and one decoded on TPUs,\nwe use CPUs to decode for model evaluation during training and experimentation and use TPUs to serve\nproduction tra\ufb03c.",
        "subsection": [
            {
                "id": "8.1",
                "section": "Datasets",
                "text": "We evaluate our model on the WMT En \u2192Fr dataset, the WMT En \u2192De dataset, as well as many Google-\ninternal production datasets. On WMT En \u2192Fr, the training set contains 36M sentence pairs. On WMT\nEn\u2192De, the training set contains 5M sentence pairs. In both cases, we use newstest2014 as the test sets to\ncompare against previous work [ 31,37,45]. The combination of newstest2012 and newstest2013 is used as\nthe development set.\nIn addition to WMT, we also evaluate our model on some Google-internal datasets representing a wider\nspectrum of languages with distinct linguistic properties: English \u2194French, English\u2194Spanish and English\n\u2194Chinese.",
                "subsection": []
            },
            {
                "id": "8.2",
                "section": "Evaluation metrics",
                "text": "We evaluate our models using the standard BLEU score metric. To be comparable to previous work [ 41,31,45],\nwe report tokenized BLEU score as computed by the multi-bleu.pl script, downloaded from the public\nimplementation of Moses (on Github), which is also used in [31].\nAs is well-known, BLEU score does not fully capture the quality of a translation. For that reason we also\ncarry out side-by-side (SxS) evaluations where we have human raters evaluate and compare the quality of\ntwo translations presented side by side for a given source sentence. Side-by-side scores range from 0 to 6,\nwith a score of 0 meaning \u201ccompletely nonsense translation\u201d , and a score of 6 meaning \u201cperfect translation:\nthe meaning of the translation is completely consistent with the source, and the grammar is correct\u201d . A\ntranslation is given a score of 4 if \u201cthe sentence retains most of the meaning of the source sentence, but may\nhave some grammar mistakes\u201d , and a translation is given a score of 2 if \u201cthe sentence preserves some of the\nmeaning of the source sentence but misses signi\ufb01cant parts\u201d . These scores are generated by human raters\nwho are \ufb02uent in both languages and hence often capture translation quality better than BLEU scores.",
                "subsection": []
            },
            {
                "id": "8.3",
                "section": "Training procedure",
                "text": "The models are trained by a system we implemented using TensorFlow[ 1]. The training setup follows the\nclassic data parallelism paradigm. There are 12 replicas running concurrently on separate machines. Every\nreplica updates the shared parameters asynchronously.\nWe initialize all trainable parameters uniformly between [-0.04, 0.04]. As is common wisdom in training\nRNN models, we apply gradient clipping (similar to [ 41]): all gradients are uniformly scaled down such that\nthe norm of the modi\ufb01ed gradients is no larger than a \ufb01xed constant, which is 5.0in our case. If the norm of\nthe original gradients is already smaller than or equal to the given threshold, then gradients are not changed.\nFor the \ufb01rst stage of maximum likelihood training (that is, to optimize for objective function 7), we\nuse a combination of Adam [ 25] and simple SGD learning algorithms provided by the TensorFlow runtime\nsystem. We run Adam for the \ufb01rst 60k steps, after which we switch to simple SGD. Each step in training is a\nmini-batch of 128 examples.\nWe \ufb01nd that Adam accelerates training at the beginning, but Adam alone converges to a worse point\nthan a combination of Adam \ufb01rst, followed by SGD (Figure 5). For the Adam part, we use a learning rate of\n0.511.522.533.544.55  \nSGD only\nAdam only\nAdam then SGDFigure 5: Log perplexity vs. steps for Adam, SGD and Adam-then-SGD on WMT En \u2192Fr during maximum\nlikelihood training. Adam converges much faster than SGD at the beginning. Towards the end, however,\nAdam-then-SGD is gradually better. Notice the bump in the red curve (Adam-then-SGD) at around 60k\nsteps where we switch from Adam to SGD. We suspect that this bump occurs due to di\ufb00erent optimization\ntrajectories of Adam vs. SGD. When we switch from Adam to SGD, the model \ufb01rst su\ufb00ers a little, but is\nable to quickly recover afterwards.\n0.0002, and for the SGD part, we use a learning rate of 0.5. We \ufb01nd that it is important to also anneal the\nlearning rate after a certain number of total steps. For the WMT En \u2192Fr dataset, we begin to anneal the\nlearning rate after 1.2M steps, after which we halve the learning rate every 200k steps for an additional 800k\nsteps. On WMT En \u2192Fr, it takes around 6 days to train a basic model using 96 NVIDIA K80 GPUs.\nOnce a model is fully converged using the ML objective, we switch to RL based model re\ufb01nement, i.e., we\nfurther optimize the objective function as in equation 9. We re\ufb01ne a model until the BLEU score does not\nchange much on the development set. For this model re\ufb01nement phase, we simply run the SGD optimization\nalgorithm. The number of steps needed to re\ufb01ne a model varies from dataset to dataset. For WMT En \u2192Fr,\nit takes around 3 days to complete 400k steps.\nTo prevent over\ufb01tting, we apply dropout during training with a scheme similar to [ 44]. For the WMT\nEn\u2192Fr and En\u2192De datasets, we set the dropout probability to be 0.2and0.3respectively. Due to various\ntechnical reasons, dropout is only applied during the ML training phase, not during the RL re\ufb01nement phase.\nThe exact hyper-parameters vary from dataset to dataset and from model to model. For the WMT\nEn\u2192De dataset, since it is signi\ufb01cantly smaller than the WMT En \u2192Fr dataset, we use a higher dropout\nprobability, and also train smaller models for fewer steps overall. On the production data sets, we typically\ndo not use dropout, and we train the models for more steps.",
                "subsection": []
            },
            {
                "id": "8.4",
                "section": "Evaluation after maximum likelihood training",
                "text": "The models in our experiments are word-based, character-based, mixed word-character-based or several\nwordpiece models with varying vocabulary sizes.\nFor the word model, we selected the most frequent 212K source words as the source vocabulary and the\nmost popular 80k target words as the target vocabulary. Words not in the source vocabulary or the target\nvocabulary (unknown words) are converted into special <first_char>_UNK_<last_char> symbols. Note, in\nthis case, there is more than one UNK (e.g., our production word models have roughly 5000 di\ufb00erent UNKs\nin this case). We then use the attention mechanism to copy a corresponding word from the source to replace\nthese unknown words during decoding [37].\nThe mixed word-character model is similar to the word model, except the out-of-vocabulary (OOV) words\nare converted into sequences of characters with special delimiters around them as described in section 4.2 in\nmore detail. In our experiments, the vocabulary size for the mixed word-character model is 32K. For the pure\ncharacter model, we simply split all words into constituent characters, resulting typically in a few hundred\nbasic characters (including special symbols appearing in the data). For the wordpiece models, we train 3\ndi\ufb00erent models with vocabulary sizes of 8K, 16K, and 32K.\nTable 4 summarizes our results on the WMT En \u2192Fr dataset. In this table, we also compare against other\nstrong baselines without model ensembling. As can be seen from the table, \u201cWPM-32K\u201d, a wordpiece model\nwith a shared source and target vocabulary of 32K wordpieces, performs well on this dataset and achieves the\nbest quality as well as the fastest inference speed.\nThe pure character model (char input, char output) works surprisingly well on this task, not much worse\nthan the best wordpiece models in BLEU score. However, these models are rather slow to train and slow to\nuse as the sequences are much longer.\nOur best model, WPM-32K, achieves a BLEU score of 38.95. Note that this BLEU score represents the\naveraged score of 8 models we trained. The maximum BLEU score of the 8 models is higher at 39.37. We\npoint out that our models are completely self-contained, as opposed to previous models reported in [ 45],\nwhich depend on some external alignment models to achieve their best results. Also note that all our test set\nnumbers were achieved by picking an optimal model on the development set which was then used to decode\nthe test set.\nNote that the timing numbers for this section are obtained on CPUs, not TPUs. We use here the same\nCPU machine as described above, and run the decoder with a batchsize of 16 sentences in parallel and a\nmaximum of 4 concurrent hypotheses at any time per sentence. The time per sentence is the total decoding\ntime divided by the number of respective sentences in the test set.\nTable 4: Single model results on WMT En \u2192Fr (newstest2014)\nModel BLEU CPU decoding time\nper sentence (s)\nWord 37.90 0.2226\nCharacter 38.01 1.0530\nWPM-8K 38.27 0.1919\nWPM-16K 37.60 0.1874\nWPM-32K 38.95 0.2118\nMixed Word/Character 38.39 0.2774\nPBMT [15] 37.0\nLSTM (6 layers) [31] 31.5\nLSTM (6 layers + PosUnk) [31] 33.1\nDeep-Att [45] 37.7\nDeep-Att + PosUnk [45] 39.2\nSimilarly, the results of WMT En \u2192De are presented in Table 5. Again, we \ufb01nd that wordpiece models\nachieves the best BLEU scores.\nTable 5: Single model results on WMT En \u2192De (newstest2014)\nModel BLEU CPU decoding time\nper sentence (s)\nWord 23.12 0.2972\nCharacter (512 nodes) 22.62 0.8011\nWPM-8K 23.50 0.2079\nWPM-16K 24.36 0.1931\nWPM-32K 24.61 0.1882\nMixed Word/Character 24.17 0.3268\nPBMT [6] 20.7\nRNNSearch [37] 16.5\nRNNSearch-LV [37] 16.9\nRNNSearch-LV [37] 16.9\nDeep-Att [45] 20.6\nWMT En\u2192De is considered a more di\ufb03cult task than WMT En \u2192Fr as it has much less training data,\nand German, as a more morphologically rich language, needs a huge vocabulary for word models. Thus\nit is more advantageous to use wordpiece or mixed word/character models, which provide a gain of more\nthan 2 BLEU points on top of the word model and about 4 BLEU points on top of previously reported\nresults in [ 6,45]. Our best model, WPM-32K, achieves a BLEU score of 24.61, which is averaged over 8 runs.\nConsistently, on the production corpora, wordpiece models tend to be better than other models both in terms\nof speed and accuracy.",
                "subsection": []
            },
            {
                "id": "8.5",
                "section": "Evaluation of rl-re",
                "text": "The models trained in the previous section are optimized for log-likelihood of the next step prediction which\nmay not correlate well with translation quality, as discussed in section 5. We use RL training to \ufb01ne-tune\nsentence BLEU scores after normal maximum-likelihood training.\nThe results of RL \ufb01ne-tuning on the best En \u2192Fr and En\u2192De models are presented in Table 6, which\nshow that \ufb01ne-tuning the models with RL can improve BLEU scores. On WMT En \u2192Fr, model re\ufb01nement\nimproves BLEU score by close to 1 point. On En \u2192De, RL-re\ufb01nement slightly hurts the test performance\neven though we observe about 0.4 BLEU points improvement on the development set. The results presented\nin Table 6 are the average of 8 independent models. We also note that there is an overlap between the\nwins from the RL re\ufb01nement and the decoder \ufb01ne-tuning (i.e., the introduction of length normalization and\ncoverage penalty). On a less \ufb01ne-tuned decoder (e.g., if the decoder does beam search by log-probability\nonly), the win from RL would have been bigger (as is evident from comparing results in Table 2 and Table 3).\nTable 6: Single model test BLEU scores, averaged over 8 runs, on WMT En \u2192Fr and En\u2192De\nDataset Trained with log-likelihood Re\ufb01ned with RL\nEn\u2192Fr 38.95 39.92\nEn\u2192De 24.67 24.60",
                "subsection": []
            },
            {
                "id": "8.6",
                "section": "Model ensemble and human evaluation",
                "text": "We ensemble 8 RL-re\ufb01ned models to obtain a state-of-the-art result of 41.16 BLEU points on the WMT\nEn\u2192Fr dataset. Our results are reported in Table 7.\nWe ensemble 8 RL-re\ufb01ned models to obtain a state-of-the-art result of 26.30 BLEU points on the WMT\nEn\u2192De dataset. Our results are reported in Table 8.\nFinally, to better understand the quality of our models and the e\ufb00ect of RL re\ufb01nement, we carried out a\nfour-way side-by-side human evaluation to compare our NMT translations against the reference translations\nTable 7: Model ensemble results on WMT En \u2192Fr (newstest2014)\nModel BLEU\nWPM-32K (8 models) 40.35\nRL-re\ufb01ned WPM-32K (8 models) 41.16\nLSTM (6 layers) [31] 35.6\nLSTM (6 layers + PosUnk) [31] 37.5\nDeep-Att + PosUnk (8 models) [45] 40.4\nTable 8: Model ensemble results on WMT En \u2192De (newstest2014). See Table 5 for a comparison against\nnon-ensemble models.\nModel BLEU\nWPM-32K (8 models) 26.20\nRL-re\ufb01ned WPM-32K (8 models) 26.30\nand the best phrase-based statistical machine translations. During the side-by-side comparison, humans\nare asked to rate four translations given a source sentence. The four translations are: 1) the best phrase-\nbased translations as downloaded from http://matrix.statmt.org/systems/show/2065 , 2) an ensemble of 8\nML-trained models, 3) an ensemble of 8 ML-trained and then RL-re\ufb01ned models, and 4) reference human\ntranslations as taken directly from newstest2014, Our results are presented in Table 9.\nTable 9: Human side-by-side evaluation scores of WMT En \u2192Fr models.\nModel BLEU Side-by-side\naveraged score\nPBMT [15] 37.0 3.87\nNMT before RL 40.35 4.46\nNMT after RL 41.16 4.44\nHuman 4.82\nThe results show that even though RL re\ufb01nement can achieve better BLEU scores, it barely improves the\nhuman impression of the translation quality. This could be due to a combination of factors including: 1) the\nrelatively small sample size for the experiment (only 500 examples for side-by-side), 2) the improvement in\nBLEU score by RL is relatively small after model ensembling (0.81), which may be at a scale that human\nside-by-side evaluations are insensitive to, and 3) the possible mismatch between BLEU as a metric and\nreal translation quality as perceived by human raters. Table 11 contains some example translations from\nPBMT, \"NMT before RL\" and \"Human\", along with the side-by-side scores that human raters assigned to\neach translation (some of which we disagree with, see the table caption).",
                "subsection": []
            },
            {
                "id": "8.7",
                "section": "Results on production data",
                "text": "We have carried out extensive experiments on many Google-internal production data sets. As the experiments\nabove cast doubt on whether RL improves the real translation quality or simply the BLEU metric, RL-based\nmodel re\ufb01nement is not used during these experiments. Given the larger volume of training data available in\nthe Google corpora, dropout is also not needed in these experiments.\nIn this section we describe our experiments with human perception of the translation quality. We asked\nhuman raters to rate translations in a three-way side-by-side comparison. The three sides are from: 1)\ntranslations from the production phrase-based statistical translation system used by Google, 2) translations\nfrom our GNMT system, and 3) translations by humans \ufb02uent in both languages. Reported here in Table 10\nare averaged rated scores for English \u2194French, English\u2194Spanish and English \u2194Chinese. All the GNMT\nmodels are wordpiece models, without model ensembling, and use a shared source and target vocabulary with\n32K wordpieces. On each pair of languages, the evaluation data consist of 500 randomly sampled sentences\nfrom Wikipedia and news websites, and the corresponding human translations to the target language. The\nTable 10: Mean of side-by-side scores on production data\nPBMT GNMT Human Relative\nImprovement\nEnglish\u2192Spanish 4.885 5.428 5.504 87%\nEnglish\u2192French 4.932 5.295 5.496 64%\nEnglish\u2192Chinese 4.035 4.594 4.987 58%\nSpanish\u2192English 4.872 5.187 5.372 63%\nFrench\u2192English 5.046 5.343 5.404 83%\nChinese\u2192English 3.694 4.263 4.636 60%\nresults show that our model reduces translation errors by more than 60% compared to the PBMT model on\nthese major pairs of languages. A typical distribution of side-by-side scores is shown in Figure 6.\nFigure 6: Histogram of side-by-side scores on 500 sampled sentences from Wikipedia and news websites for a\ntypical language pair, here English \u2192Spanish (PBMT blue, GNMT red, Human orange). It can be seen that\nthere is a wide distribution in scores, even for the human translation when rated by other humans, which\nshows how ambiguous the task is. It is clear that GNMT is much more accurate than PBMT.\nAs expected, on this metric the GNMT system improves also compared to the PBMT system. In some\ncases human and GNMT translations are nearly indistinguishable on the relatively simplistic and isolated\nsentences sampled from Wikipedia and news articles for this experiment. Note that we have observed that\nhuman raters, even though \ufb02uent in both languages, do not necessarily fully understand each randomly\nsampled sentence su\ufb03ciently and hence cannot necessarily generate the best possible translation or rate a\ngiven translation accurately. Also note that, although the scale for the scores goes from 0 (complete nonsense)\nto 6 (perfect translation) the human translations get an imperfect score of only around 5 in Table 10, which\nshows possible ambiguities in the translations and also possibly non-calibrated raters and translators with a\nvarying level of pro\ufb01ciency.\nTesting our GNMT system on particularly di\ufb03cult translation cases and longer inputs than just single\nsentences is the subject of future work.",
                "subsection": []
            }
        ]
    },
    {
        "id": "7",
        "section": "Decoder",
        "text": "We use beam search during decoding to \ufb01nd the sequence Ythat maximizes a score function s(Y,X)given\na trained model. We introduce two important re\ufb01nements to the pure max-probability based beam search\nalgorithm: a coverage penalty [ 42] and length normalization. With length normalization, we aim to account for\nthe fact that we have to compare hypotheses of di\ufb00erent length. Without some form of length-normalization\nregular beam search will favor shorter results over longer ones on average since a negative log-probability\nis added at each step, yielding lower (more negative) scores for longer sentences. We \ufb01rst tried to simply\ndivide by the length to normalize. We then improved on that original heuristic by dividing by length\u03b1,\nwith 0<\u03b1< 1where\u03b1is optimized on a development set ( \u03b1\u2208[0.6\u22120.7]was usually found to be best).\nEventually we designed the empirically-better scoring function below, which also includes a coverage penalty\nto favor translations that fully cover the source sentence according to the attention module.\nMore concretely, the scoring function s(Y,X)that we employ to rank candidate translations is de\ufb01ned as\nfollows:\ns(Y,X) = log(P(Y|X))/lp(Y) +cp(X;Y)\nlp(Y) =(5 +|Y|)\u03b1\n(5 + 1)\u03b1\ncp(X;Y) =\u03b2\u2217|X|/summationdisplay\ni=1log(min(|Y|/summationdisplay\nj=1pi,j,1.0)),(14)\nwherepi,jis the attention probability of the j-th target word yjon thei-th source word xi. By construction\n(equation 4),/summationtext|X|\ni=0pi,jis equal to 1. Parameters \u03b1and\u03b2control the strength of the length normalization\nand the coverage penalty. When \u03b1= 0and\u03b2= 0, our decoder falls back to pure beam search by probability.\nDuring beam search, we typically keep 8-12 hypotheses but we \ufb01nd that using fewer (4 or 2) has only\nslight negative e\ufb00ects on BLEU scores. Besides pruning the number of considered hypotheses, two other\nforms of pruning are used. Firstly, at each step, we only consider tokens that have local scores that are\nnot more than beamsize below the best token for this step. Secondly, after a normalized best score has\nbeen found according to equation 14, we prune all hypotheses that are more than beamsize below the best\nnormalized score so far. The latter type of pruning only applies to full hypotheses because it compares scores\nin the normalized space, which is only available when a hypothesis ends. This latter form of pruning also has\nthe e\ufb00ect that very quickly no more hypotheses will be generated once a su\ufb03ciently good hypothesis has\nbeen found, so the search will end quickly. The pruning speeds up search by 30%\u221240%when run on CPUs\ncompared to not pruning (where we simply stop decoding after a predetermined maximum output length of\ntwice the source length). Typically we use beamsize = 3.0, unless otherwise noted.\nTo improve throughput during decoding we can put many sentences (typically up to 35) of similar length\ninto a batch and decode all of those in parallel to make use of available hardware optimized for parallel\ncomputations. In this case the beam search only \ufb01nishes if all hypotheses for all sentences in the batch are out\nof beam, which is slightly less e\ufb03cient theoretically, but in practice is of negligible additional computational\ncost.\n\u03b1\nBLEU 0.00.20.40.60.81.0\n0.030.330.730.931.131.231.1\n0.231.431.431.431.330.830.3\n\u03b20.431.431.431.431.130.529.6\n0.631.431.431.330.930.128.9\n0.831.431.431.230.829.828.1\n1.031.431.331.230.629.427.2\nTable 2: WMT\u201914 En \u2192Fr BLEU score with respect to di\ufb00erent values of \u03b1and\u03b2. The model in this\nexperiment trained using ML without RL re\ufb01nement. A single WMT En \u2192Fr model achieves a BLEU score\nof 30.3 on the development set when the beam search scoring function is purely based on the sequence\nprobability (i.e., both \u03b1and\u03b2are0). Slightly larger \u03b1and\u03b2values improve BLEU score by up to +1.1\n(\u03b1= 0.2,\u03b2= 0.2), with a wide range of \u03b1and\u03b2values giving results very close to the best BLEU scores.\nTable 2 shows the impact of \u03b1and\u03b2on the BLEU score when decoding the WMT\u201914 English-to-French\ndevelopment set. The model used here for experiments is trained using the ML objective only (without\nRL re\ufb01nement). As can be seen from the results, having some length normalization and coverage penalty\nimproves BLEU score considerably (from 30.3 to 31.4).\nWe \ufb01nd that length normalization ( \u03b1) and coverage penalty ( \u03b2) are less e\ufb00ective for models with RL\nre\ufb01nement. Table 3 summarizes our results. This is understandable, as during RL re\ufb01nement, the models\nalready learn to pay attention to the full source sentence to not under-translate or over-translate, which\nwould result in a penalty on the BLEU (or GLEU) scores.\n\u03b1\nBLEU 0.00.20.40.60.81.0\n0.00.3200.3210.3220.3220.3220.322\n0.20.3220.3220.3220.3220.3210.321\n\u03b20.40.3220.3220.3220.3210.3210.316\n0.60.3220.3220.3210.3210.3190.309\n0.80.3220.3220.3210.3210.3160.302\n1.00.3220.3210.3210.3200.3130.295\nTable 3: WMT En \u2192Fr BLEU score with respect to di\ufb00erent values of \u03b1and\u03b2. The model used here is\ntrained using ML, then re\ufb01ned with RL. Compared to the results in Table 2, coverage penalty and length\nnormalization appear to be less e\ufb00ective for models after RL-based model re\ufb01nements. Results are obtained\non the development set.\nWe found that the optimal \u03b1and\u03b2vary slightly for di\ufb00erent models. Based on tuning results using\ninternal Google datasets, we use \u03b1= 0.2and\u03b2= 0.2in our experiments, unless noted otherwise.",
        "subsection": []
    },
    {
        "id": "8",
        "section": "Experiments and results",
        "text": "In this section, we present our experimental results on two publicly available corpora used extensively as\nbenchmarks for Neural Machine Translation systems: WMT\u201914 English-to-French (WMT En \u2192Fr) and\nEnglish-to-German (WMT En \u2192De). On these two datasets, we benchmark GNMT models with word-based,\ncharacter-based, and wordpiece-based vocabularies. We also present the improved accuracy of our models\nafter \ufb01ne-tuning with RL and model ensembling. Our main objective with these datasets is to show the\ncontributions of various components in our implementation, in particular the wordpiece model, RL model\nre\ufb01nement, and model ensembling.\nIn addition to testing on publicly available corpora, we also test GNMT on Google\u2019s translation production\ncorpora, which are two to three decimal orders of magnitudes bigger than the WMT corpora for a given\nlanguage pair. We compare the accuracy of our model against human accuracy and the best Phrase-Based\nMachine Translation (PBMT) production system for Google Translate.\nIn all experiments, our models consist of 8 encoder layers and 8 decoder layers. (Since the bottom encoder\nlayer is actually bi-directional, in total there are 9 logically distinct LSTM passes in the encoder.) The\nattention network is a simple feedforward network with one hidden layer with 1024 nodes. All of the models\nuse 1024 LSTM nodes per encoder and decoder layers.",
        "subsection": []
    },
    {
        "id": "9",
        "section": "Conclusion",
        "text": "In this paper, we describe in detail the implementation of Google\u2019s Neural Machine Translation (GNMT)\nsystem, including all the techniques that are critical to its accuracy, speed, and robustness. On the public\nWMT\u201914 translation benchmark, our system\u2019s translation quality approaches or surpasses all currently\npublished results. More importantly, we also show that our approach carries over to much larger production\ndata sets, which have several orders of magnitude more data, to deliver high quality translations.\nOur key \ufb01ndings are: 1) that wordpiece modeling e\ufb00ectively handles open vocabularies and the challenge\nof morphologically rich languages for translation quality and inference speed, 2) that a combination of model\nand data parallelism can be used to e\ufb03ciently train state-of-the-art sequence-to-sequence NMT models\nin roughly a week, 3) that model quantization drastically accelerates translation inference, allowing the\nuse of these large models in a deployed production environment, and 4) that many additional details like\nlength-normalization, coverage penalties, and similar are essential to making NMT systems work well on real\ndata.\nUsing human-rated side-by-side comparison as a metric, we show that our GNMT system approaches the\naccuracy achieved by average bilingual human translators on some of our test sets. In particular, compared\nto the previous phrase-based production system, this GNMT system delivers roughly a 60% reduction in\ntranslation errors on several popular language pairs.\nAcknowledgements\nWe would like to thank the entire Google Brain Team and Google Translate Team for their foundational\ncontributions to this project.",
        "subsection": []
    },
    {
        "missing": []
    },
    {
        "references": [
            "Abadi, M., Barham, P., Chen, J., Chen, Z., Davis, A., Dean, J., Devin, M., Ghemawat, S., Irving, G., Isard, M., Kudlur, M., Levenberg, J., Monga, R., Moore, S., Murray, D. G., Steiner, B., Tucker, P., Vasudevan, V., Warden, P., Wicke, M., Yu, Y., and Zheng, X. Tensor\ufb02ow: A system for large-scale machine learning. Tech. rep., Google Brain, 2016. arXiv preprint.",
            "Bahdanau, D., Cho, K., and Bengio, Y. Neural machine translation by jointly learning to align and translate. In International Conference on Learning Representations (2015).",
            "Brown, P., Cocke, J., Pietra, S. D., Pietra, V. D., Jelinek, F., Mercer, R., and Roossin, P. A statistical approach to language translation. In Proceedings of the 12th Conference on Computational Linguistics - Volume 1 (Stroudsburg, PA, USA, 1988), COLING \u201988, Association for Computational Linguistics, pp. 71\u201376.",
            "Brown, P. F., Cocke, J., Pietra, S. A. D., Pietra, V. J. D., Jelinek, F., Lafferty, J. D., Mercer, R. L., and Roossin, P. S. A statistical approach to machine translation. Computational linguistics 16 , 2 (1990), 79\u201385.",
            "Brown, P. F., Pietra, V. J. D., Pietra, S. A. D., and Mercer, R. L. The mathematics of statistical machine translation: Parameter estimation. Comput. Linguist. 19 , 2 (June 1993), 263\u2013311.",
            "Buck, C., Heafield, K., and Van Ooyen, B. N-gram counts and language models from the common crawl. In LREC(2014), vol. 2, Citeseer, p. 4.",
            "Cho, K., van Merrienboer, B., G\u00fcl\u00e7ehre, \u00c7., Bougares, F., Schwenk, H., and Bengio, Y.Learning phrase representations using RNN encoder-decoder for statistical machine translation. In Conference on Empirical Methods in Natural Language Processing (2014).",
            "Chrisman, L. Learning recursive distributed representations for holistic computation. Connection Science 3 , 4 (1991), 345\u2013366.",
            "Chung, J., Cho, K., and Bengio, Y. A character-level decoder without explicit segmentation for neural machine translation. arXiv preprint arXiv:1603.06147 (2016).",
            "Chung, J., Cho, K., and Bengio, Y. A character-level decoder without explicit segmentation for neural machine translation. CoRR abs/1603.06147 (2016).",
            "Costa-Juss\u00e0, M. R., and Fonollosa, J. A. R. Character-based neural machine translation. CoRR abs/1603.00810 (2016).",
            "Dean, J., Corrado, G. S., Monga, R., Chen, K., Devin, M., Le, Q. V., Mao, M. Z., Ranzato, M., Senior, A., Tucker, P., Yang, K., and Ng, A. Y. Large scale distributed deep networks. In NIPS(2012).",
            "Devlin, J., Zbib, R., Huang, Z., Lamar, T., Schwartz, R. M., and Makhoul, J. Fast and robust neural network joint models for statistical machine translation. In ACL (1) (2014), Citeseer, pp. 1370\u20131380.",
            "Dong, D., Wu, H., He, W., Yu, D., and Wang, H. Multi-task learning for multiple language translation. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics (2015), pp. 1723\u20131732.",
            "Durrani, N., Haddow, B., Koehn, P., and Heafield, K. Edinburgh\u2019s phrase-based machine translationsystemsforWMT-14. In Proceedings of the Ninth Workshop on Statistical Machine Translation (2014), Association for Computational Linguistics Baltimore, MD, USA, pp. 97\u2013104.",
            "Fahlman, S. E., and Lebiere, C. The cascade-correlation learning architecture. In Advances in Neural Information Processing Systems 2 (1990), Morgan Kaufmann, pp. 524\u2013532.",
            "Gers, F. A., Schmidhuber, J., and Cummins, F. Learning to forget: Continual prediction with LSTM.Neural computation 12 , 10 (2000), 2451\u20132471.",
            "G\u00fcl\u00e7ehre, \u00c7., Ahn, S., Nallapati, R., Zhou, B., and Bengio, Y. Pointing the unknown words. CoRR abs/1603.08148 (2016).",
            "Gupta, S., Agrawal, A., Gopalakrishnan, K., and Narayanan, P. Deep learning with limited numerical precision. CoRR abs/1502.02551 (2015).",
            "Han, S., Mao, H., and Dally, W. J. Deep compression: Compressing deep neural network with pruning, trained quantization and hu\ufb00man coding. CoRR abs/1510.00149 (2015).",
            "He, K., Zhang, X., Ren, S., and Sun, J. Deep residual learning for image recognition. In IEEE Conference on Computer Vision and Pattern Recognition (2015).",
            "Hochreiter, S., Bengio, Y., Frasconi, P., and Schmidhuber, J. Gradient \ufb02ow in recurrent nets: the di\ufb03culty of learning long-term dependencies, 2001.",
            "Hochreiter, S., and Schmidhuber, J. Long short-term memory. Neural computation 9 , 8 (1997), 1735\u20131780.",
            "Kalchbrenner, N., and Blunsom, P. Recurrent continuous translation models. In Conference on Empirical Methods in Natural Language Processing (2013).",
            "Kingma, D. P., and Ba, J. Adam: A method for stochastic optimization. CoRR abs/1412.6980 (2014).",
            "Koehn, P., Och, F. J., and Marcu, D. Statistical phrase-based translation. In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics (2003).",
            "Li, F., and Liu, B. Ternary weight networks. CoRR abs/1605.04711 (2016).",
            "Luong, M., and Manning, C. D. Achieving open vocabulary neural machine translation with hybrid word-character models. CoRR abs/1604.00788 (2016).",
            "Luong, M.-T., Le, Q. V., Sutskever, I., Vinyals, O., and Kaiser, L. Multi-task sequence to sequence learning. In International Conference on Learning Representations (2015).",
            "Luong, M.-T., Pham, H., and Manning, C. D. E\ufb00ective approaches to attention-based neural machine translation. In Conference on Empirical Methods in Natural Language Processing (2015).",
            "Luong, M.-T., Sutskever, I., Le, Q. V., Vinyals, O., and Zaremba, W. Addressing the rare word problem in neural machine translation. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (2015).",
            "Norouzi, M., Bengio, S., Chen, Z., Jaitly, N., Schuster, M., Wu, Y., and Schuurmans, D.Reward augmented maximum likelihood for neural structured prediction. In Neural Information Processing Systems (2016).",
            "Pascanu, R., Mikolov, T., and Bengio, Y. Understanding the exploding gradient problem. CoRR abs/1211.5063 (2012).",
            "Ranzato, M., Chopra, S., Auli, M., and Zaremba, W. Sequence level training with recurrent neural networks. In International Conference on Learning Representations (2015).",
            "Schuster, M., and Nakajima, K. Japanese and Korean voice search. 2012 IEEE International Conference on Acoustics, Speech and Signal Processing (2012).",
            "Schuster, M., and Paliwal, K. Bidirectional recurrent neural networks. IEEE Transactions on Signal Processing 45 , 11 (Nov. 1997), 2673\u20132681.",
            "S\u00e9bastien, J., Kyunghyun, C., Memisevic, R., and Bengio, Y. On using very large target vocabulary for neural machine translation. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (2015).",
            "Sennrich, R., Haddow, B., and Birch, A. Neural machine translation of rare words with subword units. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (2016).",
            "Shen, S., Cheng, Y., He, Z., He, W., Wu, H., Sun, M., and Liu, Y. Minimum risk training for neural machine translation. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (2016).",
            "Srivastava, R. K., Greff, K., and Schmidhuber, J. Highway networks. CoRR abs/1505.00387 (2015).",
            "Sutskever, I., Vinyals, O., and Le, Q. V. Sequence to sequence learning with neural networks. In Advances in Neural Information Processing Systems (2014), pp. 3104\u20133112.",
            "Tu, Z., Lu, Z., Liu, Y., Liu, X., and Li, H. Coverage-based neural machine translation. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (2016).",
            "Wu, J., Leng, C., Wang, Y., Hu, Q., and Cheng, J. Quantized convolutional neural networks for mobile devices. CoRR abs/1512.06473 (2015).",
            "Zaremba, W., Sutskever, I., and Vinyals, O. Recurrent neural network regularization, 2014.",
            "Zhou, J., Cao, Y., Wang, X., Li, P., and Xu, W. Deep recurrent models with fast-forward connections for neural machine translation. CoRR abs/1606.04199 (2016)."
        ]
    },
    {
        "title": "Google's Neural Machine Translation System: Bridging the Gap between\n  Human and Machine Translation",
        "arxiv_id": "1609.08144"
    }
]