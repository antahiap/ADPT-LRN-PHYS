[
    {
        "id": "",
        "section": "Abstract",
        "text": "We present a novel neural network for process-\ning sequences. The ByteNet is a one-dimensional\nconvolutional neural network that is composed of\ntwo parts, one to encode the source sequence and\nthe other to decode the target sequence. The two\nnetwork parts are connected by stacking the de-\ncoder on top of the encoder and preserving the\ntemporal resolution of the sequences. To ad-\ndress the differing lengths of the source and the\ntarget, we introduce an ef\ufb01cient mechanism by\nwhich the decoder is dynamically unfolded over\nthe representation of the encoder. The ByteNet\nuses dilation in the convolutional layers to in-\ncrease its receptive \ufb01eld. The resulting network\nhas two core properties: it runs in time that\nis linear in the length of the sequences and it\nsidesteps the need for excessive memorization.\nThe ByteNet decoder attains state-of-the-art per-\nformance on character-level language modelling\nand outperforms the previous best results ob-\ntained with recurrent networks. The ByteNet\nalso achieves state-of-the-art performance on\ncharacter-to-character machine translation on the\nEnglish-to-German WMT translation task, sur-\npassing comparable neural translation models\nthat are based on recurrent networks with atten-\ntional pooling and run in quadratic time. We\n\ufb01nd that the latent alignment structure contained\nin the representations re\ufb02ects the expected align-\nment between the tokens.",
        "subsection": []
    },
    {
        "id": "1.",
        "section": "Introduction",
        "text": "In neural language modelling, a neural network estimates\na distribution over sequences of words or characters that\nbelong to a given language (Bengio et al., 2003). In neu-\nral machine translation, the network estimates a distribu-\ntion over sequences in the target language conditioned on\na given sequence in the source language. The network can\nbe thought of as composed of two parts: a source network\n(the encoder) that encodes the source sequence into a rep-\nresentation and a target network (the decoder) that uses the\nt0t1t2t3t4t5t6t7t8t9t11t12t13t14t15t16t10\ns0s1s2s3s4s5s6s7s8s9s10s11s12s13s14s15s16t11t12t13t14t15t16t17t10t9t8t7t6t5t4t3t2t1t0t1t2t3t4t5t6t7t8t9t11t12t13t14t15t16t10\ns0s1s2s3s4s5s6s7s8s9s10s11s12s13s14s15s16t11t12t13t14t15t16t17t10t9t8t7t6t5t4t3t2t1Figure 1. The architecture of the ByteNet. The target decoder\n(blue) is stacked on top of the source encoder (red). The decoder\ngenerates the variable-length target sequence using dynamic un-\nfolding.\nrepresentation of the source encoder to generate the target\nsequence (Kalchbrenner & Blunsom, 2013).\nRecurrent neural networks (RNN) are powerful sequence\nmodels (Hochreiter & Schmidhuber, 1997) and are widely\nused in language modelling (Mikolov et al., 2010), yet they\nhave a potential drawback. RNNs have an inherently se-\nrial structure that prevents them from being run in parallel\nalong the sequence length during training and evaluation.\nForward and backward signals in a RNN also need to tra-\nverse the full distance of the serial path to reach from one\ntoken in the sequence to another. The larger the distance,\nthe harder it is to learn the dependencies between the tokens\n(Hochreiter et al., 2001).\nA number of neural architectures have been proposed\nfor modelling translation, such as encoder-decoder net-\nworks (Kalchbrenner & Blunsom, 2013; Sutskever et al.,\n2014; Cho et al., 2014; Kaiser & Bengio, 2016), networks\nwith attentional pooling (Bahdanau et al., 2014) and two-\ndimensional networks (Kalchbrenner et al., 2016a). De-\nNeural Machine Translation in Linear Time\nEOSEOSEOS|s||t||\u02c6t|EOSEOSEOS|s||t||\u02c6t|\nFigure 2. Dynamic unfolding in the ByteNet architecture. At each step the decoder is conditioned on the source representation produced\nby the encoder for that step, or simply on no representation for steps beyond the extended length j^tj. The decoding ends when the target\nnetwork produces an end-of-sequence (EOS) symbol.\neither have running time that is super-linear in the length of\nthe source and target sequences, or they process the source\nsequence into a constant size representation, burdening the\nmodel with a memorization step. Both of these drawbacks\ngrow more severe as the length of the sequences increases.\nWe present a family of encoder-decoder neural networks\nthat are characterized by two architectural mechanisms\naimed to address the drawbacks of the conventional ap-\nproaches mentioned above. The \ufb01rst mechanism involves\nthestacking of the decoder on top of the representation of\nthe encoder in a manner that preserves the temporal res-\nolution of the sequences; this is in contrast with architec-\ntures that encode the source into a \ufb01xed-size representation\n(Kalchbrenner & Blunsom, 2013; Sutskever et al., 2014).\nThe second mechanism is the dynamic unfolding mecha-\nnism that allows the network to process in a simple and ef-\n\ufb01cient way source and target sequences of different lengths\n(Sect. 3.2).\nThe ByteNet is the instance within this family of models\nthat uses one-dimensional convolutional neural networks\n(CNN) of \ufb01xed depth for both the encoder and the decoder\n(Fig. 1). The two CNNs use increasing factors of dilation to\nrapidly grow the receptive \ufb01elds; a similar technique is also\nused in (van den Oord et al., 2016a). The convolutions in\nthe decoder CNN are masked to prevent the network from\nseeing future tokens in the target sequence (van den Oord\net al., 2016b).\nThe network has bene\ufb01cial computational and learning\nproperties. From a computational perspective, the network\nhas a running time that is linear in the length of the source\nand target sequences (up to a constant c\u0019logdwhere\ndis the size of the desired dependency \ufb01eld). The com-\nputation in the encoder during training and decoding and\nin the decoder during training can also be run ef\ufb01ciently\nin parallel along the sequences (Sect. 2). From a learn-\ning perspective, the representation of the source sequence\nin the ByteNet is resolution preserving ; the representation\nsidesteps the need for memorization and allows for maxi-\nmal bandwidth between encoder and decoder. In addition,\nthe distance traversed by forward and backward signals be-\ntween any input and output tokens corresponds to the \ufb01xed\ndepth of the networks and is largely independent of the dis-tance between the tokens. Dependencies over large dis-\ntances are connected by short paths and can be learnt more\neasily.\nWe apply the ByteNet model to strings of characters\nfor character-level language modelling and character-to-\ncharacter machine translation. We evaluate the decoder\nnetwork on the Hutter Prize Wikipedia task (Hutter,\n2012) where it achieves the state-of-the-art performance\nof 1.31 bits/character. We further evaluate the encoder-\ndecoder network on character-to-character machine trans-\nlation on the English-to-German WMT benchmark where\nit achieves a state-of-the-art BLEU score of 22.85 (0.380\nbits/character) and 25.53 (0.389 bits/character) on the 2014\nand 2015 test sets, respectively. On the character-level ma-\nchine translation task, ByteNet betters a comparable ver-\nsion of GNMT (Wu et al., 2016a) that is a state-of-the-art\nsystem. These results show that deep CNNs are simple,\nscalable and effective architectures for challenging linguis-\ntic processing tasks.\nThe paper is organized as follows. Section 2 lays out the\nbackground and some desiderata for neural architectures\nunderlying translation models. Section 3 de\ufb01nes the pro-\nposed family of architectures and the speci\ufb01c convolutional\ninstance (ByteNet) used in the experiments. Section 4 anal-\nyses ByteNet as well as existing neural translation models\nbased on the desiderata set out in Section 2. Section 5 re-\nports the experiments on language modelling and Section 6\nreports the experiments on character-to-character machine\ntranslation.",
        "subsection": [
            {
                "id": "2.1.",
                "section": "Desiderata",
                "text": "Beyond these basic properties the de\ufb01nition of a neural\ntranslation model does not determine a unique neural ar-\nchitecture, so we aim at identifying some desiderata.\nFirst, the running time of the network should be linear in\nthe length of the source and target strings. This ensures\nthat the model is scalable to longer strings, which is the\ncase when using characters as tokens.\nThe use of operations that run in parallel along the se-\nquence length can also be bene\ufb01cial for reducing compu-\ntation time.\nSecond, the size of the source representation should be lin-\near in the length of the source string, i.e. it should be res-\nolution preserving , and not have constant size. This is to\navoid burdening the model with an additional memoriza-\ntion step before translation. In more general terms, the size\nof a representation should be proportional to the amount of\ninformation it represents or predicts.\nThird, the path traversed by forward and backward signals\nin the network (between input and ouput tokens) should be\nshort. Shorter paths whose length is largely decoupled from\nthe sequence distance between the two tokens have the po-\ntential to better propagate the signals (Hochreiter et al.,\n2001) and to let the network learn long-range dependencies\nmore easily.",
                "subsection": []
            }
        ]
    },
    {
        "id": "2.",
        "section": "Neural translation model",
        "text": "Given a string sfrom a source language, a neural transla-\ntion model estimates a distribution p(tjs)over strings tof\na target language. The distribution indicates the probability\nof a string tbeing a translation of s. A product of condi-\ntionals over the tokens in the target t=t0; :::; t Nleads to\na tractable formulation of the distribution:\np(tjs) =NY\ni=0p(tijt<i;s)Neural Machine Translation in Linear Time\nEach conditional factor expresses complex and long-range\ndependencies among the source and target tokens. The\nstrings are usually sentences of the respective languages;\nthe tokens are words or, as in the our case, characters.\nThe network that models p(tjs)is composed of two parts:\na source network (the encoder) that processes the source\nstring into a representation and a target network (the de-\ncoder) that uses the source representation to generate the\ntarget string (Kalchbrenner & Blunsom, 2013). The de-\ncoder functions as a language model for the target lan-\nguage.\nA neural translation model has some basic properties. The\ndecoder is autoregressive in the target tokens and the model\nis sensitive to the ordering of the tokens in the source and\ntarget strings. It is also useful for the model to be able\nto assign a non-zero probability to any string in the target\nlanguage and retain an open vocabulary.",
        "subsection": [
            {
                "id": "3.1.",
                "section": "Encoder-decoder stacking",
                "text": "A notable feature of the proposed family of architectures\nis the way the encoder and the decoder are connected. To\nmaximize the representational bandwidth between the en-\ncoder and the decoder, we place the decoder on top of\nthe representation computed by the encoder. This is in\ncontrast to models that compress the source representation\ninto a \ufb01xed-size vector (Kalchbrenner & Blunsom, 2013;\nSutskever et al., 2014) or that pool over the source rep-\nresentation with a mechanism such as attentional pooling\n(Bahdanau et al., 2014).",
                "subsection": []
            },
            {
                "id": "3.2.",
                "section": "Dynamic unfolding",
                "text": "An encoder and a decoder network that process sequences\nof different lengths cannot be directly connected due to the\ndifferent sizes of the computed representations. We cir-\ncumvent this issue via a mechanism which we call dynamic\nunfolding, which works as follows.\nGiven source and target sequences sand twith respective\nlengthsjsjandjtj, one \ufb01rst chooses a suf\ufb01ciently tight up-\nper bound^jtjon the target length jtjas a linear function of\nthe source lengthjsj:\n^jtj=ajsj+bNeural Machine Translation in Linear Time\ns0s1s2s3s4s5t0t1t2t3t4t5t1t2t3t4t5t6s0s1s2s3s4s5t0t1t2t3t4t5t1t2t3t4t5t6\ns0s1s2s3s4s5t0t1t2t3t4t5t1t2t3t4t5t6s0s1s2s3s4s5t0t1t2t3t4t5t1t2t3t4t5t6\nFigure 4. Recurrent ByteNet variants of the ByteNet architecture.\nLeft: Recurrent ByteNet with convolutional source network and\nrecurrent target network. Right: Recurrent ByteNet with bidirec-\ntional recurrent source network and recurrent target network. The\nlatter architecture is a strict generalization of the RNN Enc-Dec\nnetwork.\nThe tight upper bound^jtjis chosen in such a way that, on\nthe one hand, it is greater than the actual length jtjin almost\nall cases and, on the other hand, it does not increase exces-\nsively the amount of computation that is required. Once\na linear relationship is chosen, one designs the source en-\ncoder so that, given a source sequence of length jsj, the\nencoder outputs a representation of the established length\n^jtj. In our case, we let a= 1:20andb= 0when translating\nfrom English into German, as German sentences tend to be\nsomewhat longer than their English counterparts (Fig. 5).\nIn this manner the representation produced by the encoder\ncan be ef\ufb01ciently computed, while maintaining high band-\nwidth and being resolution-preserving. Once the encoder\nrepresentation is computed, we let the decoder unfold step-\nby-step over the encoder representation until the decoder it-\nself outputs an end-of-sequence symbol; the unfolding pro-\ncess may freely proceed beyond the estimated length^jtjof\nthe encoder representation. Figure 2 gives an example of\ndynamic unfolding.",
                "subsection": []
            },
            {
                "id": "3.3.",
                "section": "Input embedding tensor",
                "text": "Given the target sequence t=t0; :::; t nthe ByteNet de-\ncoder embeds each of the \ufb01rst ntokens t0; :::; t n\u00001via a\nlook-up table (the ntokens t1; :::; t nserve as targets for the\npredictions). The resulting embeddings are concatenated\ninto a tensor of size n\u00022dwhere dis the number of inner\nchannels in the network.",
                "subsection": []
            },
            {
                "id": "3.4.",
                "section": "Masked one-dimensional convolutions",
                "text": "The decoder applies masked one-dimensional convolutions\n(van den Oord et al., 2016b) to the input embedding ten-\nsor that have a masked kernel of size k. The masking en-\nsures that information from future tokens does not affect\nthe prediction of the current token. The operation can be\nimplemented either by zeroing out some of the weights of\na wider kernel of size 2k\u00001or by padding the input map.\n0 100 200 300 400 500\nGerman01002003004005000 100 200 300 400 500\nGerman0100200300400500English\n\u03c1=.968Figure 5. Lengths of sentences in characters and their correlation\ncoef\ufb01cient for the English-to-German WMT NewsTest-2013 val-\nidation data. The correlation coef\ufb01cient is similarly high ( \u001a >\n0:96) for all other language pairs that we inspected.",
                "subsection": []
            },
            {
                "id": "3.5.",
                "section": "Dilation",
                "text": "The masked convolutions use dilation to increase the re-\nceptive \ufb01eld of the target network (Chen et al., 2014; Yu\n& Koltun, 2015). Dilation makes the receptive \ufb01eld grow\nexponentially in terms of the depth of the networks, as op-\nposed to linearly. We use a dilation scheme whereby the di-\nlation rates are doubled every layer up to a maximum rate r\n(for our experiments r= 16 ). The scheme is repeated mul-\ntiple times in the network always starting from a dilation\nrate of 1 (van den Oord et al., 2016a; Kalchbrenner et al.,\n2016b).",
                "subsection": []
            },
            {
                "id": "3.6.",
                "section": "Residual blocks",
                "text": "Each layer is wrapped in a residual block that contains\nadditional convolutional layers with \ufb01lters of size 1\u00021\n(He et al., 2016). We adopt two variants of the residual\nblocks: one with ReLUs, which is used in the machine\ntranslation experiments, and one with Multiplicative Units\n(Kalchbrenner et al., 2016b), which is used in the language\nmodelling experiments. Figure 3 diagrams the two vari-\nants of the blocks. In both cases, we use layer normal-\nization (Ba et al., 2016) before the activation function, as\nit is well suited to sequence processing where computing\nthe activation statistics over the following future tokens (as\nwould be done by batch normalization) must be avoided.\nAfter a series of residual blocks of increased dilation, the\nnetwork applies one more convolution and ReLU followed\nby a convolution and a \ufb01nal softmax layer.",
                "subsection": []
            }
        ]
    },
    {
        "id": "3.",
        "section": "Bytenet",
        "text": "We aim at building neural language and translation mod-\nels that capture the desiderata set out in Sect. 2.1. The\nproposed ByteNet architecture is composed of a de-\ncoder that is stacked on an encoder (Sect. 3.1) and\ngenerates variable-length outputs via dynamic unfolding\nLayer-Norm\nReLUReLU2d\n1\u21e511\u21e51Layer-Norm\nReLUReLU\nMasked 1\u21e5kMasked 1\u21e5kdLayer-Norm\nReLUReLU\n1\u21e511\u21e51\n++2dLayer-Norm\nReLU2d\n1\u21e51Layer-Norm\nReLU\nMasked 1\u21e5kdLayer-Norm\nReLU\n1\u21e51\n+2d\nLayer-Norm\nReLUReLU2d\n1\u21e511\u21e51Layer-Norm\nReLUReLUd\n1\u21e511\u21e51\n++2d\nMasked 1\u21e5kMUMasked 1\u21e5kMU\n1\u21e51M U1\u21e51M ULayer-Norm\nReLU2d\n1\u21e51Layer-Norm\nReLUd\n1\u21e51\n+2d\nMasked 1\u21e5kMU\n1\u21e51M U\n+ddddd\n\u0000\u0000\n\u21e5\u21e5\n\u21e5\u21e5\n++\ntanhtanh\nMasked 1\u21e5kMasked 1\u21e5k\n\u21e5\u21e5\n\u0000\u0000\n\u0000\u0000\ntanhtanhd\nLayer-Norm+ddddd\n\u0000\n\u21e5\n\u21e5\n+\ntanh\nMasked 1\u21e5k\n\u21e5\n\u0000\n\u0000\ntanhd\nLayer-NormFigure 3. Left: Residual block with ReLUs (He et al., 2016)\nadapted for decoders. Right: Residual Multiplicative Block\nadapted for decoders and corresponding expansion of the MU\n(Kalchbrenner et al., 2016b).\n(Sect. 3.2). The decoder is a language model that is formed\nof one-dimensional convolutional layers that are masked\n(Sect. 3.4) and use dilation (Sect. 3.5). The encoder pro-\ncesses the source string into a representation and is formed\nof one-dimensional convolutional layers that use dilation\nbut are notmasked. Figure 1 depicts the two networks and\ntheir combination.",
        "subsection": [
            {
                "id": "4.1.",
                "section": "Recurrent bytenets",
                "text": "The ByteNet is composed of two stacked encoder and de-\ncoder networks where the decoder network dynamically\nadapts to the output length. This way of combining the\nnetworks is not tied to the networks being strictly convolu-\ntional. We may consider two variants of the ByteNet that\nuse recurrent networks for one or both of the networks (see\nFigure 4). The \ufb01rst variant replaces the convolutional de-\ncoder with a recurrent one that is similarly stacked and dy-\nnamically unfolded. The second variant also replaces the\nconvolutional encoder with a recurrent encoder, e.g. a bidi-\nrectional RNN. The target RNN is then placed on top of the\nsource RNN. Considering the latter Recurrent ByteNet, we\ncan see that the RNN Enc-Dec network (Sutskever et al.,\n2014; Cho et al., 2014) is a Recurrent ByteNet where all\nconnections between source and target \u2013 except for the \ufb01rst\none that connects s0andt0\u2013 have been severed. The Re-\ncurrent ByteNet is a generalization of the RNN Enc-Dec\nand, modulo the type of weight-sharing scheme, so is the\nconvolutional ByteNet.",
                "subsection": []
            },
            {
                "id": "4.2.",
                "section": "Comparison of properties",
                "text": "In our comparison we consider the following neural\ntranslation models: the Recurrent Continuous Translation\nModel (RCTM) 1 and 2 (Kalchbrenner & Blunsom, 2013);\nthe RNN Enc-Dec (Sutskever et al., 2014; Cho et al., 2014);\nthe RNN Enc-Dec Att with the attentional pooling mecha-\nnism (Bahdanau et al., 2014) of which there are a few vari-\nations (Luong et al., 2015; Chung et al., 2016a); the Grid\nLSTM translation model (Kalchbrenner et al., 2016a) that\nuses a multi-dimensional architecture; the Extended Neural\nGPU model (Kaiser & Bengio, 2016) that has a convolu-\ntional RNN architecture; the ByteNet and the two Recur-\nrent ByteNet variants.\nOur comparison criteria re\ufb02ect the desiderata set out in\nSect. 2.1. We separate the \ufb01rst (computation time) desider-atum into three columns. The \ufb01rst column indicates the\ntime complexity of the network as a function of the length\nof the sequences and is denoted by Time . The other two\ncolumns NetSandNetTindicate, respectively, whether the\nsource and the target network use a convolutional struc-\nture (CNN) or a recurrent one (RNN); a CNN structure\nhas the advantage that it can be run in parallel along the\nlength of the sequence. The second (resolution preserva-\ntion) desideratum corresponds to the RPcolumn, which\nindicates whether the source representation in the network\nis resolution preserving. Finally, the third desideratum\n(short forward and backward \ufb02ow paths) is re\ufb02ected by two\ncolumns. The PathScolumn corresponds to the length in\nlayer steps of the shortest path between a source token and\nany output target token. Similarly, the PathTcolumn cor-\nresponds to the length of the shortest path between an input\ntarget token and any output target token. Shorter paths lead\nto better forward and backward signal propagation.\nTable 1 summarizes the properties of the models. The\nByteNet, the Recurrent ByteNets and the RNN Enc-Dec\nare the only networks that have linear running time (up\nto the constant c). The RNN Enc-Dec, however, does not\npreserve the source sequence resolution, a feature that ag-\ngravates learning for long sequences such as those that ap-\npear in character-to-character machine translation (Luong\n& Manning, 2016). The RCTM 2, the RNN Enc-Dec Att,\nthe Grid LSTM and the Extended Neural GPU do preserve\nthe resolution, but at a cost of a quadratic running time. The\nByteNet stands out also for its Path properties. The dilated\nstructure of the convolutions connects any two source or\ntarget tokens in the sequences by way of a small number\nof network layers corresponding to the depth of the source\nor target networks. For character sequences where learning\nlong-range dependencies is important, paths that are sub-\nNeural Machine Translation in Linear Time\nModel Inputs Outputs WMT Test \u201914 WMT Test \u201915\nPhrase Based MT (Freitag et al., 2014; Williams et al., 2015) phrases phrases 20:7 24 :0\nRNN Enc-Dec (Luong et al., 2015) words words 11:3\nReverse RNN Enc-Dec (Luong et al., 2015) words words 14:0\nRNN Enc-Dec Att (Zhou et al., 2016) words words 20:6\nRNN Enc-Dec Att (Luong et al., 2015) words words 20:9\nGNMT (RNN Enc-Dec Att) (Wu et al., 2016a) word-pieces word-pieces 24:61\nRNN Enc-Dec Att (Chung et al., 2016b) BPE BPE 19:98 21 :72\nRNN Enc-Dec Att (Chung et al., 2016b) BPE char 21:33 23 :45\nGNMT (RNN Enc-Dec Att) (Wu et al., 2016a) char char 22:62\nByteNet char char 23:75 26 :26\nTable 2. BLEU scores on En-De WMT NewsTest 2014 and 2015 test sets.\nModel Test\nStacked LSTM (Graves, 2013) 1.67\nGF-LSTM (Chung et al., 2015) 1.58\nGrid-LSTM (Kalchbrenner et al., 2016a) 1.47\nLayer-normalized LSTM (Chung et al., 2016a) 1.46\nMI-LSTM (Wu et al., 2016b) 1.44\nRecurrent Memory Array Structures (Rocki, 2016) 1.40\nHM-LSTM (Chung et al., 2016a) 1.40\nLayer Norm HyperLSTM (Ha et al., 2016) 1.38\nLarge Layer Norm HyperLSTM (Ha et al., 2016) 1.34\nRecurrent Highway Networks (Srivastava et al., 2015) 1.32\nByteNet Decoder 1:31\nTable 3. Negative log-likelihood results in bits/byte on the Hutter\nPrize Wikipedia benchmark.",
                "subsection": []
            }
        ]
    },
    {
        "id": "4.",
        "section": "Model comparison",
        "text": "In this section we analyze the properties of various previ-\nously introduced neural translation models as well as the\nByteNet family of models. For the sake of a more com-\nplete analysis, we include two recurrent ByteNet variants\nNeural Machine Translation in Linear Time\nModel Net S NetT Time RP Path S PathT\nRCTM 1 CNN RNN jSjjSj+jTj no jSj j Tj\nRCTM 2 CNN RNN jSjjSj+jTj yes jSj j Tj\nRNN Enc-Dec RNN RNN jSj+jTj no jSj+jTj jTj\nRNN Enc-Dec Att RNN RNN jSjjTj yes 1 jTj\nGrid LSTM RNN RNN jSjjTj yes jSj+jTj jSj+jTj\nExtended Neural GPU cRNN cRNN jSjjSj+jSjjTjyes jSj j Tj\nRecurrent ByteNet RNN RNN jSj+jTj yes max( jSj;jTj) jTj\nRecurrent ByteNet CNN RNN cjSj+jTj yes c jTj\nByteNet CNN CNN cjSj+cjTj yes c c\nTable 1. Properties of various neural translation models.",
        "subsection": []
    },
    {
        "id": "5.",
        "section": "Character prediction",
        "text": "We \ufb01rst evaluate the ByteNet Decoder separately on a\ncharacter-level language modelling benchmark. We use the\nHutter Prize version of the Wikipedia dataset and follow the\nstandard split where the \ufb01rst 90 million bytes are used for\ntraining, the next 5 million bytes are used for validation and\nthe last 5 million bytes are used for testing (Chung et al.,\n2015). The total number of characters in the vocabulary is\n205.\nThe ByteNet Decoder that we use for the result has 30\nresidual blocks split into six sets of \ufb01ve blocks each; for\nthe \ufb01ve blocks in each set the dilation rates are, respec-\ntively, 1;2;4;8and16. The masked kernel has size 3. This\ngives a receptive \ufb01eld of 315 characters. The number of\nhidden units dis 512. For this task we use residual multi-\nplicative blocks (Fig. 3 Right). For the optimization we use\nAdam (Kingma & Ba, 2014) with a learning rate of 0:0003\nand a weight decay term of 0:0001 . We apply dropout to\nthe last ReLU layer before the softmax dropping units with\na probability of 0.1. We do not reduce the learning rate dur-\ning training. At each step we sample a batch of sequences\nof 500 characters each, use the \ufb01rst 100 characters as the\nminimum context and predict the latter 400 characters.WMT Test \u201914 WMT Test \u201915\nBits/character 0.521 0.532\nBLEU 23.75 26.26\nTable 4. Bits/character with respective BLEU score achieved by\nthe ByteNet translation model on the English-to-German WMT\ntranslation task.\nTable 3 lists recent results of various neural sequence\nmodels on the Wikipedia dataset. All the results ex-\ncept for the ByteNet result are obtained using some vari-\nant of the LSTM recurrent neural network (Hochreiter &\nSchmidhuber, 1997). The ByteNet decoder achieves 1.31\nbits/character on the test set.",
        "subsection": []
    },
    {
        "id": "6.",
        "section": "Character-level machine translation",
        "text": "We evaluate the full ByteNet on the WMT English to Ger-\nman translation task. We use NewsTest 2013 for validation\nand NewsTest 2014 and 2015 for testing. The English and\nGerman strings are encoded as sequences of characters; no\nexplicit segmentation into words or morphemes is applied\nto the strings. The outputs of the network are strings of\ncharacters in the target language. We keep 323 characters\nin the German vocabulary and 296 in the English vocabu-\nlary.\nThe ByteNet used in the experiments has 30 residual blocks\nin the encoder and 30 residual blocks in the decoder. As\nin the ByteNet Decoder, the residual blocks are arranged\nin sets of \ufb01ve with corresponding dilation rates of 1;2;4;8\nand16. For this task we use the residual blocks with ReLUs\n(Fig. 3 Left). The number of hidden units dis 800. The\nsize of the kernel in the source network is 3, whereas the\nsize of the masked kernel in the target network is 3. For the\noptimization we use Adam with a learning rate of 0:0003 .\nEach sentence is padded with special characters to the near-\nNeural Machine Translation in Linear Time\nDirector Jon Favreau, who is currently working on Disney\u2019s forthcoming Jungle Book \ufb01lm,\ntold the website Hollywood Reporter: \u201cI think times are changing. \u201d\nRegisseur Jon Favreau, der derzeit an Disneys bald erscheinenden Dschungelbuch-Film arbeitet,\nsagte gegenber der Webseite Hollywood Reporter: \u201cIch glaube, die Zeiten \u00a8andern sich. \u201d\nRegisseur Jon Favreau, der zur Zeit an Disneys kommendem Jungle Book Film arbeitet,\nhat der Website Hollywood Reporter gesagt: \u201cIch denke, die Zeiten \u00a8andern sich\u201d.\nMatt Casaday, 25, a senior at Brigham Young University, says he had paid 42 cents on Amazon.com\nfor a used copy of \u201cStrategic Media Decisions: Understanding The Business End Of The Advertising Business. \u201d\nMatt Casaday, 25, Abschlussstudent an der Brigham Young University, sagt, dass er auf Amazon.com 42 Cents ausgegeben hat\nf\u00a8ur eine gebrauchte Ausgabe von \u201cStrategic Media Decisions: Understanding The Business End Of The Advertising Business. \u201d\nMatt Casaday, 25, ein Senior an der Brigham Young University, sagte, er habe 42 Cent auf Amazon.com\nf\u00a8ur eine gebrauchte Kopie von \u201cStrategic Media Decisions: Understanding The Business End Of The Advertising Business\u201d.\nTable 5. Raw output translations generated from the ByteNet that highlight interesting reordering and transliteration phenomena. For\neach group, the \ufb01rst row is the English source, the second row is the ground truth German target, and the third row is the ByteNet\ntranslation.\nplied to each source sentence as a part of dynamic unfold-\ning (eq. 2). Each pair of sentences is mapped to a bucket\nbased on the pair of padded lengths for ef\ufb01cient batching\nduring training. We use vanilla beam search according to\nthe total likelihood of the generated candidate and accept\nonly candidates which end in a end-of-sentence token. We\nuse a beam of size 12. We do not use length normalization,\nnor do we keep score of which parts of the source sentence\nhave been translated (Wu et al., 2016a).\nTable 2 and Table 4 contain the results of the experiments.\nOn NewsTest 2014 the ByteNet achieves the highest perfor-\nmance in character-level and subword-level neural machine\ntranslation, and compared to the word-level systems it is\nsecond only to the version of GNMT that uses word-pieces.\nOn NewsTest 2015, to our knowledge, ByteNet achieves\nthe best published results to date.\nTable 5 contains some of the unaltered generated transla-\ntions from the ByteNet that highlight reordering and other\nphenomena such as transliteration. The character-level\naspect of the model makes post-processing unnecessary\nin principle. We further visualize the sensitivity of the\nByteNet\u2019s predictions to speci\ufb01c source and target inputs\nusing gradient-based visualization (Simonyan et al., 2013).\nFigure 6 represents a heatmap of the magnitude of the gra-\ndients of the generated outputs with respect to the source\nand target inputs. For visual clarity, we sum the gradients\nfor all the characters that make up each word and normal-\nize the values along each column. In contrast with the at-\ntentional pooling mechanism (Bahdanau et al., 2014), this\ngeneral technique allows us to inspect not just dependen-\ncies of the outputs on the source inputs, but also dependen-\ncies of the outputs on previous target inputs, or on any other\nneural network layers.7. Conclusion\nWe have introduced the ByteNet, a neural translation model\nthat has linear running time, decouples translation from\nmemorization and has short signal propagation paths for\ntokens in sequences. We have shown that the ByteNet de-\ncoder is a state-of-the-art character-level language model\nbased on a convolutional neural network that outperforms\nrecurrent neural language models. We have also shown that\nthe ByteNet generalizes the RNN Enc-Dec architecture and\nachieves state-of-the-art results for character-to-character\nmachine translation and excellent results in general, while\nmaintaining linear running time complexity. We have re-\nvealed the latent structure learnt by the ByteNet and found\nit to mirror the expected alignment between the tokens in\nthe sentences.",
        "subsection": []
    },
    {
        "missing": []
    },
    {
        "references": []
    },
    {
        "title": "Neural Machine Translation in Linear Time",
        "arxiv_id": "1610.10099"
    }
]