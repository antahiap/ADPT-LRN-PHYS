[
    {
        "id": "",
        "section": "Abstract",
        "text": "We propose a novel deep network structure called \u201c Network InNetwork\u201d(NIN)\nto enhance model discriminability for local patches within the receptive \ufb01eld. The\nconventional convolutional layer uses linear \ufb01lters followed by a nonlinear acti-\nvation function to scan the input. Instead, we build micro neural networks with\nmore complex structures to abstract the data within the receptive \ufb01eld. We in-\nstantiate the micro neural network with a multilayer perceptron, which is a potent\nfunction approximator. The feature maps are obtained by sliding the micro net-\nworks over the input in a similar manner as CNN; they are then fed into the next\nlayer. Deep NIN can be implemented by stacking mutiple of the above described\nstructure. With enhanced local modeling via the micro network, we are able to uti-\nlize global average pooling over feature maps in the classi\ufb01cation layer, which is\neasier to interpret and less prone to over\ufb01tting than traditional fully connected lay-\ners. We demonstrated the state-of-the-art classi\ufb01cation performances with NIN on\nCIFAR-10 and CIFAR-100, and reasonable performances on SVHN and MNIST\ndatasets.",
        "subsection": []
    },
    {
        "id": "1",
        "section": "Introduction",
        "text": "Convolutional neural networks (CNNs) [1] consist of alternating convolutional layers and pooling\nlayers. Convolution layers take inner product of the linear \ufb01lter and the underlying receptive \ufb01eld\nfollowed by a nonlinear activation function at every local portion of the input. The resulting outputs\nare called feature maps.\nThe convolution \ufb01lter in CNN is a generalized linear model (GLM) for the underlying data patch,\nand we argue that the level of abstraction is low with GLM. By abstraction we mean that the fea-\nture is invariant to the variants of the same concept [2]. Replacing the GLM with a more potent\nnonlinear function approximator can enhance the abstraction ability of the local model. GLM can\nachieve a good extent of abstraction when the samples of the latent concepts are linearly separable,\ni.e. the variants of the concepts all live on one side of the separation plane de\ufb01ned by the GLM. Thus\nconventional CNN implicitly makes the assumption that the latent concepts are linearly separable.\nHowever, the data for the same concept often live on a nonlinear manifold, therefore the represen-\ntations that capture these concepts are generally highly nonlinear function of the input. In NIN, the\nGLM is replaced with a \u201dmicro network\u201d structure which is a general nonlinear function approxi-\nmator. In this work, we choose multilayer perceptron [3] as the instantiation of the micro network,\nwhich is a universal function approximator and a neural network trainable by back-propagation.\nThe resulting structure which we call an mlpconv layer is compared with CNN in Figure 1. Both the\nlinear convolutional layer and the mlpconv layer map the local receptive \ufb01eld to an output feature\nvector. The mlpconv maps the input local patch to the output feature vector with a multilayer percep-\ntron (MLP) consisting of multiple fully connected layers with nonlinear activation functions. The\n(a) Linear convolution layer\n . . .\n . . . . . .\n . . . (b) Mlpconv layer\nFigure 1: Comparison of linear convolution layer and mlpconv layer. The linear convolution layer\nincludes a linear \ufb01lter while the mlpconv layer includes a micro network (we choose the multilayer\nperceptron in this paper). Both layers map the local receptive \ufb01eld to a con\ufb01dence value of the latent\nconcept.\nover the input in a similar manner as CNN and are then fed into the next layer. The overall structure\nof the NIN is the stacking of multiple mlpconv layers. It is called \u201cNetwork In Network\u201d (NIN) as\nwe have micro networks (MLP), which are composing elements of the overall deep network, within\nmlpconv layers,\nInstead of adopting the traditional fully connected layers for classi\ufb01cation in CNN, we directly\noutput the spatial average of the feature maps from the last mlpconv layer as the con\ufb01dence of\ncategories via a global average pooling layer, and then the resulting vector is fed into the softmax\nlayer. In traditional CNN, it is dif\ufb01cult to interpret how the category level information from the\nobjective cost layer is passed back to the previous convolution layer due to the fully connected\nlayers which act as a black box in between. In contrast, global average pooling is more meaningful\nand interpretable as it enforces correspondance between feature maps and categories, which is made\npossible by a stronger local modeling using the micro network. Furthermore, the fully connected\nlayers are prone to over\ufb01tting and heavily depend on dropout regularization [4] [5], while global\naverage pooling is itself a structural regularizer, which natively prevents over\ufb01tting for the overall\nstructure.",
        "subsection": []
    },
    {
        "id": "2",
        "section": "Convolutional neural networks",
        "text": "Classic convolutional neuron networks [1] consist of alternatively stacked convolutional layers and\nspatial pooling layers. The convolutional layers generate feature maps by linear convolutional \ufb01lters\nfollowed by nonlinear activation functions (recti\ufb01er, sigmoid, tanh, etc.). Using the linear recti\ufb01er\nas an example, the feature map can be calculated as follows:\nfi;j;k=max(wT\nkxi;j;0): (1)\nHere (i; j)is the pixel index in the feature map, xijstands for the input patch centered at location\n(i; j), andkis used to index the channels of the feature map.\nThis linear convolution is suf\ufb01cient for abstraction when the instances of the latent concepts are\nlinearly separable. However, representations that achieve good abstraction are generally highly non-\nlinear functions of the input data. In conventional CNN, this might be compensated by utilizing\nan over-complete set of \ufb01lters [6] to cover all variations of the latent concepts. Namely, individual\nlinear \ufb01lters can be learned to detect different variations of a same concept. However, having too\nmany \ufb01lters for a single concept imposes extra burden on the next layer, which needs to consider all\ncombinations of variations from the previous layer [7]. As in CNN, \ufb01lters from higher layers map\nto larger regions in the original input. It generates a higher level concept by combining the lower\nlevel concepts from the layer below. Therefore, we argue that it would be bene\ufb01cial to do a better\nabstraction on each local patch, before combining them into higher level concepts.\nIn the recent maxout network [8], the number of feature maps is reduced by maximum pooling\napplying the activation function). Maximization over linear functions makes a piecewise linear\napproximator which is capable of approximating any convex functions. Compared to conventional\nconvolutional layers which perform linear separation, the maxout network is more potent as it can\nseparate concepts that lie within convex sets. This improvement endows the maxout network with\nthe best performances on several benchmark datasets.\nHowever, maxout network imposes the prior that instances of a latent concept lie within a convex set\nin the input space, which does not necessarily hold. It would be necessary to employ a more general\nfunction approximator when the distributions of the latent concepts are more complex. We seek to\nachieve this by introducing the novel \u201cNetwork In Network\u201d structure, in which a micro network is\nintroduced within each convolutional layer to compute more abstract features for local patches.\nSliding a micro network over the input has been proposed in several previous works. For example,\nthe Structured Multilayer Perceptron (SMLP) [9] applies a shared multilayer perceptron on different\npatches of the input image; in another work, a neural network based \ufb01lter is trained for face detection\n[10]. However, they are both designed for speci\ufb01c problems and both contain only one layer of the\nsliding network structure. NIN is proposed from a more general perspective, the micro network is\nintegrated into CNN structure in persuit of better abstractions for all levels of features.",
        "subsection": [
            {
                "id": "3.1",
                "section": "Mlp convolution layers",
                "text": "Given no priors about the distributions of the latent concepts, it is desirable to use a universal func-\ntion approximator for feature extraction of the local patches, as it is capable of approximating more\nabstract representations of the latent concepts. Radial basis network and multilayer perceptron are\ntwo well known universal function approximators. We choose multilayer perceptron in this work\nfor two reasons. First, multilayer perceptron is compatible with the structure of convolutional neural\nnetworks, which is trained using back-propagation. Second, multilayer perceptron can be a deep\nmodel itself, which is consistent with the spirit of feature re-use [2]. This new type of layer is\ncalled mlpconv in this paper, in which MLP replaces the GLM to convolve over the input. Figure\n1 illustrates the difference between linear convolutional layer and mlpconv layer. The calculation\nperformed by mlpconv layer is shown as follows:\nf1\ni;j;k 1=max(w1\nk1Txi;j+bk1;0):\n...\nfn\ni;j;kn=max(wn\nknTfn\u00001\ni;j+bkn;0): (2)\nHere nis the number of layers in the multilayer perceptron. Recti\ufb01ed linear unit is used as the\nactivation function in the multilayer perceptron.\nFrom cross channel (cross feature map) pooling point of view, Equation 2 is equivalent to cas-\ncaded cross channel parametric pooling on a normal convolution layer. Each pooling layer performs\nweighted linear recombination on the input feature maps, which then go through a recti\ufb01er linear\nunit. The cross channel pooled feature maps are cross channel pooled again and again in the next\nlayers. This cascaded cross channel parameteric pooling structure allows complex and learnable\ninteractions of cross channel information.\nThe cross channel parametric pooling layer is also equivalent to a convolution layer with 1x1 con-\n . . .\n . . .\n . . .\n . . .\n . . .\n . . .\n . . .....Figure 2: The overall structure of Network In Network. In this paper the NINs include the stacking\nof three mlpconv layers and one global average pooling layer.\nComparison to maxout layers: the maxout layers in the maxout network performs max pooling\nacross multiple af\ufb01ne feature maps [8]. The feature maps of maxout layers are calculated as follows:\nfi;j;k= max\nm(wT\nkmxi;j): (3)\nMaxout over linear functions forms a piecewise linear function which is capable of modeling any\nconvex function. For a convex function, samples with function values below a speci\ufb01c threshold\nform a convex set. Therefore, by approximating convex functions of the local patch, maxout has\nthe capability of forming separation hyperplanes for concepts whose samples are within a convex\nset (i.e. l2balls, convex cones). Mlpconv layer differs from maxout layer in that the convex func-\ntion approximator is replaced by a universal function approximator, which has greater capability in\nmodeling various distributions of latent concepts.",
                "subsection": []
            },
            {
                "id": "3.2",
                "section": "Global average pooling",
                "text": "Conventional convolutional neural networks perform convolution in the lower layers of the network.\nFor classi\ufb01cation, the feature maps of the last convolutional layer are vectorized and fed into fully\nconnected layers followed by a softmax logistic regression layer [4] [8] [11]. This structure bridges\nthe convolutional structure with traditional neural network classi\ufb01ers. It treats the convolutional\nlayers as feature extractors, and the resulting feature is classi\ufb01ed in a traditional way.\nHowever, the fully connected layers are prone to over\ufb01tting, thus hampering the generalization abil-\nity of the overall network. Dropout is proposed by Hinton et al. [5] as a regularizer which randomly\nsets half of the activations to the fully connected layers to zero during training. It has improved the\ngeneralization ability and largely prevents over\ufb01tting [4].\nIn this paper, we propose another strategy called global average pooling to replace the traditional\nfully connected layers in CNN. The idea is to generate one feature map for each corresponding\ncategory of the classi\ufb01cation task in the last mlpconv layer. Instead of adding fully connected layers\non top of the feature maps, we take the average of each feature map, and the resulting vector is fed\ndirectly into the softmax layer. One advantage of global average pooling over the fully connected\nlayers is that it is more native to the convolution structure by enforcing correspondences between\nfeature maps and categories. Thus the feature maps can be easily interpreted as categories con\ufb01dence\nmaps. Another advantage is that there is no parameter to optimize in the global average pooling\nthus over\ufb01tting is avoided at this layer. Futhermore, global average pooling sums out the spatial\ninformation, thus it is more robust to spatial translations of the input.\nWe can see global average pooling as a structural regularizer that explicitly enforces feature maps to\nbe con\ufb01dence maps of concepts (categories). This is made possible by the mlpconv layers, as they\nmakes better approximation to the con\ufb01dence maps than GLMs.",
                "subsection": []
            },
            {
                "id": "3.3",
                "section": "Network in network structure",
                "text": "The overall structure of NIN is a stack of mlpconv layers, on top of which lie the global average\nlayers as in CNN and maxout networks. Figure 2 shows an NIN with three mlpconv layers. Within\neach mlpconv layer, there is a three-layer perceptron. The number of layers in both NIN and the\nmicro networks is \ufb02exible and can be tuned for speci\ufb01c tasks.",
                "subsection": []
            }
        ]
    },
    {
        "id": "3",
        "section": "Network in network",
        "text": "We \ufb01rst highlight the key components of our proposed \u201cNetwork In Network\u201d structure: the MLP\nconvolutional layer and the global averaging pooling layer in Sec. 3.1 and Sec. 3.2 respectively.\nThen we detail the overall NIN in Sec. 3.3.",
        "subsection": [
            {
                "id": "4.1",
                "section": "Overview",
                "text": "We evaluate NIN on four benchmark datasets: CIFAR-10 [12], CIFAR-100 [12], SVHN [13] and\nMNIST [1]. The networks used for the datasets all consist of three stacked mlpconv layers, and\nthe mlpconv layers in all the experiments are followed by a spatial max pooling layer which down-\nsamples the input image by a factor of two. As a regularizer, dropout is applied on the outputs of all\nbut the last mlpconv layers. Unless stated speci\ufb01cally, all the networks used in the experiment sec-\ntion use global average pooling instead of fully connected layers at the top of the network. Another\nregularizer applied is weight decay as used by Krizhevsky et al. [4]. Figure 2 illustrates the overall\nstructure of NIN network used in this section. The detailed settings of the parameters are provided\nin the supplementary materials. We implement our network on the super fast cuda-convnet code\ndeveloped by Alex Krizhevsky [4]. Preprocessing of the datasets, splitting of training and validation\nsets all follow Goodfellow et al. [8].\nWe adopt the training procedure used by Krizhevsky et al. [4]. Namely, we manually set proper\ninitializations for the weights and the learning rates. The network is trained using mini-batches of\nsize 128. The training process starts from the initial weights and learning rates, and it continues\nuntil the accuracy on the training set stops improving, and then the learning rate is lowered by a\nscale of 10. This procedure is repeated once such that the \ufb01nal learning rate is one percent of the\ninitial value.",
                "subsection": []
            },
            {
                "id": "4.2",
                "section": "Cifar-",
                "text": "The CIFAR-10 dataset [12] is composed of 10 classes of natural images with 50,000 training images\nin total, and 10,000 testing images. Each image is an RGB image of size 32x32. For this dataset, we\napply the same global contrast normalization and ZCA whitening as was used by Goodfellow et al.\nin the maxout network [8]. We use the last 10,000 images of the training set as validation data.\nThe number of feature maps for each mlpconv layer in this experiment is set to the same number\nas in the corresponding maxout network. Two hyper-parameters are tuned using the validation set,\ni.e. the local receptive \ufb01eld size and the weight decay. After that the hyper-parameters are \ufb01xed and\nwe re-train the network from scratch with both the training set and the validation set. The resulting\nmodel is used for testing. We obtain a test error of 10.41% on this dataset, which improves more\nthan one percent compared to the state-of-the-art. A comparison with previous methods is shown in\nTable 1.\nTable 1: Test set error rates for CIFAR-10 of various methods.\nMethod Test Error\nStochastic Pooling [11] 15.13%\nCNN + Spearmint [14] 14.98%\nConv. maxout + Dropout [8] 11.68%\nNIN + Dropout 10.41%\nCNN + Spearmint + Data Augmentation [14] 9.50%\nConv. maxout + Dropout + Data Augmentation [8] 9.38%\nDropConnect + 12 networks + Data Augmentation [15] 9.32%\nNIN + Dropout + Data Augmentation 8.81%\nIt turns out in our experiment that using dropout in between the mlpconv layers in NIN boosts the\nperformance of the network by improving the generalization ability of the model. As is shown\nin Figure 3, introducing dropout layers in between the mlpconv layers reduced the test error by\nin between the mlpconv layers to all the models used in this paper. The model without dropout\nregularizer achieves an error rate of 14.51% for the CIFAR-10 dataset, which already surpasses\nmany previous state-of-the-arts with regularizer (except maxout). Since performance of maxout\nwithout dropout is not available, only dropout regularized version are compared in this paper.\n020 40 60 80100 120 140 160 180 20000.10.20.30.40.50.60.70.80.9\nNumber of epochsError rate\n  \ntraining error w/o dropout\ntraining error w/ dropout\ntesting error w/o dropout\ntesting error w/ dropout020 40 60 80100 120 140 160 180 20000.10.20.30.40.50.60.70.80.9\nNumber of epochsError rate\n  \ntraining error w/o dropout\ntraining error w/ dropout\ntesting error w/o dropout\ntesting error w/ dropout\nFigure 3: The regularization effect of dropout in between mlpconv layers. Training and testing error\nof NIN with and without dropout in the \ufb01rst 200 epochs of training is shown.\nTo be consistent with previous works, we also evaluate our method on the CIFAR-10 dataset with\ntranslation and horizontal \ufb02ipping augmentation. We are able to achieve a test error of 8.81%, which\nsets the new state-of-the-art performance.",
                "subsection": []
            },
            {
                "id": "4.3",
                "section": "Cifar-",
                "text": "The CIFAR-100 dataset [12] is the same in size and format as the CIFAR-10 dataset, but it contains\n100 classes. Thus the number of images in each class is only one tenth of the CIFAR-10 dataset. For\nCIFAR-100 we do not tune the hyper-parameters, but use the same setting as the CIFAR-10 dataset.\nThe only difference is that the last mlpconv layer outputs 100 feature maps. A test error of 35.68%\nis obtained for CIFAR-100 which surpasses the current best performance without data augmentation\nby more than one percent. Details of the performance comparison are shown in Table 2.\nTable 2: Test set error rates for CIFAR-100 of various methods.\nMethod Test Error\nLearned Pooling [16] 43.71%\nStochastic Pooling [11] 42.51%\nConv. maxout + Dropout [8] 38.57%\nTree based priors [17] 36.85%\nNIN + Dropout 35.68%",
                "subsection": []
            },
            {
                "id": "4.4",
                "section": "Street view house numbers",
                "text": "The SVHN dataset [13] is composed of 630,420 32x32 color images, divided into training set,\ntesting set and an extra set. The task of this data set is to classify the digit located at the center of\neach image. The training and testing procedure follow Goodfellow et al. [8]. Namely 400 samples\nper class selected from the training set and 200 samples per class from the extra set are used for\nvalidation. The remainder of the training set and the extra set are used for training. The validation\nset is only used as a guidance for hyper-parameter selection, but never used for training the model.\nPreprocessing of the dataset again follows Goodfellow et al. [8], which was a local contrast normal-\nization. The structure and parameters used in SVHN are similar to those used for CIFAR-10, which\nTable 3: Test set error rates for SVHN of various methods.\nMethod Test Error\nStochastic Pooling [11] 2.80%\nRecti\ufb01er + Dropout [18] 2.78%\nRecti\ufb01er + Dropout + Synthetic Translation [18] 2.68%\nConv. maxout + Dropout [8] 2.47%\nNIN + Dropout 2.35%\nMulti-digit Number Recognition [19] 2.16%\nDropConnect [15] 1.94%\ntest error rate of 2.35%. We compare our result with methods that did not augment the data, and the\ncomparison is shown in Table 3.",
                "subsection": []
            },
            {
                "id": "4.5",
                "section": "Mnist",
                "text": "The MNIST [1] dataset consists of hand written digits 0-9 which are 28x28 in size. There are 60,000\ntraining images and 10,000 testing images in total. For this dataset, the same network structure as\nused for CIFAR-10 is adopted. But the numbers of feature maps generated from each mlpconv layer\nare reduced. Because MNIST is a simpler dataset compared with CIFAR-10; fewer parameters are\nneeded. We test our method on this dataset without data augmentation. The result is compared with\nprevious works that adopted convolutional structures, and are shown in Table 4.\nTable 4: Test set error rates for MNIST of various methods.\nMethod Test Error\n2-Layer CNN + 2-Layer NN [11] 0.53%\nStochastic Pooling [11] 0.47%\nNIN + Dropout 0.47%\nConv. maxout + Dropout [8] 0.45%\nWe achieve comparable but not better performance (0.47%) than the current best (0.45%) since\nMNIST has been tuned to a very low error rate.",
                "subsection": []
            },
            {
                "id": "4.6",
                "section": "Global average pooling as a regularizer",
                "text": "Global average pooling layer is similar to the fully connected layer in that they both perform linear\ntransformations of the vectorized feature maps. The difference lies in the transformation matrix. For\nglobal average pooling, the transformation matrix is pre\ufb01xed and it is non-zero only on block diag-\nonal elements which share the same value. Fully connected layers can have dense transformation\nmatrices and the values are subject to back-propagation optimization. To study the regularization\neffect of global average pooling, we replace the global average pooling layer with a fully connected\nlayer, while the other parts of the model remain the same. We evaluated this model with and without\ndropout before the fully connected linear layer. Both models are tested on the CIFAR-10 dataset,\nand a comparison of the performances is shown in Table 5.\nTable 5: Global average pooling compared to fully connected layer.\nMethod Testing Error\nmlpconv + Fully Connected 11.59%\nmlpconv + Fully Connected + Dropout 10.88%\nmlpconv + Global Average Pooling 10.41%\nAs is shown in Table 5, the fully connected layer without dropout regularization gave the worst\nno regularizer is applied. Adding dropout before the fully connected layer reduced the testing error\n(10.88%). Global average pooling has achieved the lowest testing error (10.41%) among the three.\nWe then explore whether the global average pooling has the same regularization effect for conven-\ntional CNNs. We instantiate a conventional CNN as described by Hinton et al. [5], which consists of\nthree convolutional layers and one local connection layer. The local connection layer generates 16\nfeature maps which are fed to a fully connected layer with dropout. To make the comparison fair, we\nreduce the number of feature map of the local connection layer from 16 to 10, since only one feature\nmap is allowed for each category in the global average pooling scheme. An equivalent network with\nglobal average pooling is then created by replacing the dropout + fully connected layer with global\naverage pooling. The performances were tested on the CIFAR-10 dataset.\nThis CNN model with fully connected layer can only achieve the error rate of 17.56%. When\ndropout is added we achieve a similar performance (15.99%) as reported by Hinton et al. [5]. By\nreplacing the fully connected layer with global average pooling in this model, we obtain the error\nrate of 16.46%, which is one percent improvement compared with the CNN without dropout. It\nagain veri\ufb01es the effectiveness of the global average pooling layer as a regularizer. Although it is\nslightly worse than the dropout regularizer result, we argue that the global average pooling might be\ntoo demanding for linear convolution layers as it requires the linear \ufb01lter with recti\ufb01ed activation to\nmodel the con\ufb01dence maps of the categories.",
                "subsection": []
            },
            {
                "id": "4.7",
                "section": "Visualization of nin",
                "text": "We explicitly enforce feature maps in the last mlpconv layer of NIN to be con\ufb01dence maps of the\ncategories by means of global average pooling, which is possible only with stronger local receptive\n\ufb01eld modeling, e.g. mlpconv in NIN. To understand how much this purpose is accomplished, we\nextract and directly visualize the feature maps from the last mlpconv layer of the trained model for\nCIFAR-10.\nFigure 4 shows some examplar images and their corresponding feature maps for each of the ten\ncategories selected from CIFAR-10 test set. It is expected that the largest activations are observed in\nthe feature map corresponding to the ground truth category of the input image, which is explicitly\nenforced by global average pooling. Within the feature map of the ground truth category, it can be\nobserved that the strongest activations appear roughly at the same region of the object in the original\nimage. It is especially true for structured objects, such as the car in the second row of Figure 4. Note\nthat the feature maps for the categories are trained with only category information. Better results are\nexpected if bounding boxes of the objects are used for \ufb01ne grained labels.\nThe visualization again demonstrates the effectiveness of NIN. It is achieved via a stronger local re-\nceptive \ufb01eld modeling using mlpconv layers. The global average pooling then enforces the learning\nof category level feature maps. Further exploration can be made towards general object detection.\nDetection results can be achieved based on the category level feature maps in the same \ufb02avor as in\nthe scene labeling work of Farabet et al. [20].",
                "subsection": []
            }
        ]
    },
    {
        "id": "4",
        "section": "Experiments",
        "text": "",
        "subsection": []
    },
    {
        "id": "5",
        "section": "Conclusions",
        "text": "We proposed a novel deep network called \u201cNetwork In Network\u201d (NIN) for classi\ufb01cation tasks. This\nnew structure consists of mlpconv layers which use multilayer perceptrons to convolve the input\nand a global average pooling layer as a replacement for the fully connected layers in conventional\nCNN. Mlpconv layers model the local patches better, and global average pooling acts as a structural\nregularizer that prevents over\ufb01tting globally. With these two components of NIN we demonstrated\nstate-of-the-art performance on CIFAR-10, CIFAR-100 and SVHN datasets. Through visualization\nof the feature maps, we demonstrated that feature maps from the last mlpconv layer of NIN were\ncon\ufb01dence maps of the categories, and this motivates the possibility of performing object detection\nvia NIN.",
        "subsection": []
    },
    {
        "missing": []
    },
    {
        "references": [
            " Yann LeCun, L \u00b4eon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recognition.1        2         3         4         5         6        7         8         9       10     1        2         3         4         5         6        7         8         9       10     Figure 4: Visualization of the feature maps from the last mlpconv layer. Only top 10% activations in the feature maps are shown. The categories corresponding to the feature maps are: 1. airplane, 2. automobile, 3. bird, 4. cat, 5. deer, 6. dog, 7. frog, 8. horse, 9. ship, 10. truck. Feature maps corresponding to the ground truth of the input images are highlighted. The left panel and right panel are just different examplars.",
            " Y Bengio, A Courville, and P Vincent. Representation learning: A review and new perspec- tives. IEEE transactions on pattern analysis and machine intelligence , 35:1798\u20131828, 2013.",
            " Frank Rosenblatt. Principles of neurodynamics. perceptrons and the theory of brain mecha- nisms. Technical report, DTIC Document, 1961.",
            " Alex Krizhevsky, Ilya Sutskever, and Geoff Hinton. Imagenet classi\ufb01cation with deep con- volutional neural networks. In Advances in Neural Information Processing Systems 25 , pages 1106\u20131114, 2012.",
            " Geoffrey E Hinton, Nitish Srivastava, Alex Krizhevsky, Ilya Sutskever, and Ruslan R Salakhut- dinov. Improving neural networks by preventing co-adaptation of feature detectors. arXiv preprint arXiv:1207.0580 , 2012.",
            " Quoc V Le, Alexandre Karpenko, Jiquan Ngiam, and Andrew Ng. Ica with reconstruction cost for ef\ufb01cient overcomplete feature learning. In Advances in Neural Information Processing Systems , pages 1017\u20131025, 2011.",
            " Ian J Goodfellow. Piecewise linear multilayer perceptrons and dropout. arXiv preprint arXiv:1301.5088 , 2013.",
            " Ian J Goodfellow, David Warde-Farley, Mehdi Mirza, Aaron Courville, and Yoshua Bengio. Maxout networks. arXiv preprint arXiv:1302.4389 , 2013.",
            " C \u00b8 a \u02d8glar G \u00a8ulc \u00b8ehre and Yoshua Bengio. Knowledge matters: Importance of prior information for optimization. arXiv preprint arXiv:1301.4083 , 2013.",
            " Henry A Rowley, Shumeet Baluja, Takeo Kanade, et al. Human face detection in visual scenes . School of Computer Science, Carnegie Mellon University Pittsburgh, PA, 1995.",
            " Matthew D Zeiler and Rob Fergus. Stochastic pooling for regularization of deep convolutional neural networks. arXiv preprint arXiv:1301.3557 , 2013.",
            " Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. Master\u2019s thesis, Department of Computer Science, University of Toronto , 2009.",
            " Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y Ng. Reading digits in natural images with unsupervised feature learning. In NIPS Workshop on Deep Learning and Unsupervised Feature Learning , volume 2011, 2011.",
            " Jasper Snoek, Hugo Larochelle, and Ryan P Adams. Practical bayesian optimization of ma- chine learning algorithms.[15] Li Wan, Matthew Zeiler, Sixin Zhang, Yann L Cun, and Rob Fergus. Regularization of neural networks using dropconnect. In Proceedings of the 30th International Conference on Machine Learning (ICML-13) , pages 1058\u20131066, 2013.",
            " 9.32% NIN + Dropout + Data Augmentation 8.81% It turns out in our experiment that using dropout in between the mlpconv layers in NIN boosts the performance of the network by improving the generalization ability of the model. As is shown in Figure 3, introducing dropout layers in between the mlpconv layers reduced the test error by in between the mlpconv layers to all the models used in this paper. The model without dropout regularizer achieves an error rate of 14.51% for the CIFAR-10 dataset, which already surpasses many previous state-of-the-arts with regularizer (except maxout). Since performance of maxout without dropout is not available, only dropout regularized version are compared in this paper. 020 40 60 80100 120 140 160 180 20000.10.20.30.40.50.60.70.80.9 Number of epochsError rate    training error w/o dropout training error w/ dropout testing error w/o dropout testing error w/ dropout020 40 60 80100 120 140 160 180 20000.10.20.30.40.50.60.70.80.9 Number of epochsError rate    training error w/o dropout training error w/ dropout testing error w/o dropout testing error w/ dropout Figure 3: The regularization effect of dropout in between mlpconv layers. Training and testing error of NIN with and without dropout in the \ufb01rst 200 epochs of training is shown. To be consistent with previous works, we also evaluate our method on the CIFAR-10 dataset with translation and horizontal \ufb02ipping augmentation. We are able to achieve a test error of 8.81%, which sets the new state-of-the-art performance. 4.3 CIFAR-100 The CIFAR-100 dataset [12] is the same in size and format as the CIFAR-10 dataset, but it contains 100 classes. Thus the number of images in each class is only one tenth of the CIFAR-10 dataset. For CIFAR-100 we do not tune the hyper-parameters, but use the same setting as the CIFAR-10 dataset. The only difference is that the last mlpconv layer outputs 100 feature maps. A test error of 35.68% is obtained for CIFAR-100 which surpasses the current best performance without data augmentation by more than one percent. Details of the performance comparison are shown in Table 2. Table 2: Test set error rates for CIFAR-100 of various methods. Method Test Error Learned Pooling [16] 43.71% Stochastic Pooling [11] 42.51% Conv. maxout + Dropout [8] 38.57% Tree based priors [17] 36.85% NIN + Dropout 35.68% 4.4 Street View House Numbers The SVHN dataset [13] is composed of 630,420 32x32 color images, divided into training set, testing set and an extra set. The task of this data set is to classify the digit located at the center of each image. The training and testing procedure follow Goodfellow et al. [8]. Namely 400 samples per class selected from the training set and 200 samples per class from the extra set are used for validation. The remainder of the training set and the extra set are used for training. The validation set is only used as a guidance for hyper-parameter selection, but never used for training the model. Preprocessing of the dataset again follows Goodfellow et al. [8], which was a local contrast normal- ization. The structure and parameters used in SVHN are similar to those used for CIFAR-10, which Table 3: Test set error rates for SVHN of various methods. Method Test Error Stochastic Pooling [11] 2.80% Recti\ufb01er + Dropout [18] 2.78% Recti\ufb01er + Dropout + Synthetic Translation [18] 2.68% Conv. maxout + Dropout [8] 2.47% NIN + Dropout 2.35% Multi-digit Number Recognition [19] 2.16% DropConnect [15] 1.94% test error rate of 2.35%. We compare our result with methods that did not augment the data, and the comparison is shown in Table 3. 4.5 MNIST The MNIST [1] dataset consists of hand written digits 0-9 which are 28x28 in size. There are 60,000 training images and 10,000 testing images in total. For this dataset, the same network structure as used for CIFAR-10 is adopted. But the numbers of feature maps generated from each mlpconv layer are reduced. Because MNIST is a simpler dataset compared with CIFAR-10; fewer parameters are needed. We test our method on this dataset without data augmentation. The result is compared with previous works that adopted convolutional structures, and are shown in Table 4. Table 4: Test set error rates for MNIST of various methods. Method Test Error 2-Layer CNN + 2-Layer NN [11] 0.53% Stochastic Pooling [11] 0.47% NIN + Dropout 0.47% Conv. maxout + Dropout [8] 0.45% We achieve comparable but not better performance (0.47%) than the current best (0.45%) since MNIST has been tuned to a very low error rate. 4.6 Global Average Pooling as a Regularizer Global average pooling layer is similar to the fully connected layer in that they both perform linear transformations of the vectorized feature maps. The difference lies in the transformation matrix. For global average pooling, the transformation matrix is pre\ufb01xed and it is non-zero only on block diag- onal elements which share the same value. Fully connected layers can have dense transformation matrices and the values are subject to back-propagation optimization. To study the regularization effect of global average pooling, we replace the global average pooling layer with a fully connected layer, while the other parts of the model remain the same. We evaluated this model with and without dropout before the fully connected linear layer. Both models are tested on the CIFAR-10 dataset, and a comparison of the performances is shown in Table 5. Table 5: Global average pooling compared to fully connected layer. Method Testing Error mlpconv + Fully Connected 11.59% mlpconv + Fully Connected + Dropout 10.88% mlpconv + Global Average Pooling 10.41% As is shown in Table 5, the fully connected layer without dropout regularization gave the worst no regularizer is applied. Adding dropout before the fully connected layer reduced the testing error (10.88%). Global average pooling has achieved the lowest testing error (10.41%) among the three. We then explore whether the global average pooling has the same regularization effect for conven- tional CNNs. We instantiate a conventional CNN as described by Hinton et al. [5], which consists of three convolutional layers and one local connection layer. The local connection layer generates 16 feature maps which are fed to a fully connected layer with dropout. To make the comparison fair, we reduce the number of feature map of the local connection layer from 16 to 10, since only one feature map is allowed for each category in the global average pooling scheme. An equivalent network with global average pooling is then created by replacing the dropout + fully connected layer with global average pooling. The performances were tested on the CIFAR-10 dataset. This CNN model with fully connected layer can only achieve the error rate of 17.56%. When dropout is added we achieve a similar performance (15.99%) as reported by Hinton et al. [5]. By replacing the fully connected layer with global average pooling in this model, we obtain the error rate of 16.46%, which is one percent improvement compared with the CNN without dropout. It again veri\ufb01es the effectiveness of the global average pooling layer as a regularizer. Although it is slightly worse than the dropout regularizer result, we argue that the global average pooling might be too demanding for linear convolution layers as it requires the linear \ufb01lter with recti\ufb01ed activation to model the con\ufb01dence maps of the categories. 4.7 Visualization of NIN We explicitly enforce feature maps in the last mlpconv layer of NIN to be con\ufb01dence maps of the categories by means of global average pooling, which is possible only with stronger local receptive \ufb01eld modeling, e.g. mlpconv in NIN. To understand how much this purpose is accomplished, we extract and directly visualize the feature maps from the last mlpconv layer of the trained model for CIFAR-10. Figure 4 shows some examplar images and their corresponding feature maps for each of the ten categories selected from CIFAR-10 test set. It is expected that the largest activations are observed in the feature map corresponding to the ground truth category of the input image, which is explicitly enforced by global average pooling. Within the feature map of the ground truth category, it can be observed that the strongest activations appear roughly at the same region of the object in the original image. It is especially true for structured objects, such as the car in the second row of Figure 4. Note that the feature maps for the categories are trained with only category information. Better results are expected if bounding boxes of the objects are used for \ufb01ne grained labels. The visualization again demonstrates the effectiveness of NIN. It is achieved via a stronger local re- ceptive \ufb01eld modeling using mlpconv layers. The global average pooling then enforces the learning of category level feature maps. Further exploration can be made towards general object detection. Detection results can be achieved based on the category level feature maps in the same \ufb02avor as in the scene labeling work of Farabet et al. [20]. 5 Conclusions We proposed a novel deep network called \u201cNetwork In Network\u201d (NIN) for classi\ufb01cation tasks. This new structure consists of mlpconv layers which use multilayer perceptrons to convolve the input and a global average pooling layer as a replacement for the fully connected layers in conventional CNN. Mlpconv layers model the local patches better, and global average pooling acts as a structural regularizer that prevents over\ufb01tting globally. With these two components of NIN we demonstrated state-of-the-art performance on CIFAR-10, CIFAR-100 and SVHN datasets. Through visualization of the feature maps, we demonstrated that feature maps from the last mlpconv layer of NIN were con\ufb01dence maps of the categories, and this motivates the possibility of performing object detection via NIN. References [1] Yann LeCun, L \u00b4eon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recognition.1        2         3         4         5         6        7         8         9       10     1        2         3         4         5         6        7         8         9       10     Figure 4: Visualization of the feature maps from the last mlpconv layer. Only top 10% activations in the feature maps are shown. The categories corresponding to the feature maps are: 1. airplane, 2. automobile, 3. bird, 4. cat, 5. deer, 6. dog, 7. frog, 8. horse, 9. ship, 10. truck. Feature maps corresponding to the ground truth of the input images are highlighted. The left panel and right panel are just different examplars. [2] Y Bengio, A Courville, and P Vincent. Representation learning: A review and new perspec- tives. IEEE transactions on pattern analysis and machine intelligence , 35:1798\u20131828, 2013. [3] Frank Rosenblatt. Principles of neurodynamics. perceptrons and the theory of brain mecha- nisms. Technical report, DTIC Document, 1961. [4] Alex Krizhevsky, Ilya Sutskever, and Geoff Hinton. Imagenet classi\ufb01cation with deep con- volutional neural networks. In Advances in Neural Information Processing Systems 25 , pages 1106\u20131114, 2012. [5] Geoffrey E Hinton, Nitish Srivastava, Alex Krizhevsky, Ilya Sutskever, and Ruslan R Salakhut- dinov. Improving neural networks by preventing co-adaptation of feature detectors. arXiv preprint arXiv:1207.0580 , 2012. [6] Quoc V Le, Alexandre Karpenko, Jiquan Ngiam, and Andrew Ng. Ica with reconstruction cost for ef\ufb01cient overcomplete feature learning. In Advances in Neural Information Processing Systems , pages 1017\u20131025, 2011. [7] Ian J Goodfellow. Piecewise linear multilayer perceptrons and dropout. arXiv preprint arXiv:1301.5088 , 2013. [8] Ian J Goodfellow, David Warde-Farley, Mehdi Mirza, Aaron Courville, and Yoshua Bengio. Maxout networks. arXiv preprint arXiv:1302.4389 , 2013. [9] C \u00b8 a \u02d8glar G \u00a8ulc \u00b8ehre and Yoshua Bengio. Knowledge matters: Importance of prior information for optimization. arXiv preprint arXiv:1301.4083 , 2013. [10] Henry A Rowley, Shumeet Baluja, Takeo Kanade, et al. Human face detection in visual scenes . School of Computer Science, Carnegie Mellon University Pittsburgh, PA, 1995. [11] Matthew D Zeiler and Rob Fergus. Stochastic pooling for regularization of deep convolutional neural networks. arXiv preprint arXiv:1301.3557 , 2013. [12] Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. Master\u2019s thesis, Department of Computer Science, University of Toronto , 2009. [13] Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y Ng. Reading digits in natural images with unsupervised feature learning. In NIPS Workshop on Deep Learning and Unsupervised Feature Learning , volume 2011, 2011. [14] Jasper Snoek, Hugo Larochelle, and Ryan P Adams. Practical bayesian optimization of ma- chine learning algorithms.[15] Li Wan, Matthew Zeiler, Sixin Zhang, Yann L Cun, and Rob Fergus. Regularization of neural networks using dropconnect. In Proceedings of the 30th International Conference on Machine Learning (ICML-13) , pages 1058\u20131066, 2013.",
            " Mateusz Malinowski and Mario Fritz. Learnable pooling regions for image classi\ufb01cation. arXiv preprint arXiv:1301.3516 , 2013.",
            " Nitish Srivastava and Ruslan Salakhutdinov. Discriminative transfer learning with tree-based priors. In Advances in Neural Information Processing Systems , pages 2094\u20132102, 2013.",
            " Nitish Srivastava. Improving neural networks with dropout . PhD thesis, University of Toronto, 2013.",
            " Ian J Goodfellow, Yaroslav Bulatov, Julian Ibarz, Sacha Arnoud, and Vinay Shet. Multi-digit number recognition from street view imagery using deep convolutional neural networks. arXiv preprint arXiv:1312.6082 , 2013.",
            ""
        ]
    },
    {
        "title": "Network In Network",
        "arxiv_id": "1312.4400"
    }
]