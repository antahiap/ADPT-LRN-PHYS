[
    {
        "id": "",
        "section": "Abstract",
        "text": "In this paper we address the question of how\nto render sequence-level networks better at\nhandling structured input. We propose a ma-\nchine reading simulator which processes text\nincrementally from left to right and performs\nshallow reasoning with memory and atten-\ntion. The reader extends the Long Short-Term\nMemory architecture with a memory network\nin place of a single memory cell. This en-\nables adaptive memory usage during recur-\nrence with neural attention, offering a way to\nweakly induce relations among tokens. The\nsystem is initially designed to process a single\nsequence but we also demonstrate how to inte-\ngrate it with an encoder-decoder architecture.\nExperiments on language modeling, sentiment\nanalysis, and natural language inference show\nthat our model matches or outperforms the\nstate of the art.\n1 Introduction\nHow can a sequence-level network induce relations\nwhich are presumed latent during text processing?\nHow can a recurrent network attentively memorize\nlonger sequences in a way that humans do? In this\npaper we design a machine reader that automatically\nlearns to understand text. The term machine read-\ning is related to a wide range of tasks from answer-\ning reading comprehension questions (Clark et al.,\n2013), to fact and relation extraction (Etzioni et al.,\n2011; Fader et al., 2011), ontology learning (Poon\nand Domingos, 2010), and textual entailment (Da-\ngan et al., 2005). Rather than focusing on a speci\ufb01c\ntask, we develop a general-purpose reading simula-tor, drawing inspiration from human language pro-\ncessing and the fact language comprehension is in-\ncremental with readers continuously extracting the\nmeaning of utterances on a word-by-word basis.\nIn order to understand texts, our machine reader\nshould provide facilities for extracting and repre-\nsenting meaning from natural language text, storing\nmeanings internally, and working with stored mean-\nings to derive further consequences. Ideally, such\na system should be robust, open-domain, and de-\ngrade gracefully in the presence of semantic rep-\nresentations which may be incomplete, inaccurate,\nor incomprehensible. It would also be desirable to\nsimulate the behavior of English speakers who pro-\ncess text sequentially, from left to right, \ufb01xating\nnearly every word while they read (Rayner, 1998)\nand creating partial representations for sentence pre-\n\ufb01xes (Konieczny, 2000; Tanenhaus et al., 1995).\nLanguage modeling tools such as recurrent neural\nnetworks (RNN) bode well with human reading be-\nhavior (Frank and Bod, 2011). RNNs treat each sen-\ntence as a sequence of words and recursively com-\npose each word with its previous memory , until the\nmeaning of the whole sentence has been derived. In\npractice, however, sequence-level networks are met\nwith at least three challenges. The \ufb01rst one concerns\nmodel training problems associated with vanishing\nand exploding gradients (Hochreiter, 1991; Bengio\net al., 1994), which can be partially ameliorated with\ngated activation functions, such as the Long Short-\nTerm Memory (LSTM) (Hochreiter and Schmidhu-\nber, 1997), and gradient clipping (Pascanu et al.,\n2013). The second issue relates to memory com-\npression problems. As the input sequence gets com-\nThe FBI is chasing a criminal on the run .\nThe\nThe FBI is chasing a criminal on the run .\nThe\nThe FBI\nFBI is chasing a criminal on the run .\nThe\nThe FBI\nFBI is\nischasing a criminal on the run .\nThe\nThe FBI\nFBI is\nischasing\nchasing a criminal on the run .\nThe\nThe FBI\nFBI is\nischasing\nchasing a\nacriminal on the run .\nThe\nThe FBI\nFBI is\nischasing\nchasing a\nacriminal\ncriminal on the run .\nThe\nThe FBI\nFBI is\nischasing\nchasing a\nacriminal\ncriminal on\nonthe run .\nThe\nThe FBI\nFBI is\nischasing\nchasing a\nacriminal\ncriminal on\non the\ntherun .\nThe\nThe FBI\nFBI is\nischasing\nchasing a\nacriminal\ncriminal on\non the\nthe run\nrun .\nFigure 1: Illustration of our model while reading the\nsentence The FBI is chasing a criminal on the run .\nColor redrepresents the current word being \ufb01xated,\nblue represents memories. Shading indicates the de-\ngree of memory activation.\n\ufb01ciently large memory capacity is required to store\npast information. As a result, the network general-\nizes poorly to long sequences while wasting memory\non shorter ones. Finally, it should be acknowledged\nthat sequence-level networks lack a mechanism for\nhandling the structure of the input. This imposes\nan inductive bias which is at odds with the fact that\nlanguage has inherent structure. In this paper, we\ndevelop a text processing system which addresses\nthese limitations while maintaining the incremental,\ngenerative property of a recurrent language model.\nRecent attempts to render neural networks more\nstructure aware have seen the incorporation of exter-\nnal memories in the context of recurrent neural net-\nworks (Weston et al., 2015; Sukhbaatar et al., 2015;\nGrefenstette et al., 2015). The idea is to use multiple\nmemory slots outside the recurrence to piece-wise\nstore representations of the input; read and write\noperations for each slot can be modeled as an at-\ntention mechanism with a recurrent controller. We\nalso leverage memory and attention to empower a\nrecurrent network with stronger memorization capa-\nbility and more importantly the ability to discover\nrelations among tokens. This is realized by insert-\ning a memory network module in the update of a re-\ncurrent network together with attention for memory\naddressing. The attention acts as a weak inductive\nmodule discovering relations between input tokens,\nand is trained without direct supervision. As a point\nof departure from previous work, the memory net-\nwork we employ is internal to the recurrence, thus\nstrengthening the interaction of the two and lead-\ning to a representation learner which is able to rea-son over shallow structures. The resulting model,\nwhich we term Long Short-Term Memory-Network\n(LSTMN), is a reading simulator that can be used\nfor sequence processing tasks.\nFigure 1 illustrates the reading behavior of the\nLSTMN. The model processes text incrementally\nwhile learning which past tokens in the memory and\nto what extent they relate to the current token being\nprocessed. As a result, the model induces undirected\nrelations among tokens as an intermediate step of\nlearning representations. We validate the perfor-\nmance of the LSTMN in language modeling, sen-\ntiment analysis, and natural language inference. In\nall cases, we train LSTMN models end-to-end with\ntask-speci\ufb01c supervision signals, achieving perfor-\nmance comparable or better to state-of-the-art mod-\nels and superior to vanilla LSTMs.\n2 Related Work\nOur machine reader is a recurrent neural network ex-\nhibiting two important properties: it is incremental,\nsimulating human behavior, and performs shallow\nstructure reasoning over input streams.\nRecurrent neural network (RNNs) have been suc-\ncessfully applied to various sequence modeling and\nsequence-to-sequence transduction tasks. The latter\nhave assumed several guises in the literature such\nas machine translation (Bahdanau et al., 2014), sen-\ntence compression (Rush et al., 2015), and reading\ncomprehension (Hermann et al., 2015). A key con-\ntributing factor to their success has been the abil-\nity to handle well-known problems with exploding\nor vanishing gradients (Bengio et al., 1994), leading\nto models with gated activation functions (Hochre-\niter and Schmidhuber, 1997; Cho et al., 2014), and\nmore advanced architectures that enhance the in-\nformation \ufb02ow within the network (Koutn \u00b4\u0131k et al.,\n2014; Chung et al., 2015; Yao et al., 2015).\nA remaining practical bottleneck for RNNs is\nmemory compression (Bahdanau et al., 2014): since\nthe inputs are recursively combined into a single\nmemory representation which is typically too small\nin terms of parameters, it becomes dif\ufb01cult to accu-\nrately memorize sequences (Zaremba and Sutskever,\n2014). In the encoder-decoder architecture, this\nproblem can be sidestepped with an attention mech-\nanism which learns soft alignments between the de-\net al., 2014). In our model, memory and attention\nare added within a sequence encoder allowing the\nnetwork to uncover lexical relations between tokens.\nThe idea of introducing a structural bias to neu-\nral models is by no means new. For example, it is\nre\ufb02ected in the work of Socher et al. (2013a) who\napply recursive neural networks for learning natural\nlanguage representations. In the context of recur-\nrent neural networks, efforts to build modular, struc-\ntured neural models date back to Das et al. (1992)\nwho connect a recurrent neural network with an ex-\nternal memory stack for learning context free gram-\nmars. Recently, Weston et al. (2015) propose Mem-\nory Networks to explicitly segregate memory stor-\nage from the computation of neural networks in gen-\neral. Their model is trained end-to-end with a mem-\nory addressing mechanism closely related to soft at-\ntention (Sukhbaatar et al., 2015) and has been ap-\nplied to machine translation (Meng et al., 2015).\nGrefenstette et al. (2015) de\ufb01ne a set of differen-\ntiable data structures (stacks, queues, and dequeues)\nas memories controlled by a recurrent neural net-\nwork. Tran et al. (2016) combine the LSTM with an\nexternal memory block component which interacts\nwith its hidden state. Kumar et al. (2016) employ\na structured neural network with episodic memory\nmodules for natural language and also visual ques-\ntion answering (Xiong et al., 2016).\nSimilar to the above work, we leverage memory\nand attention in a recurrent neural network for induc-\ning relations between tokens as a module in a larger\nnetwork responsible for representation learning. As\na property of soft attention, all intermediate rela-\ntions we aim to capture are soft and differentiable.\nThis is in contrast to shift-reduce type neural mod-\nels (Dyer et al., 2015; Bowman et al., 2016) where\nthe intermediate decisions are hard and induction is\nmore dif\ufb01cult. Finally, note that our model captures\nundirected lexical relations and is thus distinct from\nwork on dependency grammar induction (Klein and\nManning, 2004) where the learned head-modi\ufb01er re-\nlations are directed.\n3 The Machine Reader\nIn this section we present our machine reader which\nis designed to process structured input while retain-\ning the incrementality of a recurrent neural network.\nThe core of our model is a Long Short-Term Mem-ory (LSTM) unit with an extended memory tape that\nexplicitly simulates the human memory span. The\nmodel performs implicit relation analysis between\ntokens with an attention-based memory addressing\nmechanism at every time step. In the following, we\n\ufb01rst review the standard Long Short-Term Memory\nand then describe our model.\n3.1 Long Short-Term Memory\nA Long Short-Term Memory (LSTM) recurrent neu-\nral network processes a variable-length sequence\nx= (x1;x2;\u0001\u0001\u0001;xn)by incrementally adding new\ncontent into a single memory slot, with gates con-\ntrolling the extent to which new content should be\nmemorized, old content should be erased, and cur-\nrent content should be exposed. At time step t, the\nmemory ctand the hidden state htare updated with\nthe following equations:\n2\n664it\nft\not\n\u02c6ct3\n775=2\n664s\ns\ns\ntanh3\n775W\u0001[ht\u00001;xt] (1)\nct=ft\fct\u00001+it\f\u02c6ct (2)\nht=ot\ftanh(ct) (3)\nwhere i,f, and oare gate activations. Compared\nto the standard RNN, the LSTM uses additive mem-\nory updates and it separates the memory cfrom the\nhidden state h, which interacts with the environment\nwhen making predictions.\n3.2 Long Short-Term Memory-Network\nThe \ufb01rst question that arises with LSTMs is the ex-\ntent to which they are able to memorize sequences\nunder recursive compression. LSTMs can produce\na list of state representations during composition,\nhowever, the next state is always computed from the\ncurrent state. That is to say, given the current state\nht, the next state ht+1is conditionally independent of\nstates h1\u0001\u0001\u0001ht\u00001and tokens x1\u0001\u0001\u0001xt. While the recur-\nsive state update is performed in a Markov manner, it\nis assumed that LSTMs maintain unbounded mem-\nory (i.e., the current state alone summarizes well the\ntokens it has seen so far). This assumption may fail\nFigure 2: Long Short-Term Memory-Network.\nColor indicates degree of memory activation.\nor when the memory size is not large enough. An-\nother undesired property of LSTMs concerns model-\ning structured input. An LSTM aggregates informa-\ntion on a token-by-token basis in sequential order,\nbut there is no explicit mechanism for reasoning over\nstructure and modeling relations between tokens.\nOur model aims to address both limitations. Our\nsolution is to modify the standard LSTM structure\nby replacing the memory cell with a memory net-\nwork (Weston et al., 2015). The resulting Long\nShort-Term Memory-Network (LSTMN) stores the\ncontextual representation of each input token with\na unique memory slot and the size of the memory\ngrows with time until an upper bound of the memory\nspan is reached. This design enables the LSTM to\nreason about relations between tokens with a neural\nattention layer and then perform non-Markov state\nupdates. Although it is feasible to apply both write\nand read operations to the memories with attention,\nwe concentrate on the latter. We conceptualize the\nread operation as attentively linking the current to-\nken to previous memories and selecting useful con-\ntent when processing it. Although not the focus of\nthis work, the signi\ufb01cance of the write operation\ncan be analogously justi\ufb01ed as a way of incremen-\ntally updating previous memories, e.g., to correct\nwrong interpretations when processing garden path\nsentences (Ferreira and Henderson, 1991).\nThe architecture of the LSTMN is shown in Fig-\nure 2 and the formal de\ufb01nition is provided as fol-\nlows. The model maintains two sets of vectors\nstored in a hidden state tape used to interact with theenvironment (e.g., computing attention), and a mem-\nory tape used to represent what is actually stored in\nmemory.1Therefore, each token is associated with\na hidden vector and a memory vector. Let xtde-\nnote the current input; Ct\u00001= (c1;\u0001\u0001\u0001;ct\u00001)denotes\nthe current memory tape, and Ht\u00001= (h1;\u0001\u0001\u0001;ht\u00001)\nthe previous hidden tape. At time step t, the model\ncomputes the relation between xtand x1\u0001\u0001\u0001xt\u00001\nthrough h1\u0001\u0001\u0001ht\u00001with an attention layer:\nat\ni=vTtanh(Whhi+Wxxt+W\u02dch\u02dcht\u00001) (4)\nst\ni=softmax (at\ni) (5)\nThis yields a probability distribution over the hidden\nstate vectors of previous tokens. We can then com-\npute an adaptive summary vector for the previous\nhidden tape and memory tape denoted by \u02dc ctand\u02dcht,\nrespectively:\n\u0014\u02dcht\n\u02dcct\u0015\n=t\u00001\n\u00e5\ni=1st\ni\u0001\u0014hi\nci\u0015\n(6)\nand use them for computing the values of ctandht\nin the recurrent update as:\n2\n664it\nft\not\n\u02c6ct3\n775=2\n664s\ns\ns\ntanh3\n775W\u0001[\u02dcht;xt] (7)\nct=ft\f\u02dcct+it\f\u02c6ct (8)\nht=ot\ftanh(ct) (9)\nwhere v,Wh,WxandW\u02dchare the new weight terms of\nthe network.\nA key idea behind the LSTMN is to use attention\nfor inducing relations between tokens. These rela-\ntions are soft and differentiable, and components of\na larger representation learning network. Although\nit is appealing to provide direct supervision for the\nattention layer, e.g., with evidence collected from\na dependency treebank, we treat it as a submod-\nule being optimized within the larger network in a\ndownstream task. It is also possible to have a more\nstructured relational reasoning module by stacking\nmultiple memory and hidden layers in an alternat-\ning fashion, resembling a stacked LSTM (Graves,\n1For comparison, LSTMs maintain a hidden vector and a\nmemory vector; memory networks (Weston et al., 2015) have a\n2013) or a multi-hop memory network (Sukhbaatar\net al., 2015). This can be achieved by feeding the\noutput hk\ntof the lower layer kas input to the upper\nlayer (k+1). The attention at the (k+1)th layer is\ncomputed as:\nat\ni;k+1=vTtanh(Whhk+1\ni+Wlhk\nt+W\u02dch\u02dchk+1\nt\u00001)(10)\nSkip-connections (Graves, 2013) can be applied to\nfeed xtto upper layers as well.\n4 Modeling Two Sequences with LSTMN\nNatural language processing tasks such as machine\ntranslation and textual entailment are concerned\nwith modeling two sequences rather than a single\none. A standard tool for modeling two sequences\nwith recurrent networks is the encoder-decoder ar-\nchitecture where the second sequence (also known\nas the target ) is being processed conditioned on the\n\ufb01rst one (also known as the source ). In this section\nwe explain how to combine the LSTMN which ap-\nplies attention for intra-relation reasoning, with the\nencoder-decoder network whose attention module\nlearns the inter-alignment between two sequences.\nFigures 3a and 3b illustrate two types of combina-\ntion. We describe the models more formally below.\nShallow Attention Fusion Shallow fusion simply\ntreats the LSTMN as a separate module that can\nbe readily used in an encoder-decoder architecture,\nin lieu of a standard RNN or LSTM. As shown in\nFigure 3a, both encoder and decoder are modeled\nas LSTMNs with intra-attention. Meanwhile, inter-\nattention is triggered when the decoder reads a tar-\nget token, similar to the inter-attention introduced in\nBahdanau et al. (2014).\nDeep Attention Fusion Deep fusion combines\ninter- and intra-attention (initiated by the decoder)\nwhen computing state updates. We use different no-\ntation to represent the two sets of attention. Follow-\ning Section 3.2, CandHdenote the target memory\ntape and hidden tape, which store representations of\nthe target symbols that have been processed so far.\nThe computation of intra-attention follows Equa-\ntions (4)\u2013(9). Additionally, we use A= [a1;\u0001\u0001\u0001;am]\nandY= [g1;\u0001\u0001\u0001;gm]to represent the source mem-\nory tape and hidden tape, with mbeing the length of\nthe source sequence conditioned upon. We computeinter-attention between the input at time step tand\ntokens in the entire source sequence as follows:\nbt\nj=uTtanh(Wggj+Wxxt+W\u02dcg\u02dcgt\u00001) (11)\npt\nj=softmax (bt\nj) (12)\nAfter that we compute the adaptive representation of\nthe source memory tape \u02dcatand hidden tape \u02dcgtas:\n\u0014\u02dcgt\n\u02dcat\u0015\n=m\n\u00e5\nj=1pt\nj\u0001\u0014gj\naj\u0015\n(13)\nWe can then transfer the adaptive source represen-\ntation \u02dcatto the target memory with another gating\noperation rt, analogous to the gates in Equation (7).\nrt=s(Wr\u0001[\u02dcgt;xt]) (14)\nThe new target memory includes inter-alignment\nrt\f\u02dcat, intra-relation ft\f\u02dcct, and the new input in-\nformation it\f\u02c6ct:\nct=rt\f\u02dcat+ft\f\u02dcct+it\f\u02c6ct (15)\nht=ot\ftanh(ct) (16)\nAs shown in the equations above and Figure 3b, the\nmajor change of deep fusion lies in the recurrent\nstorage of the inter-alignment vector in the target\nmemory network, as a way to help the target net-\nwork review source information.\n5 Experiments\nIn this section we present our experiments for eval-\nuating the performance of the LSTMN machine\nreader. We start with language modeling as it\nis a natural testbed for our model. We then as-\nsess the model\u2019s ability to extract meaning repre-\nsentations for generic sentence classi\ufb01cation tasks\nsuch as sentiment analysis. Finally, we examine\nwhether the LSTMN can recognize the semantic\nrelationship between two sentences by applying it\nto a natural language inference task. Our code\nis available at https://github.com/cheng6076/\nSNLI-attention(a) Decoder with shallow attention fusion.\n (b) Decoder with deep attention fusion.\nFigure 3: LSTMNs for sequence-to-sequence modeling. The encoder uses intra-attention, while the decoder\nincorporates both intra- and inter-attention. The two \ufb01gures present two ways to combine the intra- and\ninter-attention in the decoder.\nModels Layers Perplexity\nKN5 \u2014 141\nRNN 1 129\nLSTM 1 115\nLSTMN 1 108\nsLSTM 3 115\ngLSTM 3 107\ndLSTM 3 109\nLSTMN 3 102\nTable 1: Language model perplexity on the Penn\nTreebank. The size of memory is 300 for all models.\n5.1 Language Modeling\nOur language modeling experiments were con-\nducted on the English Penn Treebank dataset. Fol-\nlowing common practice (Mikolov et al., 2010), we\ntrained on sections 0\u201320 (1M words), used sec-\ntions 21\u201322 for validation (80K words), and sec-\ntions 23\u201324 (90K words for testing). The dataset\ncontains approximately 1 million tokens and a vo-\ncabulary size of 10K. The average sentence length\nis 21. We use perplexity as our evaluation metric:\nPPL=exp(NLL=T), where NLL denotes the nega-\ntive log likelihood of the entire test set and Tthe\ncorresponding number of tokens. We used stochas-\ntic gradient descent for optimization with an ini-\ntial learning rate of 0.65, which decays by a factor\nof 0.85 per epoch if no signi\ufb01cant improvement has\nbeen observed on the validation set. We renormal-\nize the gradient if its norm is greater than 5. The\nmini-batch size was set to 40. The dimensions ofthe word embeddings were set to 150 for all models.\nIn this suite of experiments we compared the\nLSTMN against a variety of baselines. The \ufb01rst\none is a Kneser-Ney 5-gram language model (KN5)\nwhich generally serves as a non-neural baseline for\nthe language modeling task. We also present per-\nplexity results for the standard RNN and LSTM\nmodels. We also implemented more sophisti-\ncated LSTM architectures, such as a stacked LSTM\n(sLSTM), a gated-feedback LSTM (gLSTM; Chung\net al. (2015)) and a depth-gated LSTM (dLSTM;\nYao et al. (2015)). The gated-feedback LSTM has\nfeedback gates connecting the hidden states across\nmultiple time steps as an adaptive control of the in-\nformation \ufb02ow. The depth-gated LSTM uses a depth\ngate to connect memory cells of vertically adjacent\nlayers. In general, both gLSTM and dLSTM are\nable to capture long-term dependencies to some de-\ngree, but they do not explicitly keep past memories.\nWe set the number of layers to 3 in this experiment,\nmainly to agree with the language modeling exper-\niments of Chung et al. (2015). Also note that that\nthere are no single-layer variants for gLSTM and\ndLSTM; they have to be implemented as multi-layer\nsystems. The hidden unit size of the LSTMN and all\ncomparison models (except KN5) was set to 300.\nThe results of the language modeling task are\nshown in Table 1. Perplexity results for KN5 and\nRNN are taken from Mikolov et al. (2015). As can\nFigure 4: Examples of intra-attention (language\nmodeling). Bold lines indicate higher attention\nscores. Arrows denote which word is being focused\nwhen attention is computed, but not the direction of\nthe relation.\ntwo baselines and the LSTM by a signi\ufb01cant mar-\ngin. Amongst all deep architectures, the three-layer\nLSTMN also performs best. We can study the mem-\nory activation mechanism of the machine reader by\nvisualizing the attention scores. Figure 4 shows\nfour sentences sampled from the Penn Treebank val-\nidation set. Although we explicitly encourage the\nreader to attend to any memory slot, much attention\nfocuses on recent memories. This agrees with the\nlinguistic intuition that long-term dependencies are\nrelatively rare. As illustrated in Figure 4 the model\ncaptures some valid lexical relations (e.g., the de-\npendency between sitsandat,sitsandplays ,every-\none andis,isandwatching ). Note that arcs here\nare undirected and are different from the directed\narcs denoting head-modi\ufb01er relations in dependency\ngraphs.\n5.2 Sentiment Analysis\nOur second task concerns the prediction of senti-\nment labels of sentences. We used the Stanford Sen-\ntiment Treebank (Socher et al., 2013a), which con-\ntains \ufb01ne-grained sentiment labels (very positive,\npositive, neutral, negative, very negative) for 11,855\nsentences. Following previous work on this dataset,\nwe used 8,544 sentences for training, 1,101 for val-\nidation, and 2,210 for testing. The average sentence\nlength is 19.1. In addition, we also performed a bi-\nnary classi\ufb01cation task (positive, negative) after re-\nmoving the neutral label. This resulted in 6,920 sen-Models Fine-grained Binary\nRAE (Socher et al., 2011) 43.2 82.4\nRNTN (Socher et al., 2013b) 45.7 85.4\nDRNN (Irsoy and Cardie, 2014) 49.8 86.6\nDCNN (Blunsom et al., 2014) 48.5 86.8\nCNN-MC (Kim, 2014) 48.0 88.1\nT-CNN (Lei et al., 2015) 51.2 88.6\nPV (Le and Mikolov, 2014) 48.7 87.8\nCT-LSTM (Tai et al., 2015) 51.0 88.0\nLSTM (Tai et al., 2015) 46.4 84.9\n2-layer LSTM (Tai et al., 2015) 46.0 86.3\nLSTMN 47.6 86.3\n2-layer LSTMN 47.9 87.0\nTable 2: Model accuracy (%) on the Sentiment Tree-\nbank (test set). The memory size of LSTMN models\nis set to 168 to be compatible with previously pub-\nlished LSTM variants (Tai et al., 2015).\ntences for training, 872 for validation and 1,821 for\ntesting. Table 2 reports results on both \ufb01ne-grained\nand binary classi\ufb01cation tasks.\nWe experimented with 1- and 2-layer LSTMNs.\nFor the latter model, we predict the sentiment la-\nbel of the sentence based on the averaged hidden\nvector passed to a 2-layer neural network classi\ufb01er\nwith ReLU as the activation function. The mem-\nory size for both LSTMN models was set to 168 to\nbe compatible with previous LSTM models (Tai et\nal., 2015) applied to the same task. We used pre-\ntrained 300-D Glove 840B vectors (Pennington et\nal., 2014) to initialize the word embeddings. The\ngradient for words with Glove embeddings, was\nscaled by 0.35 in the \ufb01rst epoch after which all word\nembeddings were updated normally.\nWe used Adam (Kingma and Ba, 2015) for op-\ntimization with the two momentum parameters set\nto 0.9 and 0.999 respectively. The initial learning\nrate was set to 2E-3. The regularization constant was\n1E-4 and the mini-batch size was 5. A dropout rate\nof 0.5 was applied to the neural network classi\ufb01er.\nWe compared our model with a wide range of top-\nperforming systems. Most of these models (includ-\ning ours) are LSTM variants (third block in Table 2),\nrecursive neural networks (\ufb01rst block), or convolu-\ntional neural networks (CNNs; second block). Re-\ncursive models assume the input sentences are rep-\nresented as parse trees and can take advantage of\nannotations at the phrase level. LSTM-type models\nFigure 5: Examples of intra-attention (sentiment\nanalysis). Bold lines (red) indicate attention be-\ntween sentiment important words.\nexception of CT-LSTM (Tai et al., 2015) which op-\nerates over tree-structured network topologies such\nas constituent trees. For comparison, we also report\nthe performance of the paragraph vector model (PV;\nLe and Mikolov (2014); see Table 2, second block)\nwhich neither operates on trees nor sequences but\nlearns distributed document representations param-\neterized directly.\nThe results in Table 2 show that both 1- and\n2-layer LSTMNs outperform the LSTM baselines\nwhile achieving numbers comparable to state of the\nart. The number of layers for our models was set to\nbe comparable to previously published results. On\nthe \ufb01ne-grained and binary classi\ufb01cation tasks our\n2-layer LSTMN performs close to the best system\nT-CNN (Lei et al., 2015). Figure 5 shows examples\nof intra-attention for sentiment words. Interestingly,\nthe network learns to associate sentiment important\nwords such as though andfantastic ornotandgood .\n5.3 Natural Language Inference\nThe ability to reason about the semantic relation-\nship between two sentences is an integral part of\ntext understanding. We therefore evaluate our model\non recognizing textual entailment, i.e., whether two\npremise-hypothesis pairs are entailing, contradic-\ntory, or neutral. For this task we used the Stan-\nford Natural Language Inference (SNLI) dataset\n(Bowman et al., 2015), which contains premise-\nhypothesis pairs and target labels indicating their\nrelation. After removing sentences with unknown\nlabels, we end up with 549,367 pairs for training,\n9,842 for development and 9,824 for testing. The\nvocabulary size is 36,809 and the average sentence\nlength is 22. We performed lower-casing and tok-\nenization for the entire dataset.Recent approaches use two sequential LSTMs to\nencode the premise and the hypothesis respectively,\nand apply neural attention to reason about their logi-\ncal relationship (Rockt \u00a8aschel et al., 2016; Wang and\nJiang, 2016). Furthermore, Rockt \u00a8aschel et al. (2016)\nshow that a non-standard encoder-decoder architec-\nture which processes the hypothesis conditioned on\nthe premise results signi\ufb01cantly boosts performance.\nWe use a similar approach to tackle this task with\nLSTMNs. Speci\ufb01cally, we use two LSTMNs to read\nthe premise and hypothesis, and then match them\nby comparing their hidden state tapes. We perform\naverage pooling for the hidden state tape of each\nLSTMN, and concatenate the two averages to form\nthe input to a 2-layer neural network classi\ufb01er with\nReLU as the activation function.\nWe used pre-trained 300-D Glove 840B vectors\n(Pennington et al., 2014) to initialize the word em-\nbeddings. Out-of-vocabulary (OOV) words were\ninitialized randomly with Gaussian samples ( \u00b5=0,\ns=1). We only updated OOV vectors in the \ufb01rst\nepoch, after which all word embeddings were up-\ndated normally. The dropout rate was selected from\n[0.1, 0.2, 0.3, 0.4]. We used Adam (Kingma and Ba,\n2015) for optimization with the two momentum pa-\nrameters set to 0.9 and 0.999 respectively, and the\ninitial learning rate set to 1E-3. The mini-batch size\nwas set to 16 or 32. For a fair comparison against\nprevious work, we report results with different hid-\nden/memory dimensions (i.e., 100, 300, and 450).\nWe compared variants of our model against dif-\nferent types of LSTMs (see the second block in Ta-\nble 3). Speci\ufb01cally, these include a model which\nencodes the premise and hypothesis independently\nwith two LSTMs (Bowman et al., 2015), a shared\nLSTM (Rockt \u00a8aschel et al., 2016), a word-by-word\nattention model (Rockt \u00a8aschel et al., 2016), and a\nmatching LSTM (mLSTM; Wang and Jiang (2016)).\nThis model sequentially processes the hypothesis,\nand at each position tries to match the current word\nwith an attention-weighted representation of the\npremise (rather than basing its predictions on whole\nsentence embeddings). We also compared our mod-\nels with a bag-of-words baseline which averages the\npre-trained embeddings for the words in each sen-\ntence and concatenates them to create features for a\nlogistic regression classi\ufb01er (\ufb01rst block in Table 3).\nModels hjqjM Test\nBOW concatenation \u2014 \u2014 59.8\nLSTM (Bowman et al., 2015) 100 221k 77.6\nLSTM-att (Rockt \u00a8aschel et al., 2016) 100 252k 83.5\nmLSTM (Wang and Jiang, 2016) 300 1.9M 86.1\nLSTMN 100 260k 81.5\nLSTMN shallow fusion 100 280k 84.3\nLSTMN deep fusion 100 330k 84.5\nLSTMN shallow fusion 300 1.4M 85.2\nLSTMN deep fusion 300 1.7M 85.7\nLSTMN shallow fusion 450 2.8M 86.0\nLSTMN deep fusion 450 3.4M 86.3\nTable 3: Parameter counts jqjM, size of hidden\nunith, and model accuracy (%) on the natural lan-\nguage inference task.\nto LSTMs (with and without attention; 2nd block\nin Table 3). We also observe that fusion is gen-\nerally bene\ufb01cial, and that deep fusion slightly im-\nproves over shallow fusion. One explanation is that\nwith deep fusion the inter-attention vectors are re-\ncurrently memorized by the decoder with a gating\noperation, which also improves the information \ufb02ow\nof the network. With standard training, our deep fu-\nsion yields the state-of-the-art performance in this\ntask. Although encouraging, this result should be in-\nterpreted with caution since our model has substan-\ntially more parameters compared to related systems.\nWe could compare different models using the same\nnumber of total parameters. However, this would in-\nevitably introduce other biases, e.g., the number of\nhyper-parameters would become different.\n6 Conclusions\nIn this paper we proposed a machine reading simula-\ntor to address the limitations of recurrent neural net-\nworks when processing inherently structured input.\nOur model is based on a Long Short-Term Mem-\nory architecture embedded with a memory network,\nexplicitly storing contextual representations of in-\nput tokens without recursively compressing them.\nMore importantly, an intra-attention mechanism is\nemployed for memory addressing, as a way to in-\nduce undirected relations among tokens. The at-\ntention layer is not optimized with a direct super-\nvision signal but with the entire network in down-\nstream tasks. Experimental results across three tasks\nshow that our model yields performance comparableor superior to state of the art.\nAlthough our experiments focused on LSTMs, the\nidea of building more structure aware neural models\nis general and can be applied to other types of net-\nworks. When direct supervision is provided, simi-\nlar architectures can be adapted to tasks such as de-\npendency parsing and relation extraction. In the fu-\nture, we hope to develop more linguistically plausi-\nble neural architectures able to reason over nested\nstructures and neural models that learn to discover\ncompositionality with weak or indirect supervision.\nAcknowledgments\nWe thank members of the ILCC at the School of\nInformatics and the anonymous reviewers for help-\nful comments. The support of the European Re-\nsearch Council under award number 681760 \u201cTrans-\nlating Multiple Modalities into Text\u201d is gratefully\nacknowledged.\nReferences\n[Andreas et al.2016] Jacob Andreas, Marcus Rohrbach,\nTrevor Darrell, and Dan Klein. 2016. Learning to\ncompose neural networks for question answering. In\nProceedings of the 2016 NAACL: HLT , pages 1545\u2013\n1554, San Diego, California.\n[Bahdanau et al.2014] Dzmitry Bahdanau, Kyunghyun\nCho, and Yoshua Bengio. 2014. Neural machine\ntranslation by jointly learning to align and translate.\nInProceedings of the 2014 ICLR , Banff, Alberta.\n[Bengio et al.1994] Yoshua Bengio, Patrice Simard, and\nPaolo Frasconi. 1994. Learning long-term depen-\ndencies with gradient descent is dif\ufb01cult. Neural Net-\nworks, IEEE Transactions on , 5(2):157\u2013166.\n[Blunsom et al.2014] Phil Blunsom, Edward Grefenstette,\nand Nal Kalchbrenner. 2014. A convolutional neural\nnetwork for modelling sentences. In Proceedings of\nthe 52nd ACL , pages 655\u2013665, Baltimore, Maryland.\n[Bowman et al.2015] Samuel R Bowman, Gabor Angeli,\nChristopher Potts, and Christopher D Manning. 2015.\nA large annotated corpus for learning natural language\ninference. In Proceedings of the 2015 EMNLP , pages\n22\u201332, Lisbon, Portugal.\n[Bowman et al.2016] Samuel R Bowman, Jon Gauthier,\nAbhinav Rastogi, Raghav Gupta, Christopher D Man-\nning, and Christopher Potts. 2016. A fast uni\ufb01ed\nmodel for parsing and sentence understanding. In Pro-\nceedings of the 54th ACL , pages 1466\u20131477, Berlin,\n[Cho et al.2014] Kyunghyun Cho, Bart Van Merri \u00a8enboer,\nCaglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares,\nHolger Schwenk, and Yoshua Bengio. 2014. Learning\nphrase representations using RNN encoder-decoder\nfor statistical machine translation. In Proceedings of\nthe 2014 EMNLP , pages 1724\u20131734, Doha, Qatar.\n[Chung et al.2015] Junyoung Chung, Caglar Gulcehre,\nKyunghyun Cho, and Yoshua Bengio. 2015. Gated\nfeedback recurrent neural networks. In Proceedings of\nthe 32nd ICML , pages 2067\u20132075, Lille, France.\n[Clark et al.2013] Peter Clark, Phil Harrison, and Niran-\njan Balasubramanian. 2013. A study of the knowl-\nedge base requirements for passing an elementary sci-\nence test. In Proceedings of the 3rd Workshop on Au-\ntomated KB Construction , San Francisco, California.\n[Dagan et al.2005] Ido Dagan, Oren Glickman, and\nBernardo Magnini. 2005. The PASCAL recognising\ntextual entailment challenge. In Proceedings of the\nPASCAL Challenges Workshop on Recognising Tex-\ntual Entailment .\n[Das et al.1992] Sreerupa Das, C. Lee Giles, and Guo\nzheng Sun. 1992. Learning context-free grammars:\nCapabilities and limitations of a recurrent neural net-\nwork with an external stack memory. In Proceedings\nof the 14th Annual Conference of the Cognitive Sci-\nence Society , pages 791\u2013795. Morgan Kaufmann Pub-\nlishers.\n[Dyer et al.2015] Chris Dyer, Miguel Ballesteros, Wang\nLing, Austin Matthews, and Noah A Smith. 2015.\nTransition-based dependency parsing with stack long\nshort-term memory. In Proceedings of the 53rd ACL ,\npages 334\u2013343, Beijing, China.\n[Etzioni et al.2011] Oren Etzioni, Anthony Fader, Janara\nChristensen, Stephen Soderland, and Mausam. 2011.\nOpen information extraction: The second genera-\ntion. In Proceedings of the 22nd IJCAI , pages 3\u201310,\nBarcelona, Spain.\n[Fader et al.2011] Anthony Fader, Stephen Soderland,\nand Oren Etzioni. 2011. Identifying relations for open\ninformation extraction. In Proceedings of the 2011\nEMNLP , pages 1535\u20131545, Edinburgh, Scotland, UK.\n[Ferreira and Henderson1991] Fernanda Ferreira and\nJohn M. Henderson. 1991. Recovery from misanaly-\nses of garden-path sentences. Journal of Memory and\nLanguage , 30:725\u2013745.\n[Frank and Bod2011] Stefan L. Frank and Rens Bod.\n2011. Insensitivity of the human sentence-processing\nsystem to hierarchical structure. Pyschological Sci-\nence, 22(6):829\u2013834.\n[Graves2013] Alex Graves. 2013. Generating sequences\nwith recurrent neural networks. arXiv preprint\narXiv:1308.0850 .\n[Grefenstette et al.2015] Edward Grefenstette,\nKarl Moritz Hermann, Mustafa Suleyman, andPhil Blunsom. 2015. Learning to transduce with un-\nbounded memory. In Advances in Neural Information\nProcessing Systems , pages 1819\u20131827.\n[Hermann et al.2015] Karl Moritz Hermann, Tomas Ko-\ncisky, Edward Grefenstette, Lasse Espeholt, Will Kay,\nMustafa Suleyman, and Phil Blunsom. 2015. Teach-\ning machines to read and comprehend. In Advances in\nNeural Information Processing Systems , pages 1684\u2013\n1692.\n[Hochreiter and Schmidhuber1997] Sepp Hochreiter and\nJ\u00a8urgen Schmidhuber. 1997. Long short-term memory.\nNeural computation , 9(8):1735\u20131780.\n[Hochreiter1991] Sepp Hochreiter. 1991. Untersuchun-\ngen zu dynamischen neuronalen netzen. Diploma,\nTechnische Universit \u00a8at M \u00a8unchen .\n[Irsoy and Cardie2014] Ozan Irsoy and Claire Cardie.\n2014. Deep recursive neural networks for composi-\ntionality in language. In Advances in Neural Informa-\ntion Processing Systems , pages 2096\u20132104.\n[Kim2014] Yoon Kim. 2014. Convolutional neural net-\nworks for sentence classi\ufb01cation. In Proceedings of\nthe 2014 EMNLP , pages 1746\u20131751, Doha, Qatar.\n[Kingma and Ba2015] Diederik Kingma and Jimmy Ba.\n2015. Adam: A method for stochastic optimization.\nInProceedings of the 2015 ICLR , San Diego, Califor-\nnia.\n[Klein and Manning2004] Dan Klein and Christopher\nManning. 2004. Corpus-based induction of syntac-\ntic structure: Models of dependency and constituency.\nInProceedings of the 42nd ACL , pages 478\u2013485,\nBarcelona, Spain.\n[Konieczny2000] Lars Konieczny. 2000. Locality and\nparsing complexity. Journal of Psycholinguistics ,\n29(6):627\u2013645.\n[Koutn \u00b4\u0131k et al.2014] Jan Koutn \u00b4\u0131k, Klaus Greff, Faustino\nGomez, and J \u00a8urgen Schmidhuber. 2014. A clockwork\nRNN. In Proceedings of the 31st ICML , pages 1863\u2013\n1871, Beijing, China.\n[Kumar et al.2016] Ankit Kumar, Ozan Irsoy, Jonathan\nSu, James Bradbury, Robert English, Brian Pierce, Pe-\nter Ondruska, Ishaan Gulrajani, and Richard Socher.\n2016. Ask me anything: Dynamic memory networks\nfor natural language processing. In Proceedings of the\n33rd ICML , New York, NY .\n[Le and Mikolov2014] Quoc V Le and Tomas Mikolov.\n2014. Distributed representations of sentences and\ndocuments. In Proceedings of the 31st ICML , pages\n1188\u20131196, Beijing, China.\n[Lei et al.2015] Tao Lei, Regina Barzilay, and Tommi\nJaakkola. 2015. Molding cnns for text: non-linear,\nnon-consecutive convolutions. In Proceedings of the\n2015 EMNLP[Meng et al.2015] Fandong Meng, Zhengdong Lu,\nZhaopeng Tu, Hang Li, and Qun Liu. 2015. A deep\nmemory-based architecture for sequence-to-sequence\nlearning. In Proceedings of ICLR-Workshop 2016 ,\nSan Juan, Puerto Rico.\n[Mikolov et al.2010] Tomas Mikolov, Martin Kara\ufb01 \u00b4at,\nLukas Burget, Jan Cernock `y, and Sanjeev Khudan-\npur. 2010. Recurrent neural network based language\nmodel. In Proceedings of 11th Interspeech , pages\n1045\u20131048, Makuhari, Japan.\n[Mikolov et al.2015] Tomas Mikolov, Armand Joulin,\nSumit Chopra, Michael Mathieu, and Marc\u2019Aurelio\nRanzato. 2015. Learning longer memory in recurrent\nneural networks. In Proceedings of ICLR Workshop ,\nSan Diego, California.\n[Pascanu et al.2013] Razvan Pascanu, Tomas Mikolov,\nand Yoshua Bengio. 2013. On the dif\ufb01culty of train-\ning recurrent neural networks. In Proceedings of the\n30th ICML , pages 1310\u20131318, Atlanta, Georgia.\n[Pennington et al.2014] Jeffrey Pennington, Richard\nSocher, and Christopher D. Manning. 2014. Glove:\nGlobal vectors for word representation. In Proceed-\nings of the 2014 EMNLP , pages 1532\u20131543, Doha,\nQatar.\n[Poon and Domingos2010] Hoifung Poon and Pedro\nDomingos. 2010. Unsupervised ontology induction\nfrom text. In Proceedings of the 48th Annual Meeting\nof the Association for Computational Linguistics ,\npages 296\u2013305, Uppsala.\n[Rayner1998] Keith Rayner. 1998. Eye movements in\nreading and information processing: 20 years of re-\nsearch. Psychological Bulletin , 124(3):372\u2013422.\n[Rockt \u00a8aschel et al.2016] Tim Rockt \u00a8aschel, Edward\nGrefenstette, Karl Moritz Hermann, Tom \u00b4a\u02c7s Ko \u02c7cisk`y,\nand Phil Blunsom. 2016. Reasoning about entailment\nwith neural attention. In Proceedings of the 2016\nICLR , San Juan, Puerto Rico.\n[Rush et al.2015] Alexander M Rush, Sumit Chopra, and\nJason Weston. 2015. A neural attention model for\nabstractive sentence summarization. In Proceedings of\nthe 2015 EMNLP , pages 379\u2013389, Lisbon, Portugal.\n[Socher et al.2011] Richard Socher, Eric H Huang, Jef-\nfrey Pennin, Christopher D Manning, and Andrew Y\nNg. 2011. Dynamic pooling and unfolding recursive\nautoencoders for paraphrase detection. In Advances in\nNeural Information Processing Systems , pages 801\u2013\n809.\n[Socher et al.2013a] Richard Socher, Alex Perelygin,\nJean Wu, Jason Chuang, Christopher D. Manning, An-\ndrew Ng, and Christopher Potts. 2013a. Recursive\ndeep models for semantic compositionality over a sen-\ntiment treebank. In Proceedings of the 2013 EMNLP ,\npages 1631\u20131642, Seattle, Washington.[Socher et al.2013b] Richard Socher, Alex Perelygin,\nJean Y Wu, Jason Chuang, Christopher D Manning,\nAndrew Y Ng, and Christopher Potts. 2013b. Recur-\nsive deep models for semantic compositionality over\na sentiment treebank. In Proceedings of the 2013\nEMNLP , pages 1631\u20131642, Seattle, Washingtton.\n[Sukhbaatar et al.2015] Sainbayar Sukhbaatar, Jason We-\nston, Rob Fergus, et al. 2015. End-to-end memory\nnetworks. In Advances in Neural Information Process-\ning Systems , pages 2431\u20132439.\n[Tai et al.2015] Kai Sheng Tai, Richard Socher, and\nChristopher D Manning. 2015. Improved semantic\nrepresentations from tree-structured long short-term\nmemory networks. In Proceedings of the 53rd ACL ,\npages 1556\u20131566, Beijing, China.\n[Tanenhaus et al.1995] Michael K. Tanenhaus, Michael J.\nSpivey-Knowlton, Kathleen M. Eberhard, and Julue C.\nSedivy. 1995. Integration of visual and linguistic in-\nformation in spoken language comprehension. Sci-\nence, 268:1632\u20131634.\n[Tran et al.2016] Ke Tran, Arianna Bisazza, and Christof\nMonz. 2016. Recurrent memory network for language\nmodeling. In Proceedings of the 15th NAACL , San\nDiego, CA.\n[Wang and Jiang2016] Shuohang Wang and Jing Jiang.\n2016. Learning natural language inference with lstm.\nInProceedings of the 2016 NAACL: HLT , pages 1442\u2013\n1451, San Diego, California.\n[Weston et al.2015] Jason Weston, Sumit Chopra, and\nAntoine Bordes. 2015. Memory networks. In Pro-\nceedings of the 2015 ICLR , San Diego, USA.\n[Xiong et al.2016] Caiming Xiong, Stephen Merity, and\nRichard Socher. 2016. Dynamic memory networks\nfor visual and textual question answering. In Proceed-\nings of the 33rd ICML , New York, NY .\n[Yao et al.2015] Kaisheng Yao, Trevor Cohn, Katerina\nVylomova, Kevin Duh, and Chris Dyer. 2015.\nDepth-gated recurrent neural networks. arXiv preprint\narXiv:1508.03790 .\n[Zaremba and Sutskever2014] Wojciech Zaremba and\nIlya Sutskever. 2014. Learning to execute. arXiv\npreprint arXiv:1410.4615",
        "subsection": []
    },
    {
        "id": "10",
        "section": "Crichton street",
        "text": "fjianpeng.cheng,li.dong g@ed.ac.uk ,mlap@inf.ed.ac.uk\nAbstract\nIn this paper we address the question of how\nto render sequence-level networks better at\nhandling structured input. We propose a ma-\nchine reading simulator which processes text\nincrementally from left to right and performs\nshallow reasoning with memory and atten-\ntion. The reader extends the Long Short-Term\nMemory architecture with a memory network\nin place of a single memory cell. This en-\nables adaptive memory usage during recur-\nrence with neural attention, offering a way to\nweakly induce relations among tokens. The\nsystem is initially designed to process a single\nsequence but we also demonstrate how to inte-\ngrate it with an encoder-decoder architecture.\nExperiments on language modeling, sentiment\nanalysis, and natural language inference show\nthat our model matches or outperforms the\nstate of the art.",
        "subsection": []
    },
    {
        "id": "1",
        "section": "Introduction",
        "text": "How can a sequence-level network induce relations\nwhich are presumed latent during text processing?\nHow can a recurrent network attentively memorize\nlonger sequences in a way that humans do? In this\npaper we design a machine reader that automatically\nlearns to understand text. The term machine read-\ning is related to a wide range of tasks from answer-\ning reading comprehension questions (Clark et al.,\n2013), to fact and relation extraction (Etzioni et al.,\n2011; Fader et al., 2011), ontology learning (Poon\nand Domingos, 2010), and textual entailment (Da-\ngan et al., 2005). Rather than focusing on a speci\ufb01c\ntask, we develop a general-purpose reading simula-tor, drawing inspiration from human language pro-\ncessing and the fact language comprehension is in-\ncremental with readers continuously extracting the\nmeaning of utterances on a word-by-word basis.\nIn order to understand texts, our machine reader\nshould provide facilities for extracting and repre-\nsenting meaning from natural language text, storing\nmeanings internally, and working with stored mean-\nings to derive further consequences. Ideally, such\na system should be robust, open-domain, and de-\ngrade gracefully in the presence of semantic rep-\nresentations which may be incomplete, inaccurate,\nor incomprehensible. It would also be desirable to\nsimulate the behavior of English speakers who pro-\ncess text sequentially, from left to right, \ufb01xating\nnearly every word while they read (Rayner, 1998)\nand creating partial representations for sentence pre-\n\ufb01xes (Konieczny, 2000; Tanenhaus et al., 1995).\nLanguage modeling tools such as recurrent neural\nnetworks (RNN) bode well with human reading be-\nhavior (Frank and Bod, 2011). RNNs treat each sen-\ntence as a sequence of words and recursively com-\npose each word with its previous memory , until the\nmeaning of the whole sentence has been derived. In\npractice, however, sequence-level networks are met\nwith at least three challenges. The \ufb01rst one concerns\nmodel training problems associated with vanishing\nand exploding gradients (Hochreiter, 1991; Bengio\net al., 1994), which can be partially ameliorated with\ngated activation functions, such as the Long Short-\nTerm Memory (LSTM) (Hochreiter and Schmidhu-\nber, 1997), and gradient clipping (Pascanu et al.,\n2013). The second issue relates to memory com-\npression problems. As the input sequence gets com-\nThe FBI is chasing a criminal on the run .\nThe\nThe FBI is chasing a criminal on the run .\nThe\nThe FBI\nFBI is chasing a criminal on the run .\nThe\nThe FBI\nFBI is\nischasing a criminal on the run .\nThe\nThe FBI\nFBI is\nischasing\nchasing a criminal on the run .\nThe\nThe FBI\nFBI is\nischasing\nchasing a\nacriminal on the run .\nThe\nThe FBI\nFBI is\nischasing\nchasing a\nacriminal\ncriminal on the run .\nThe\nThe FBI\nFBI is\nischasing\nchasing a\nacriminal\ncriminal on\nonthe run .\nThe\nThe FBI\nFBI is\nischasing\nchasing a\nacriminal\ncriminal on\non the\ntherun .\nThe\nThe FBI\nFBI is\nischasing\nchasing a\nacriminal\ncriminal on\non the\nthe run\nrun .\nFigure 1: Illustration of our model while reading the\nsentence The FBI is chasing a criminal on the run .\nColor redrepresents the current word being \ufb01xated,\nblue represents memories. Shading indicates the de-\ngree of memory activation.\n\ufb01ciently large memory capacity is required to store\npast information. As a result, the network general-\nizes poorly to long sequences while wasting memory\non shorter ones. Finally, it should be acknowledged\nthat sequence-level networks lack a mechanism for\nhandling the structure of the input. This imposes\nan inductive bias which is at odds with the fact that\nlanguage has inherent structure. In this paper, we\ndevelop a text processing system which addresses\nthese limitations while maintaining the incremental,\ngenerative property of a recurrent language model.\nRecent attempts to render neural networks more\nstructure aware have seen the incorporation of exter-\nnal memories in the context of recurrent neural net-\nworks (Weston et al., 2015; Sukhbaatar et al., 2015;\nGrefenstette et al., 2015). The idea is to use multiple\nmemory slots outside the recurrence to piece-wise\nstore representations of the input; read and write\noperations for each slot can be modeled as an at-\ntention mechanism with a recurrent controller. We\nalso leverage memory and attention to empower a\nrecurrent network with stronger memorization capa-\nbility and more importantly the ability to discover\nrelations among tokens. This is realized by insert-\ning a memory network module in the update of a re-\ncurrent network together with attention for memory\naddressing. The attention acts as a weak inductive\nmodule discovering relations between input tokens,\nand is trained without direct supervision. As a point\nof departure from previous work, the memory net-\nwork we employ is internal to the recurrence, thus\nstrengthening the interaction of the two and lead-\ning to a representation learner which is able to rea-son over shallow structures. The resulting model,\nwhich we term Long Short-Term Memory-Network\n(LSTMN), is a reading simulator that can be used\nfor sequence processing tasks.\nFigure 1 illustrates the reading behavior of the\nLSTMN. The model processes text incrementally\nwhile learning which past tokens in the memory and\nto what extent they relate to the current token being\nprocessed. As a result, the model induces undirected\nrelations among tokens as an intermediate step of\nlearning representations. We validate the perfor-\nmance of the LSTMN in language modeling, sen-\ntiment analysis, and natural language inference. In\nall cases, we train LSTMN models end-to-end with\ntask-speci\ufb01c supervision signals, achieving perfor-\nmance comparable or better to state-of-the-art mod-\nels and superior to vanilla LSTMs.",
        "subsection": [
            {
                "id": "3.1",
                "section": "Long short-term memory",
                "text": "A Long Short-Term Memory (LSTM) recurrent neu-\nral network processes a variable-length sequence\nx= (x1;x2;\u0001\u0001\u0001;xn)by incrementally adding new\ncontent into a single memory slot, with gates con-\ntrolling the extent to which new content should be\nmemorized, old content should be erased, and cur-\nrent content should be exposed. At time step t, the\nmemory ctand the hidden state htare updated with\nthe following equations:\n2\n664it\nft\not\n\u02c6ct3\n775=2\n664s\ns\ns\ntanh3\n775W\u0001[ht\u00001;xt] (1)\nct=ft\fct\u00001+it\f\u02c6ct (2)\nht=ot\ftanh(ct) (3)\nwhere i,f, and oare gate activations. Compared\nto the standard RNN, the LSTM uses additive mem-\nory updates and it separates the memory cfrom the\nhidden state h, which interacts with the environment\nwhen making predictions.",
                "subsection": []
            },
            {
                "id": "3.2",
                "section": "Long short-term memory-network",
                "text": "The \ufb01rst question that arises with LSTMs is the ex-\ntent to which they are able to memorize sequences\nunder recursive compression. LSTMs can produce\na list of state representations during composition,\nhowever, the next state is always computed from the\ncurrent state. That is to say, given the current state\nht, the next state ht+1is conditionally independent of\nstates h1\u0001\u0001\u0001ht\u00001and tokens x1\u0001\u0001\u0001xt. While the recur-\nsive state update is performed in a Markov manner, it\nis assumed that LSTMs maintain unbounded mem-\nory (i.e., the current state alone summarizes well the\ntokens it has seen so far). This assumption may fail\nFigure 2: Long Short-Term Memory-Network.\nColor indicates degree of memory activation.\nor when the memory size is not large enough. An-\nother undesired property of LSTMs concerns model-\ning structured input. An LSTM aggregates informa-\ntion on a token-by-token basis in sequential order,\nbut there is no explicit mechanism for reasoning over\nstructure and modeling relations between tokens.\nOur model aims to address both limitations. Our\nsolution is to modify the standard LSTM structure\nby replacing the memory cell with a memory net-\nwork (Weston et al., 2015). The resulting Long\nShort-Term Memory-Network (LSTMN) stores the\ncontextual representation of each input token with\na unique memory slot and the size of the memory\ngrows with time until an upper bound of the memory\nspan is reached. This design enables the LSTM to\nreason about relations between tokens with a neural\nattention layer and then perform non-Markov state\nupdates. Although it is feasible to apply both write\nand read operations to the memories with attention,\nwe concentrate on the latter. We conceptualize the\nread operation as attentively linking the current to-\nken to previous memories and selecting useful con-\ntent when processing it. Although not the focus of\nthis work, the signi\ufb01cance of the write operation\ncan be analogously justi\ufb01ed as a way of incremen-\ntally updating previous memories, e.g., to correct\nwrong interpretations when processing garden path\nsentences (Ferreira and Henderson, 1991).\nThe architecture of the LSTMN is shown in Fig-\nure 2 and the formal de\ufb01nition is provided as fol-\nlows. The model maintains two sets of vectors\nstored in a hidden state tape used to interact with theenvironment (e.g., computing attention), and a mem-\nory tape used to represent what is actually stored in\nmemory.1Therefore, each token is associated with\na hidden vector and a memory vector. Let xtde-\nnote the current input; Ct\u00001= (c1;\u0001\u0001\u0001;ct\u00001)denotes\nthe current memory tape, and Ht\u00001= (h1;\u0001\u0001\u0001;ht\u00001)\nthe previous hidden tape. At time step t, the model\ncomputes the relation between xtand x1\u0001\u0001\u0001xt\u00001\nthrough h1\u0001\u0001\u0001ht\u00001with an attention layer:\nat\ni=vTtanh(Whhi+Wxxt+W\u02dch\u02dcht\u00001) (4)\nst\ni=softmax (at\ni) (5)\nThis yields a probability distribution over the hidden\nstate vectors of previous tokens. We can then com-\npute an adaptive summary vector for the previous\nhidden tape and memory tape denoted by \u02dc ctand\u02dcht,\nrespectively:\n\u0014\u02dcht\n\u02dcct\u0015\n=t\u00001\n\u00e5\ni=1st\ni\u0001\u0014hi\nci\u0015\n(6)\nand use them for computing the values of ctandht\nin the recurrent update as:\n2\n664it\nft\not\n\u02c6ct3\n775=2\n664s\ns\ns\ntanh3\n775W\u0001[\u02dcht;xt] (7)\nct=ft\f\u02dcct+it\f\u02c6ct (8)\nht=ot\ftanh(ct) (9)\nwhere v,Wh,WxandW\u02dchare the new weight terms of\nthe network.\nA key idea behind the LSTMN is to use attention\nfor inducing relations between tokens. These rela-\ntions are soft and differentiable, and components of\na larger representation learning network. Although\nit is appealing to provide direct supervision for the\nattention layer, e.g., with evidence collected from\na dependency treebank, we treat it as a submod-\nule being optimized within the larger network in a\ndownstream task. It is also possible to have a more\nstructured relational reasoning module by stacking\nmultiple memory and hidden layers in an alternat-\ning fashion, resembling a stacked LSTM (Graves,\n1For comparison, LSTMs maintain a hidden vector and a\nmemory vector; memory networks (Weston et al., 2015) have a\n2013) or a multi-hop memory network (Sukhbaatar\net al., 2015). This can be achieved by feeding the\noutput hk\ntof the lower layer kas input to the upper\nlayer (k+1). The attention at the (k+1)th layer is\ncomputed as:\nat\ni;k+1=vTtanh(Whhk+1\ni+Wlhk\nt+W\u02dch\u02dchk+1\nt\u00001)(10)\nSkip-connections (Graves, 2013) can be applied to\nfeed xtto upper layers as well.",
                "subsection": []
            }
        ]
    },
    {
        "id": "2",
        "section": "Related work",
        "text": "Our machine reader is a recurrent neural network ex-\nhibiting two important properties: it is incremental,\nsimulating human behavior, and performs shallow\nstructure reasoning over input streams.\nRecurrent neural network (RNNs) have been suc-\ncessfully applied to various sequence modeling and\nsequence-to-sequence transduction tasks. The latter\nhave assumed several guises in the literature such\nas machine translation (Bahdanau et al., 2014), sen-\ntence compression (Rush et al., 2015), and reading\ncomprehension (Hermann et al., 2015). A key con-\ntributing factor to their success has been the abil-\nity to handle well-known problems with exploding\nor vanishing gradients (Bengio et al., 1994), leading\nto models with gated activation functions (Hochre-\niter and Schmidhuber, 1997; Cho et al., 2014), and\nmore advanced architectures that enhance the in-\nformation \ufb02ow within the network (Koutn \u00b4\u0131k et al.,\n2014; Chung et al., 2015; Yao et al., 2015).\nA remaining practical bottleneck for RNNs is\nmemory compression (Bahdanau et al., 2014): since\nthe inputs are recursively combined into a single\nmemory representation which is typically too small\nin terms of parameters, it becomes dif\ufb01cult to accu-\nrately memorize sequences (Zaremba and Sutskever,\n2014). In the encoder-decoder architecture, this\nproblem can be sidestepped with an attention mech-\nanism which learns soft alignments between the de-\net al., 2014). In our model, memory and attention\nare added within a sequence encoder allowing the\nnetwork to uncover lexical relations between tokens.\nThe idea of introducing a structural bias to neu-\nral models is by no means new. For example, it is\nre\ufb02ected in the work of Socher et al. (2013a) who\napply recursive neural networks for learning natural\nlanguage representations. In the context of recur-\nrent neural networks, efforts to build modular, struc-\ntured neural models date back to Das et al. (1992)\nwho connect a recurrent neural network with an ex-\nternal memory stack for learning context free gram-\nmars. Recently, Weston et al. (2015) propose Mem-\nory Networks to explicitly segregate memory stor-\nage from the computation of neural networks in gen-\neral. Their model is trained end-to-end with a mem-\nory addressing mechanism closely related to soft at-\ntention (Sukhbaatar et al., 2015) and has been ap-\nplied to machine translation (Meng et al., 2015).\nGrefenstette et al. (2015) de\ufb01ne a set of differen-\ntiable data structures (stacks, queues, and dequeues)\nas memories controlled by a recurrent neural net-\nwork. Tran et al. (2016) combine the LSTM with an\nexternal memory block component which interacts\nwith its hidden state. Kumar et al. (2016) employ\na structured neural network with episodic memory\nmodules for natural language and also visual ques-\ntion answering (Xiong et al., 2016).\nSimilar to the above work, we leverage memory\nand attention in a recurrent neural network for induc-\ning relations between tokens as a module in a larger\nnetwork responsible for representation learning. As\na property of soft attention, all intermediate rela-\ntions we aim to capture are soft and differentiable.\nThis is in contrast to shift-reduce type neural mod-\nels (Dyer et al., 2015; Bowman et al., 2016) where\nthe intermediate decisions are hard and induction is\nmore dif\ufb01cult. Finally, note that our model captures\nundirected lexical relations and is thus distinct from\nwork on dependency grammar induction (Klein and\nManning, 2004) where the learned head-modi\ufb01er re-\nlations are directed.",
        "subsection": []
    },
    {
        "id": "3",
        "section": "The machine reader",
        "text": "In this section we present our machine reader which\nis designed to process structured input while retain-\ning the incrementality of a recurrent neural network.\nThe core of our model is a Long Short-Term Mem-ory (LSTM) unit with an extended memory tape that\nexplicitly simulates the human memory span. The\nmodel performs implicit relation analysis between\ntokens with an attention-based memory addressing\nmechanism at every time step. In the following, we\n\ufb01rst review the standard Long Short-Term Memory\nand then describe our model.",
        "subsection": [
            {
                "id": "5.1",
                "section": "Language modeling",
                "text": "Our language modeling experiments were con-\nducted on the English Penn Treebank dataset. Fol-\nlowing common practice (Mikolov et al., 2010), we\ntrained on sections 0\u201320 (1M words), used sec-\ntions 21\u201322 for validation (80K words), and sec-\ntions 23\u201324 (90K words for testing). The dataset\ncontains approximately 1 million tokens and a vo-\ncabulary size of 10K. The average sentence length\nis 21. We use perplexity as our evaluation metric:\nPPL=exp(NLL=T), where NLL denotes the nega-\ntive log likelihood of the entire test set and Tthe\ncorresponding number of tokens. We used stochas-\ntic gradient descent for optimization with an ini-\ntial learning rate of 0.65, which decays by a factor\nof 0.85 per epoch if no signi\ufb01cant improvement has\nbeen observed on the validation set. We renormal-\nize the gradient if its norm is greater than 5. The\nmini-batch size was set to 40. The dimensions ofthe word embeddings were set to 150 for all models.\nIn this suite of experiments we compared the\nLSTMN against a variety of baselines. The \ufb01rst\none is a Kneser-Ney 5-gram language model (KN5)\nwhich generally serves as a non-neural baseline for\nthe language modeling task. We also present per-\nplexity results for the standard RNN and LSTM\nmodels. We also implemented more sophisti-\ncated LSTM architectures, such as a stacked LSTM\n(sLSTM), a gated-feedback LSTM (gLSTM; Chung\net al. (2015)) and a depth-gated LSTM (dLSTM;\nYao et al. (2015)). The gated-feedback LSTM has\nfeedback gates connecting the hidden states across\nmultiple time steps as an adaptive control of the in-\nformation \ufb02ow. The depth-gated LSTM uses a depth\ngate to connect memory cells of vertically adjacent\nlayers. In general, both gLSTM and dLSTM are\nable to capture long-term dependencies to some de-\ngree, but they do not explicitly keep past memories.\nWe set the number of layers to 3 in this experiment,\nmainly to agree with the language modeling exper-\niments of Chung et al. (2015). Also note that that\nthere are no single-layer variants for gLSTM and\ndLSTM; they have to be implemented as multi-layer\nsystems. The hidden unit size of the LSTMN and all\ncomparison models (except KN5) was set to 300.\nThe results of the language modeling task are\nshown in Table 1. Perplexity results for KN5 and\nRNN are taken from Mikolov et al. (2015). As can\nFigure 4: Examples of intra-attention (language\nmodeling). Bold lines indicate higher attention\nscores. Arrows denote which word is being focused\nwhen attention is computed, but not the direction of\nthe relation.\ntwo baselines and the LSTM by a signi\ufb01cant mar-\ngin. Amongst all deep architectures, the three-layer\nLSTMN also performs best. We can study the mem-\nory activation mechanism of the machine reader by\nvisualizing the attention scores. Figure 4 shows\nfour sentences sampled from the Penn Treebank val-\nidation set. Although we explicitly encourage the\nreader to attend to any memory slot, much attention\nfocuses on recent memories. This agrees with the\nlinguistic intuition that long-term dependencies are\nrelatively rare. As illustrated in Figure 4 the model\ncaptures some valid lexical relations (e.g., the de-\npendency between sitsandat,sitsandplays ,every-\none andis,isandwatching ). Note that arcs here\nare undirected and are different from the directed\narcs denoting head-modi\ufb01er relations in dependency\ngraphs.",
                "subsection": []
            },
            {
                "id": "5.2",
                "section": "Sentiment analysis",
                "text": "Our second task concerns the prediction of senti-\nment labels of sentences. We used the Stanford Sen-\ntiment Treebank (Socher et al., 2013a), which con-\ntains \ufb01ne-grained sentiment labels (very positive,\npositive, neutral, negative, very negative) for 11,855\nsentences. Following previous work on this dataset,\nwe used 8,544 sentences for training, 1,101 for val-\nidation, and 2,210 for testing. The average sentence\nlength is 19.1. In addition, we also performed a bi-\nnary classi\ufb01cation task (positive, negative) after re-\nmoving the neutral label. This resulted in 6,920 sen-Models Fine-grained Binary\nRAE (Socher et al., 2011) 43.2 82.4\nRNTN (Socher et al., 2013b) 45.7 85.4\nDRNN (Irsoy and Cardie, 2014) 49.8 86.6\nDCNN (Blunsom et al., 2014) 48.5 86.8\nCNN-MC (Kim, 2014) 48.0 88.1\nT-CNN (Lei et al., 2015) 51.2 88.6\nPV (Le and Mikolov, 2014) 48.7 87.8\nCT-LSTM (Tai et al., 2015) 51.0 88.0\nLSTM (Tai et al., 2015) 46.4 84.9\n2-layer LSTM (Tai et al., 2015) 46.0 86.3\nLSTMN 47.6 86.3\n2-layer LSTMN 47.9 87.0\nTable 2: Model accuracy (%) on the Sentiment Tree-\nbank (test set). The memory size of LSTMN models\nis set to 168 to be compatible with previously pub-\nlished LSTM variants (Tai et al., 2015).\ntences for training, 872 for validation and 1,821 for\ntesting. Table 2 reports results on both \ufb01ne-grained\nand binary classi\ufb01cation tasks.\nWe experimented with 1- and 2-layer LSTMNs.\nFor the latter model, we predict the sentiment la-\nbel of the sentence based on the averaged hidden\nvector passed to a 2-layer neural network classi\ufb01er\nwith ReLU as the activation function. The mem-\nory size for both LSTMN models was set to 168 to\nbe compatible with previous LSTM models (Tai et\nal., 2015) applied to the same task. We used pre-\ntrained 300-D Glove 840B vectors (Pennington et\nal., 2014) to initialize the word embeddings. The\ngradient for words with Glove embeddings, was\nscaled by 0.35 in the \ufb01rst epoch after which all word\nembeddings were updated normally.\nWe used Adam (Kingma and Ba, 2015) for op-\ntimization with the two momentum parameters set\nto 0.9 and 0.999 respectively. The initial learning\nrate was set to 2E-3. The regularization constant was\n1E-4 and the mini-batch size was 5. A dropout rate\nof 0.5 was applied to the neural network classi\ufb01er.\nWe compared our model with a wide range of top-\nperforming systems. Most of these models (includ-\ning ours) are LSTM variants (third block in Table 2),\nrecursive neural networks (\ufb01rst block), or convolu-\ntional neural networks (CNNs; second block). Re-\ncursive models assume the input sentences are rep-\nresented as parse trees and can take advantage of\nannotations at the phrase level. LSTM-type models\nFigure 5: Examples of intra-attention (sentiment\nanalysis). Bold lines (red) indicate attention be-\ntween sentiment important words.\nexception of CT-LSTM (Tai et al., 2015) which op-\nerates over tree-structured network topologies such\nas constituent trees. For comparison, we also report\nthe performance of the paragraph vector model (PV;\nLe and Mikolov (2014); see Table 2, second block)\nwhich neither operates on trees nor sequences but\nlearns distributed document representations param-\neterized directly.\nThe results in Table 2 show that both 1- and\n2-layer LSTMNs outperform the LSTM baselines\nwhile achieving numbers comparable to state of the\nart. The number of layers for our models was set to\nbe comparable to previously published results. On\nthe \ufb01ne-grained and binary classi\ufb01cation tasks our\n2-layer LSTMN performs close to the best system\nT-CNN (Lei et al., 2015). Figure 5 shows examples\nof intra-attention for sentiment words. Interestingly,\nthe network learns to associate sentiment important\nwords such as though andfantastic ornotandgood .",
                "subsection": []
            },
            {
                "id": "5.3",
                "section": "Natural language inference",
                "text": "The ability to reason about the semantic relation-\nship between two sentences is an integral part of\ntext understanding. We therefore evaluate our model\non recognizing textual entailment, i.e., whether two\npremise-hypothesis pairs are entailing, contradic-\ntory, or neutral. For this task we used the Stan-\nford Natural Language Inference (SNLI) dataset\n(Bowman et al., 2015), which contains premise-\nhypothesis pairs and target labels indicating their\nrelation. After removing sentences with unknown\nlabels, we end up with 549,367 pairs for training,\n9,842 for development and 9,824 for testing. The\nvocabulary size is 36,809 and the average sentence\nlength is 22. We performed lower-casing and tok-\nenization for the entire dataset.Recent approaches use two sequential LSTMs to\nencode the premise and the hypothesis respectively,\nand apply neural attention to reason about their logi-\ncal relationship (Rockt \u00a8aschel et al., 2016; Wang and\nJiang, 2016). Furthermore, Rockt \u00a8aschel et al. (2016)\nshow that a non-standard encoder-decoder architec-\nture which processes the hypothesis conditioned on\nthe premise results signi\ufb01cantly boosts performance.\nWe use a similar approach to tackle this task with\nLSTMNs. Speci\ufb01cally, we use two LSTMNs to read\nthe premise and hypothesis, and then match them\nby comparing their hidden state tapes. We perform\naverage pooling for the hidden state tape of each\nLSTMN, and concatenate the two averages to form\nthe input to a 2-layer neural network classi\ufb01er with\nReLU as the activation function.\nWe used pre-trained 300-D Glove 840B vectors\n(Pennington et al., 2014) to initialize the word em-\nbeddings. Out-of-vocabulary (OOV) words were\ninitialized randomly with Gaussian samples ( \u00b5=0,\ns=1). We only updated OOV vectors in the \ufb01rst\nepoch, after which all word embeddings were up-\ndated normally. The dropout rate was selected from\n[0.1, 0.2, 0.3, 0.4]. We used Adam (Kingma and Ba,\n2015) for optimization with the two momentum pa-\nrameters set to 0.9 and 0.999 respectively, and the\ninitial learning rate set to 1E-3. The mini-batch size\nwas set to 16 or 32. For a fair comparison against\nprevious work, we report results with different hid-\nden/memory dimensions (i.e., 100, 300, and 450).\nWe compared variants of our model against dif-\nferent types of LSTMs (see the second block in Ta-\nble 3). Speci\ufb01cally, these include a model which\nencodes the premise and hypothesis independently\nwith two LSTMs (Bowman et al., 2015), a shared\nLSTM (Rockt \u00a8aschel et al., 2016), a word-by-word\nattention model (Rockt \u00a8aschel et al., 2016), and a\nmatching LSTM (mLSTM; Wang and Jiang (2016)).\nThis model sequentially processes the hypothesis,\nand at each position tries to match the current word\nwith an attention-weighted representation of the\npremise (rather than basing its predictions on whole\nsentence embeddings). We also compared our mod-\nels with a bag-of-words baseline which averages the\npre-trained embeddings for the words in each sen-\ntence and concatenates them to create features for a\nlogistic regression classi\ufb01er (\ufb01rst block in Table 3).\nModels hjqjM Test\nBOW concatenation \u2014 \u2014 59.8\nLSTM (Bowman et al., 2015) 100 221k 77.6\nLSTM-att (Rockt \u00a8aschel et al., 2016) 100 252k 83.5\nmLSTM (Wang and Jiang, 2016) 300 1.9M 86.1\nLSTMN 100 260k 81.5\nLSTMN shallow fusion 100 280k 84.3\nLSTMN deep fusion 100 330k 84.5\nLSTMN shallow fusion 300 1.4M 85.2\nLSTMN deep fusion 300 1.7M 85.7\nLSTMN shallow fusion 450 2.8M 86.0\nLSTMN deep fusion 450 3.4M 86.3\nTable 3: Parameter counts jqjM, size of hidden\nunith, and model accuracy (%) on the natural lan-\nguage inference task.\nto LSTMs (with and without attention; 2nd block\nin Table 3). We also observe that fusion is gen-\nerally bene\ufb01cial, and that deep fusion slightly im-\nproves over shallow fusion. One explanation is that\nwith deep fusion the inter-attention vectors are re-\ncurrently memorized by the decoder with a gating\noperation, which also improves the information \ufb02ow\nof the network. With standard training, our deep fu-\nsion yields the state-of-the-art performance in this\ntask. Although encouraging, this result should be in-\nterpreted with caution since our model has substan-\ntially more parameters compared to related systems.\nWe could compare different models using the same\nnumber of total parameters. However, this would in-\nevitably introduce other biases, e.g., the number of\nhyper-parameters would become different.",
                "subsection": []
            }
        ]
    },
    {
        "id": "4",
        "section": "Modeling two sequences with lstmn",
        "text": "Natural language processing tasks such as machine\ntranslation and textual entailment are concerned\nwith modeling two sequences rather than a single\none. A standard tool for modeling two sequences\nwith recurrent networks is the encoder-decoder ar-\nchitecture where the second sequence (also known\nas the target ) is being processed conditioned on the\n\ufb01rst one (also known as the source ). In this section\nwe explain how to combine the LSTMN which ap-\nplies attention for intra-relation reasoning, with the\nencoder-decoder network whose attention module\nlearns the inter-alignment between two sequences.\nFigures 3a and 3b illustrate two types of combina-\ntion. We describe the models more formally below.\nShallow Attention Fusion Shallow fusion simply\ntreats the LSTMN as a separate module that can\nbe readily used in an encoder-decoder architecture,\nin lieu of a standard RNN or LSTM. As shown in\nFigure 3a, both encoder and decoder are modeled\nas LSTMNs with intra-attention. Meanwhile, inter-\nattention is triggered when the decoder reads a tar-\nget token, similar to the inter-attention introduced in\nBahdanau et al. (2014).\nDeep Attention Fusion Deep fusion combines\ninter- and intra-attention (initiated by the decoder)\nwhen computing state updates. We use different no-\ntation to represent the two sets of attention. Follow-\ning Section 3.2, CandHdenote the target memory\ntape and hidden tape, which store representations of\nthe target symbols that have been processed so far.\nThe computation of intra-attention follows Equa-\ntions (4)\u2013(9). Additionally, we use A= [a1;\u0001\u0001\u0001;am]\nandY= [g1;\u0001\u0001\u0001;gm]to represent the source mem-\nory tape and hidden tape, with mbeing the length of\nthe source sequence conditioned upon. We computeinter-attention between the input at time step tand\ntokens in the entire source sequence as follows:\nbt\nj=uTtanh(Wggj+Wxxt+W\u02dcg\u02dcgt\u00001) (11)\npt\nj=softmax (bt\nj) (12)\nAfter that we compute the adaptive representation of\nthe source memory tape \u02dcatand hidden tape \u02dcgtas:\n\u0014\u02dcgt\n\u02dcat\u0015\n=m\n\u00e5\nj=1pt\nj\u0001\u0014gj\naj\u0015\n(13)\nWe can then transfer the adaptive source represen-\ntation \u02dcatto the target memory with another gating\noperation rt, analogous to the gates in Equation (7).\nrt=s(Wr\u0001[\u02dcgt;xt]) (14)\nThe new target memory includes inter-alignment\nrt\f\u02dcat, intra-relation ft\f\u02dcct, and the new input in-\nformation it\f\u02c6ct:\nct=rt\f\u02dcat+ft\f\u02dcct+it\f\u02c6ct (15)\nht=ot\ftanh(ct) (16)\nAs shown in the equations above and Figure 3b, the\nmajor change of deep fusion lies in the recurrent\nstorage of the inter-alignment vector in the target\nmemory network, as a way to help the target net-\nwork review source information.",
        "subsection": []
    },
    {
        "id": "5",
        "section": "Experiments",
        "text": "In this section we present our experiments for eval-\nuating the performance of the LSTMN machine\nreader. We start with language modeling as it\nis a natural testbed for our model. We then as-\nsess the model\u2019s ability to extract meaning repre-\nsentations for generic sentence classi\ufb01cation tasks\nsuch as sentiment analysis. Finally, we examine\nwhether the LSTMN can recognize the semantic\nrelationship between two sentences by applying it\nto a natural language inference task. Our code\nis available at https://github.com/cheng6076/\nSNLI-attention(a) Decoder with shallow attention fusion.\n (b) Decoder with deep attention fusion.\nFigure 3: LSTMNs for sequence-to-sequence modeling. The encoder uses intra-attention, while the decoder\nincorporates both intra- and inter-attention. The two \ufb01gures present two ways to combine the intra- and\ninter-attention in the decoder.\nModels Layers Perplexity\nKN5 \u2014 141\nRNN 1 129\nLSTM 1 115\nLSTMN 1 108\nsLSTM 3 115\ngLSTM 3 107\ndLSTM 3 109\nLSTMN 3 102\nTable 1: Language model perplexity on the Penn\nTreebank. The size of memory is 300 for all models.",
        "subsection": []
    },
    {
        "id": "6",
        "section": "Conclusions",
        "text": "In this paper we proposed a machine reading simula-\ntor to address the limitations of recurrent neural net-\nworks when processing inherently structured input.\nOur model is based on a Long Short-Term Mem-\nory architecture embedded with a memory network,\nexplicitly storing contextual representations of in-\nput tokens without recursively compressing them.\nMore importantly, an intra-attention mechanism is\nemployed for memory addressing, as a way to in-\nduce undirected relations among tokens. The at-\ntention layer is not optimized with a direct super-\nvision signal but with the entire network in down-\nstream tasks. Experimental results across three tasks\nshow that our model yields performance comparableor superior to state of the art.\nAlthough our experiments focused on LSTMs, the\nidea of building more structure aware neural models\nis general and can be applied to other types of net-\nworks. When direct supervision is provided, simi-\nlar architectures can be adapted to tasks such as de-\npendency parsing and relation extraction. In the fu-\nture, we hope to develop more linguistically plausi-\nble neural architectures able to reason over nested\nstructures and neural models that learn to discover\ncompositionality with weak or indirect supervision.\nAcknowledgments\nWe thank members of the ILCC at the School of\nInformatics and the anonymous reviewers for help-\nful comments. The support of the European Re-\nsearch Council under award number 681760 \u201cTrans-\nlating Multiple Modalities into Text\u201d is gratefully\nacknowledged.",
        "subsection": []
    },
    {
        "missing": []
    },
    {
        "references": []
    },
    {
        "title": "Long Short-Term Memory-Networks for Machine Reading",
        "arxiv_id": "1601.06733"
    }
]