[
    {
        "id": "",
        "section": "Abstract",
        "text": "This paper proposes a new model for extracting an interpretable sentence embed-\nding by introducing self-attention. Instead of using a vector, we use a 2-D matrix\nto represent the embedding, with each row of the matrix attending on a different\npart of the sentence. We also propose a self-attention mechanism and a special\nregularization term for the model. As a side effect, the embedding comes with an\neasy way of visualizing what speci",
        "subsection": []
    },
    {
        "id": "1",
        "section": "Introduction",
        "text": "Much progress has been made in learning semantically meaningful distributed representations of\nindividual words, also known as word embeddings (Bengio et al., 2001; Mikolov et al., 2013).\nOn the other hand, much remains to be done to obtain satisfying representations of phrases and\nsentences. Those methods generally fall into two categories. The \ufb01rst consists of universal sentence\nembeddings usually trained by unsupervised learning (Hill et al., 2016). This includes SkipThought\nvectors (Kiros et al., 2015), ParagraphVector (Le & Mikolov, 2014), recursive auto-encoders (Socher\net al., 2011; 2013), Sequential Denoising Autoencoders (SDAE), FastSent (Hill et al., 2016), etc.\nThe other category consists of models trained speci\ufb01cally for a certain task. They are usually\ncombined with downstream applications and trained by supervised learning. One generally \ufb01nds\nthat speci\ufb01cally trained sentence embeddings perform better than generic ones, although generic\nones can be used in a semi-supervised setting, exploiting large unlabeled corpora. Several models\nhave been proposed along this line, by using recurrent networks (Hochreiter & Schmidhuber, 1997;\nChung et al., 2014), recursive networks (Socher et al., 2013) and convolutional networks (Kalchbren-\nner et al., 2014; dos Santos & Gatti, 2014; Kim, 2014) as an intermediate step in creating sentence\nrepresentations to solve a wide variety of tasks including classi\ufb01cation and ranking (Yin & Sch \u00a8utze,\n2015; Palangi et al., 2016; Tan et al., 2016; Feng et al., 2015). A common approach in previous\nmethods consists in creating a simple vector representation by using the \ufb01nal hidden state of the\nRNN or the max (or average) pooling from either RNNs hidden states or convolved n-grams. Ad-\nditional works have also been done in exploiting linguistic structures such as parse and dependence\ntrees to improve sentence representations (Ma et al., 2015; Mou et al., 2015b; Tai et al., 2015).\nFor some tasks people propose to use attention mechanism on top of the CNN or LSTM model to\nintroduce extra source of information to guide the extraction of sentence embedding (dos Santos\net al., 2016). However, for some other tasks like sentiment classi\ufb01cation, this is not directly appli-\ncable since there is no such extra information: the model is only given one single sentence as input.\nIn those cases, the most common way is to add a max pooling or averaging step across all time steps\n\u0003Published as a conference paper at ICLR 2017\nwe.\nhada\n.........w2w3 wn w4h2h3h4... hnMAi2Ai3Ai4... Ainm1m2 mr mi\n.........we.\nhada\nnice\nexperiencethatin\nrestaurant.........\nw1w2w3 wn w4h1h2h3h4... hnM\nAi1Ai2Ai3Ai4... Ainm1m2 mr mi\n.........\n(a)\nh1h2hn...\nWs1tanh\nsoftmaxWs2\nr\nr2uh1h2hn...\nWs1n\nntanh\nsoftmaxWs2\nAda\nr\nr2u(b)\nFigure 1: A sample model structure showing the sentence embedding model combined with a fully\nconnected and softmax layer for sentiment analysis (a). The sentence embedding Mis computed as\nmultiple weighted sums of hidden states from a bidirectional LSTM ( h1; :::;hn), where the summa-\ntion weights ( Ai1; :::; A in) are computed in a way illustrated in (b). Blue colored shapes stand for\nhidden representations, and red colored shapes stand for weights, annotations, or input/output.\n(Lee & Dernoncourt, 2016), or just pick up the hidden representation at the last time step as the\nencoded embedding (Margarit & Subramaniam, 2016).\nA common approach in many of the aforementioned methods consists of creating a simple vector\nrepresentation by using the \ufb01nal hidden state of the RNN or the max (or average) pooling from\neither RNNs hidden states or convolved n-grams. We hypothesize that carrying the semantics along\nall time steps of a recurrent model is relatively hard and not necessary. We propose a self-attention\nmechanism for these sequential models to replace the max pooling or averaging step. Different from\nprevious approaches, the proposed self-attention mechanism allows extracting different aspects of\nthe sentence into multiple vector representations. It is performed on top of an LSTM in our sentence\nembedding model. This enables attention to be used in those cases when there are no extra inputs. In\naddition, due to its direct access to hidden representations from previous time steps, it relieves some\nlong-term memorization burden from LSTM. As a side effect coming together with our proposed\nself-attentive sentence embedding, interpreting the extracted embedding becomes very easy and\nexplicit.\nSection 2 details on our proposed self-attentive sentence embedding model, as well as a regular-\nization term we proposed for this model, which is described in Section 2.2. We also provide a\nvisualization method for this sentence embedding in section 2.3. We then evaluate our model in\nauthor pro\ufb01ling, sentiment classi\ufb01cation and textual entailment tasks in Section 4.",
        "subsection": [
            {
                "id": "2.1",
                "section": "Model",
                "text": "The proposed sentence embedding model consists of two parts. The \ufb01rst part is a bidirectional\nLSTM, and the second part is the self-attention mechanism, which provides a set of summation\nweight vectors for the LSTM hidden states. These set of summation weight vectors are dotted\nwith the LSTM hidden states, and the resulting weighted LSTM hidden states are considered as\nPublished as a conference paper at ICLR 2017\nbe applied on a downstream application. Figure 1 shows an example when the proposed sentence\nembedding model is applied to sentiment analysis, combined with a fully connected layer and a\nsoftmax layer. Besides using a fully connected layer, we also proposes an approach that prunes\nweight connections by utilizing the 2-D structure of matrix sentence embedding, which is detailed\nin Appendix A. For this section, we will use Figure 1 to describe our model.\nSuppose we have a sentence, which has ntokens, represented in a sequence of word embeddings.\nS= (w1;w2;\u0001\u0001\u0001wn) (1)\nHerewiis a vector standing for a ddimentional word embedding for the i-th word in the sentence.\nSis thus a sequence represented as a 2-D matrix, which concatenates all the word embeddings\ntogether. Sshould have the shape n-by-d.\nNow each entry in the sequence Sare independent with each other. To gain some dependency be-\ntween adjacent words within a single sentence, we use a bidirectional LSTM to process the sentence:\n\u0000 !ht=\u0000\u0000\u0000\u0000!LSTM (wt;\u0000\u0000!ht\u00001) (2)\n \u0000ht= \u0000\u0000\u0000\u0000LSTM (wt; \u0000\u0000ht+1) (3)\nAnd we concatenate each\u0000 !htwith \u0000htto obtain a hidden state ht. Let the hidden unit number for each\nunidirectional LSTM be u. For simplicity, we note all the n hts asH, who have the size n-by-2u.\nH= (h1;h2;\u0001\u0001\u0001hn) (4)\nOur aim is to encode a variable length sentence into a \ufb01xed size embedding. We achieve that by\nchoosing a linear combination of the nLSTM hidden vectors in H. Computing the linear combina-\ntion requires the self-attention mechanism. The attention mechanism takes the whole LSTM hidden\nstates Has input, and outputs a vector of weights a:\na=softmax\u0000\nws2tanh\u0000\nWs1HT\u0001\u0001\n(5)\nHere Ws1is a weight matrix with a shape of da-by-2u. and ws2is a vector of parameters with\nsizeda, where dais a hyperparameter we can set arbitrarily. Since His sized n-by-2u, the anno-\ntation vector awill have a size n. the softmax (_)ensures all the computed weights sum up to 1.\nThen we sum up the LSTM hidden states Haccording to the weight provided by ato get a vector\nrepresentation mof the input sentence.\nThis vector representation usually focuses on a speci\ufb01c component of the sentence, like a special set\nof related words or phrases. So it is expected to re\ufb02ect an aspect, or component of the semantics in\na sentence. However, there can be multiple components in a sentence that together forms the overall\nsemantics of the whole sentence, especially for long sentences. (For example, two clauses linked\ntogether by an \u201dand.\u201d) Thus, to represent the overall semantics of the sentence, we need multiple m\u2019s\nthat focus on different parts of the sentence. Thus we need to perform multiple hops of attention.\nSay we want rdifferent parts to be extracted from the sentence, with regard to this, we extend the\nws2into a r-by-damatrix, note it as Ws2, and the resulting annotation vector abecomes annotation\nmatrix A. Formally,\nA=softmax\u0000\nWs2tanh\u0000\nWs1HT\u0001\u0001\n(6)\nHere the softmax (_)is performed along the second dimension of its input. We can deem Equation\n6 as a 2-layer MLP without bias, whose hidden unit numbers is da, and parameters are fWs2; Ws1g.\nThe embedding vector mthen becomes an r-by-2uembedding matrix M. We compute the r\nweighted sums by multiplying the annotation matrix Aand LSTM hidden states H, the resulting\nmatrix is the sentence embedding:\nM=AH (7)",
                "subsection": []
            },
            {
                "id": "2.2",
                "section": "Penalization term",
                "text": "The embedding matrix Mcan suffer from redundancy problems if the attention mechanism always\nprovides similar summation weights for all the rhops. Thus we need a penalization term to encour-\nPublished as a conference paper at ICLR 2017\nThe best way to evaluate the diversity is de\ufb01nitely the Kullback Leibler divergence between any 2\nof the summation weight vectors. However, we found that not very stable in our case. We conjecture\nit is because we are maximizing a set of KL divergence (instead of minimizing only one, which is\nthe usual case), we are optimizing the annotation matrix A to have a lot of suf\ufb01ciently small or\neven zero values at different softmax output units, and these vast amount of zeros is making the\ntraining unstable. There is another feature that KL doesn\u2019t provide but we want, which is, we want\neach individual row to focus on a single aspect of semantics, so we want the probability mass in the\nannotation softmax output to be more focused. but with KL penalty we cant encourage that.\nWe hereby introduce a new penalization term which overcomes the aforementioned shortcomings.\nCompared to the KL divergence penalization, this term consumes only one third of the computation.\nWe use the dot product of Aand its transpose, subtracted by an identity matrix, as a measure of\nredundancy.\nP=\r\r\u0000\nAAT\u0000I\u0001\r\r\nF2(8)\nHerek\u000fkFstands for the Frobenius norm of a matrix. Similar to adding an L2 regularization term,\nthis penalization term Pwill be multiplied by a coef\ufb01cient, and we minimize it together with the\noriginal loss, which is dependent on the downstream application.\nLet\u2019s consider two different summation vectors aiandajinA. Because of the softmax, all entries\nwithin any summation vector in Ashould sum up to 1. Thus they can be deemed as probability\nmasses in a discrete probability distribution. For any non-diagonal elements aij(i6=j)in the AAT\nmatrix, it corresponds to a summation over elementwise product of two distributions:\n0< aij=nX\nk=1ai\nkaj\nk<1 (9)\nwhere ai\nkandaj\nkare the k-th element in the aiandajvectors, respectively. In the most extreme case,\nwhere there is no overlap between the two probability distributions aiandaj, the correspond aijwill\nbe 0. Otherwise, it will have a positive value. On the other extreme end, if the two distributions are\nidentical and all concentrates on one single word, it will have a maximum value of 1. We subtract\nan identity matrix from AATso that forces the elements on the diagonal of AATto approximate\n1, which encourages each summation vector aito focus on as few number of words as possible,\nforcing each vector to be focused on a single aspect, and all other elements to 0, which punishes\nredundancy between different summation vectors.",
                "subsection": []
            },
            {
                "id": "2.3",
                "section": "Visualization",
                "text": "The interpretation of the sentence embedding is quite straight forward because of the existence of\nannotation matrix A. For each row in the sentence embedding matrix M, we have its corresponding\nannotation vector ai. Each element in this vector corresponds to how much contribution the LSTM\nhidden state of a token on that position contributes to. We can thus draw a heat map for each row of\nthe embedding matrix MThis way of visualization gives hints on what is encoded in each part of\nthe embedding, adding an extra layer of interpretation. (See Figure 3a and 3b).\nThe second way of visualization can be achieved by summing up over all the annotation vectors,\nand then normalizing the resulting weight vector to sum up to 1. Since it sums up all aspects of\nsemantics of a sentence, it yields a general view of what the embedding mostly focuses on. We can\n\ufb01gure out which words the embedding takes into account a lot, and which ones are skipped by the\nembedding. See Figure 3c and 3d.",
                "subsection": []
            }
        ]
    },
    {
        "id": "2",
        "section": "Approach",
        "text": "",
        "subsection": []
    },
    {
        "id": "3",
        "section": "Related work",
        "text": "Various supervised and unsupervised sentence embedding models have been mentioned in Section",
        "subsection": [
            {
                "id": "4.1",
                "section": "Author profiling",
                "text": "The Author Pro\ufb01ling dataset1consists of Twitter tweets in English, Spanish, and Dutch. For some of\nthe tweets, it also provides an age and gender of the user when writing the tweet. The age range are\nsplit into 5 classes: 18-24, 25-34, 35-49, 50-64, 65+. We use English tweets as input, and use those\ntweets to predict the age range of the user. Since we are predicting the age of users, we refer to it\nas Age dataset in the rest of our paper. We randomly selected 68485 tweets as training set, 4000 for\ndevelopment set, and 4000 for test set. Performances are also chosen to be classi\ufb01cation accuracy.\nTable 1: Performance Comparision of Different Models on Yelp and Age Dataset\nModels Yelp Age\nBiLSTM + Max Pooling + MLP 61.99% 77.40%\nCNN + Max Pooling + MLP 62.05% 78.15%\nOur Model 64.21% 80.45%\nWe compare our model with two baseline models: biLSTM and CNN. For the two baseline models.\nThe biLSTM model uses a bidirectional LSTM with 300 dimensions in each direction, and use max\npooling across all LSTM hidden states to get the sentence embedding vector, then use a 2-layer\nReLU output MLP with 3000 hidden states to output the classi\ufb01cation result. The CNN model\nuses the same scheme, but substituting biLSTM with 1 layer of 1-D convolutional network. During\ntraining we use 0.5 dropout on the MLP and 0.0001 L2 regularization. We use stochastic gradient\ndescent as the optimizer, with a learning rate of 0.06, batch size 16. For biLSTM, we also clip the\n1Published as a conference paper at ICLR 2017\n(a) 1 star reviews\n(b) 5 star reviews\nFigure 2: Heatmap of Yelp reviews with the two extreme score.\nnorm of gradients to be between -0.5 and 0.5. We searched hyperparameters in a wide range and\n\ufb01nd the aforementioned set of hyperparameters yields the highest accuracy.\nFor our model, we use the same settings as what we did in biLSTM. We also use a 2-layer ReLU\noutput MLP, but with 2000 hidden units. In addition, our self-attention MLP has a hidden layer with\n350 units (the dain Section 2), we choose the matrix embedding to have 30 rows (the r), and a\ncoef\ufb01cient of 1 for the penalization term.\nWe train all the three models until convergence and select the corresponding test set performance\naccording to the best development set performance. Our results show that the model outperforms\nboth of the biLSTM and CNN baselines by a signi\ufb01cant margin.",
                "subsection": []
            },
            {
                "id": "4.2",
                "section": "Sentiment analysis",
                "text": "We choose the Yelp dataset2for sentiment analysis task. It consists of 2.7M yelp reviews, we take\nthe review as input and predict the number of stars the user who wrote that review assigned to the\ncorresponding business store. We randomly select 500K review-star pairs as training set, and 2000\nfor development set, 2000 for test set. We tokenize the review texts by Stanford tokenizer. We use\n2https://www.yelp.com/datasetPublished as a conference paper at ICLR 2017\n100 dimensional word2vec as initialization for word embeddings, and tune the embedding during\ntraining across all of our experiments. The target number of stars is an integer number in the range\nof[1;5], inclusive. We are treating the task as a classi\ufb01cation task, i.e., classify a review text into\none of the 5 classes. We use classi\ufb01cation accuracy as a measurement.\nFor the two baseline models, we use the same setting as what we used for Author Pro\ufb01ling dataset,\nexcept that we are using a batch size of 32 instead. For our model, we are also using the same\nsetting, except that we choose the hidden unit numbers in the output MLP to be 3000 instead. We\nalso observe a signi\ufb01cant performance gain comparining to the two baselines. (Table 1)\nAs an interpretation of the learned sentence embedding, we use the second way of visualization\ndescribed in Section 2.3 to plot heat maps for some of the reviews in the dataset. We randomly\nselect 5 examples of negative (1 star) and positive (5 stars) reviews from the test set, when the model\nhas a high con\ufb01dence ( >0:8) in predicting the label. As shown in Figure 2, we \ufb01nd that the model\nmajorly learns to capture some key factors in the review that indicate strongly on the sentiment\nbehind the sentence. For most of the short reviews, the model manages to capture all the key factors\nthat contribute to an extreme score, but for longer reviews, the model is still not able to capture all\nrelated factors. For example, in the 3rd review in Figure 2b), it seems that a lot of focus is spent on\none single factor, i.e., the \u201dso much fun\u201d, and the model puts a little amount of attention on other\nkey points like \u201dhighly recommend\u201d, \u201damazing food\u201d, etc.",
                "subsection": []
            },
            {
                "id": "4.3",
                "section": "Textual entailment",
                "text": "We use the biggest dataset in textual entailment, the SNLI corpus (Bowman et al., 2015) for our\nevaluation on this task. SNLI is a collection of 570k human-written English sentence pairs manually\nlabeled for balanced classi\ufb01cation with the labels entailment, contradiction, and neutral. The model\nwill be given a pair of sentences, called hypothesis and premise respectively, and asked to tell if the\nsemantics in the two sentences are contradicting with each other or not. It is also a classi\ufb01cation\ntask, so we measure the performance by accuracy.\nWe process the hypothesis and premise independently, and then extract the relation between the two\nsentence embeddings by using multiplicative interactions proposed in Memisevic (2013) (see Ap-\npendix B for details), and use a 2-layer ReLU output MLP with 4000 hidden units to map the hidden\nrepresentation into classi\ufb01cation results. Parameters of biLSTM and attention MLP are shared across\nhypothesis and premise. The biLSTM is 300 dimension in each direction, the attention MLP has\n150 hidden units instead, and both sentence embeddings for hypothesis and premise have 30 rows\n(ther). The penalization term coef\ufb01cient is set to 0.3. We use 300 dimensional GloVe (Pennington\net al., 2014) word embedding to initialize word embeddings. We use AdaGrad as the optimizer,\nwith a learning rate of 0.01. We don\u2019t use any extra regularization methods, like dropout or L2\nnormalization. Training converges after 4 epochs, which is relatively fast.\nThis task is a bit different from previous two tasks, in that it has 2 sentences as input. There are\na bunch of ways to add inter-sentence level attention, and those attentions bring a lot of bene\ufb01ts.\nTo make the comparison focused and fair, we only compare methods that fall into the sentence\nencoding-based models. i.e., there is no information exchanged between the hypothesis and premise\nbefore they are encoded into some distributed encoding.\nTable 2: Test Set Performance Compared to other Sentence Encoding Based Methods in SNLI Datset\nModel Test Accuracy\n300D LSTM encoders (Bowman et al., 2016) 80.6%\n600D (300+300) BiLSTM encoders (Liu et al., 2016b) 83.3%\n300D Tree-based CNN encoders (Mou et al., 2015a) 82.1%\n300D SPINN-PI encoders (Bowman et al., 2016) 83.2%\n300D NTI-SLSTM-LSTM encoders (Munkhdalai & Yu, 2016a) 83.4%\n1024D GRU encoders with SkipThoughts pre-training (Vendrov et al., 2015) 81.4%\n300D NSE encoders (Munkhdalai & Yu, 2016b) 84.6%\nPublished as a conference paper at ICLR 2017\nWe \ufb01nd that compared to other published approaches, our method shows a signi\ufb01cant gain ( \u00151%)\nto them, except for the 300D NSE encoders, which is the state-of-the-art in this category. However,\nthe0:2%different is relatively small compared to the differences between other methods.",
                "subsection": []
            },
            {
                "id": "4.4",
                "section": "Exploratory experiments",
                "text": "In this subsection we are going to do a set of exploratory experiments to study the relative effect of\neach component in our model.",
                "subsection": [
                    {
                        "id": "4.4.1",
                        "section": "Effect of penalization term",
                        "text": "Since the purpose of introducing the penalization term Pis majorly to discourage the redundancy\nin the embedding, we \ufb01rst directly visualize the heat maps of each row when the model is presented\nwith a sentence. We compare two identical models with the same size as detailed in Section 4.1\ntrained separately on Age dataset, one with this penalization term (where the penalization coef\ufb01cient\nis set to 1.0) and the other with no penalty. We randomly select one tweet from the test set and\ncompare the two models by plotting a heat map for each hop of attention on that single tweet. Since\nthere are 30 hops of attention for each model, which makes plotting all of them quite redundant, we\nonly plot 6 of them. These 6 hops already re\ufb02ect the situation in all of the 30 hops.\n(a)\n (b)\n(c) without penalization\n (d) with 1.0 penalization\nFigure 3: Heat maps for 2 models trained on Age dataset. The left column is trained without the\npenalization term, and the right column is trained with 1.0 penalization. (a) and (b) shows detailed\nattentions taken by 6 out of 30 rows of the matrix embedding, while (c) and (d) shows the overall\nattention by summing up all 30 attention weight vectors.\n(a) Yelp without penalization\n (b) Yelp with penalization\nFigure 4: Attention of sentence embedding on 3 different Yelp reviews. The left one is trained\nPublished as a conference paper at ICLR 2017\nTable 3: Performance comparision regarding the penalization term\nPenalization coef\ufb01cient Yelp Age\n1.0 64.21% 80.45%\n0.0 61.74% 79.27%\nFrom the \ufb01gure we can tell that the model trained without the penalization term have lots of redun-\ndancies between different hops of attention (Figure 3a), resulting in putting lot of focus on the word\n\u201dit\u201d (Figure 3c), which is not so relevant to the age of the author. However in the right column, the\nmodel shows more variations between different hops, and as a result, the overall embedding focuses\non \u201dmail-replies spam\u201d instead. (Figure 3d)\nFor the Yelp dataset, we also observe a similar phenomenon. To make the experiments more ex-\nplorative, we choose to plot heat maps of overall attention heat maps for more samples, instead of\nplotting detailed heat maps for a single sample again. Figure 4 shows overall focus of the sentence\nembedding on three different reviews. We observe that with the penalization term, the model tends\nto be more focused on important parts of the review. We think it is because that we are encouraging\nit to be focused, in the diagonals of matrix AAT(Equation 8).\nTo validate if these differences result in performance difference, we evaluate four models trained\non Yelp and Age datasets, both with and without the penalization term. Results are shown in Table\n3. Consistent with what expected, models trained with the penalization term outperforms their\ncounterpart trained without.\nIn SNLI dataset, although we observe that introducing the penalization term still contributes to en-\ncouraging the diversity of different rows in the matrix sentence embedding, and forcing the network\nto be more focused on the sentences, the quantitative effect of this penalization term is not so obvious\non SNLI dataset. Both models yield similar test set accuracies.",
                        "subsection": []
                    },
                    {
                        "id": "4.4.2",
                        "section": "Effect of multiple vectors",
                        "text": "Having multiple rows in the sentence embedding is expected to provide more abundant information\nabout the encoded content. It makes sence to evaluate how signi\ufb01cant the improvement can be\nbrought by r. Taking the models we used for Age and SNLI dataset as an example, we vary rfrom\n1to30for each task, and train the resulting 10models independently (Figure 5). Note that when\nr= 1, the sentence embedding reduces to a normal vector form.\nFrom this \ufb01gure we can \ufb01nd that, without having multiple rows, the model performs on-par with\nits competitiors which use other forms of vector sentence embeddings. But there is signi\ufb01cant\n(a)\n (b)\nFigure 5: Effect of the number of rows ( r) in matrix sentence embedding. The vertical axes indicates\ntest set accuracy and the horizontal axes indicates training epoches. Numbers in the legends stand\nfor the corresponding values of r. (a) is conducted in Age dataset and (b) is conducted in SNLI\nPublished as a conference paper at ICLR 2017\ndifference between having only one vector for the sentence embedding and multiple vectors. The\nmodels are also quite invariant with respect to r, since in the two \ufb01gures a wide range of values\nbetween 10to30are all generating comparable curves.",
                        "subsection": []
                    }
                ]
            }
        ]
    },
    {
        "id": "1.",
        "section": "Different from those models",
        "text": "allows it to extract different aspects of the sentence into multiple vector-representations. The matrix\nstructure together with the penalization term gives our model a greater capacity to disentangle the\nlatent information from the input sentence. We also do not use linguistic structures to guide our\nsentence representation model. Additionally, using our method we can easily create visualizations\nPublished as a conference paper at ICLR 2017\nSome recent work have also proposed supervised methods that use intra/self-sentence attention. Ling\net al. (2015) proposed an attention based model for word embedding, which calculates an attention\nweight for each word at each possible position in the context window. However this method cannot\nbe extended to sentence level embeddings since one cannot exhaustively enumerate all possible\nsentences. Liu et al. (2016a) proposes a sentence level attention which has a similar motivation but\ndone differently. They utilize the mean pooling over LSTM states as the attention source, and use\nthat to re-weight the pooled vector representation of the sentence.\nApart from the previous 2 variants, we want to note that Li et al. (2016) proposed a same self\nattention mechanism for question encoding in their factoid QA model, which is concurrent to our\nwork. The difference lies in that their encoding is still presented as a vector, but our attention\nproduces a matrix representation instead, with a specially designed penalty term. We applied the\nmodel for sentiment anaysis and entailment, and their model is for factoid QA.\nThe LSTMN model (Cheng et al., 2016) also proposed a very successful intra-sentence level atten-\ntion mechanism, which is later used by Parikh et al. (2016). We see our attention and theirs as having\ndifferent granularities. LSTMN produces an attention vector for each of its hidden states during the\nrecurrent iteration, which is sort of an \u201donline updating\u201d attention. It\u2019s more \ufb01ne-grained, targeting\nat discovering lexical correlations between a certain word and its previous words. On the contrary,\nour attention mechanism is only performed once, focuses directly on the semantics that makes sense\nfor discriminating the targets. It is less focused on relations between words, but more on the seman-\ntics of the whole sentence that each word contributes to. Computationally, our method also scales up\nwith the sentence length better, since it doesn\u2019t require the LSTM to compute an annotation vector\nover all of its previous words each time when the LSTMN computes its next step.",
        "subsection": []
    },
    {
        "id": "4",
        "section": "Experimental results",
        "text": "We \ufb01rst evaluate our sentence embedding model by applying it to 3 different datasets: the Age\ndataset, the Yelp dataset, and the Stanford Natural Language Inference (SNLI) Corpus. These 3\ndatasets fall into 3 different tasks, corresponding to author pro\ufb01ling, sentiment analysis, and tex-\ntual entailment, respectively. Then we also perform a set of exploratory experiments to validate\nproperties of various aspects for our sentence embedding model.",
        "subsection": []
    },
    {
        "id": "5",
        "section": "Conclusion and discussion",
        "text": "In this paper, we introduced a \ufb01xed size, matrix sentence embedding with a self-attention mecha-\nnism. Because of this attention mechanism, there is a way to interpret the sentence embedding in\ndepth in our model. Experimental results over 3 different tasks show that the model outperforms\nother sentence embedding models by a signi\ufb01cant margin.\nIntroducing attention mechanism allows the \ufb01nal sentence embedding to directly access previous\nLSTM hidden states via the attention summation. Thus the LSTM doesn\u2019t need to carry every piece\nof information towards its last hidden state. Instead, each LSTM hidden state is only expected to\nprovide shorter term context information around each word, while the higher level semantics, which\nrequires longer term dependency, can be picked up directly by the attention mechanism. This setting\nreliefs the burden of LSTM to carry on long term dependencies. Our experiments also support that,\nas we observed that our model has a bigger advantage when the contents are longer. Further more,\nthe notion of summing up elements in the attention mechanism is very primitive, it can be something\nmore complex than that, which will allow more operations on the hidden states of LSTM.\nThe model is able to encode any sequence with variable length into a \ufb01xed size representation,\nwithout suffering from long-term dependency problems. This brings a lot of scalability to the model:\nwithout any modi\ufb01cation, it can be applied directly to longer contents like paragraphs, articles, etc.\nThough this is beyond the focus of this paper, it remains an interesting direction to explore as a\nfuture work.\nAs a downside of our proposed model, the current training method heavily relies on downstream\napplications, thus we are not able to train it in an unsupervised way. The major obstacle towards\nenabling unsupervised learning in this model is that during decoding, we don\u2019t know as prior how\nthe different rows in the embedding should be divided and reorganized. Exploring all those possible\ndivisions by using a neural network could easily end up with over\ufb01tting. Although we can still do\nunsupervised learning on the proposed model by using a sequential decoder on top of the sentence\nembedding, it merits more to \ufb01nd some other structures as a decoder.\nACKNOWLEDGMENTS\nThe authors would like to acknowledge the developers of Theano (Theano Development Team,\n2016) and Lasagne. The \ufb01rst author would also like to thank IBM Watson for providing resources,\nfundings and valuable discussions to make this project possible, and Caglar Gulcehre for helpful\ndiscussions.\nREFERENCES\nYoshua Bengio, R \u00b4ejean Ducharme, and Pascal Vincent. A neural probabilistic language model. In\nAdvances in Neural Information Processing Systems , pp. 932\u2013938, 2001.\nSamuel R Bowman, Gabor Angeli, Christopher Potts, and Christopher D Manning. A large anno-\ntated corpus for learning natural language inference. arXiv preprint arXiv:1508.05326 , 2015.\nSamuel R Bowman, Jon Gauthier, Abhinav Rastogi, Raghav Gupta, Christopher D Manning, and\nChristopher Potts. A fast uni\ufb01ed model for parsing and sentence understanding. arXiv preprint\narXiv:1603.06021 , 2016.\nJianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\nreading. In Conference on Empirical Methods in Natural Language Processing (EMNLP) . Asso-\nciation for Computational Linguistics, 2016.\nJunyoung Chung, Caglar Gulcehre, KyungHyun Cho, and Yoshua Bengio. Empirical evaluation of\ngated recurrent neural networks on sequence modeling.Published as a conference paper at ICLR 2017\nCicero dos Santos and Maira Gatti. Deep convolutional neural networks for sentiment analysis of\nshort texts. In Proceedings of COLING 2014, the 25th International Conference on Computa-\ntional Linguistics: Technical Papers , pp. 69\u201378, 2014.\nCicero dos Santos, Ming Tan, Bing Xiang, and Bowen Zhou. Attentive pooling networks. arXiv\npreprint arXiv:1602.03609 , 2016.\nMinwei Feng, Bing Xiang, Michael R. Glass, Lidan Wang, and Bowen Zhou. Applying deep learn-\ning to answer selection: a study and an open task. In 2015 IEEE Workshop on Automatic Speech\nRecognition and Understanding, ASRU 2015, Scottsdale, AZ, USA, December 13-17, 2015 , pp.\n813\u2013820, 2015.\nFelix Hill, Kyunghyun Cho, and Anna Korhonen. Learning distributed representations of sentences\nfrom unlabelled data. In Proceedings of the 2016 Conference of the North American Chapter\nof the Association for Computational Linguistics: Human Language Technologies , pp. 1367\u2013\n1377, San Diego, California, June 2016. Association for Computational Linguistics. URL http:\n//www.aclweb.org/anthology/N16-1162 .\nSepp Hochreiter and J \u00a8urgen Schmidhuber. Long short-term memory. Neural computation , 9(8):\n1735\u20131780, 1997.\nNal Kalchbrenner, Edward Grefenstette, and Phil Blunsom. A convolutional neural network for\nmodelling sentences. arXiv preprint arXiv:1404.2188 , 2014.\nYoon Kim. Convolutional neural networks for sentence classi\ufb01cation. arXiv preprint\narXiv:1408.5882 , 2014.\nRyan Kiros, Yukun Zhu, Ruslan R Salakhutdinov, Richard Zemel, Raquel Urtasun, Antonio Tor-\nralba, and Sanja Fidler. Skip-thought vectors. In Advances in neural information processing\nsystems , pp. 3294\u20133302, 2015.\nQuoc V Le and Tomas Mikolov. Distributed representations of sentences and documents. In ICML ,\nvolume 14, pp. 1188\u20131196, 2014.\nJi Young Lee and Franck Dernoncourt. Sequential short-text classi\ufb01cation with recurrent and con-\nvolutional neural networks. arXiv preprint arXiv:1603.03827 , 2016.\nPeng Li, Wei Li, Zhengyan He, Xuguang Wang, Ying Cao, Jie Zhou, and Wei Xu. Dataset and neural\nrecurrent sequence labeling model for open-domain factoid question answering. arXiv preprint\narXiv:1607.06275 , 2016.\nWang Ling, Lin Chu-Cheng, Yulia Tsvetkov, and Silvio Amir. Not all contexts are created equal:\nBetter word representations with variable attention. In Proceedings of the 2015 Conference on\nEmpirical Methods in Natural Language Processing , pp. 1367\u20131372, Lisbon, Portugal, Septem-\nber 2015. Association for Computational Linguistics.\nYang Liu, Chengjie Sun, Lei Lin, and Xiaolong Wang. Learning natural language inference using\nbidirectional LSTM model and inner-attention. CoRR , abs/1605.09090, 2016a.\nYang Liu, Chengjie Sun, Lei Lin, and Xiaolong Wang. Learning natural language inference using\nbidirectional lstm model and inner-attention. arXiv preprint arXiv:1605.09090 , 2016b.\nMingbo Ma, Liang Huang, Bing Xiang, and Bowen Zhou. Dependency-based convolutional neural\nnetworks for sentence embedding. In Proceedings of the 53rd Annual Meeting of the Association\nfor Computational Linguistics and the 7th International Joint Conference on Natural Language\nProcessing , volume 2, pp. 174\u2013179, 2015.\nHoria Margarit and Raghav Subramaniam. A batch-normalized recurrent network for sentiment\nclassi\ufb01cation. In Advances in Neural Information Processing Systems , 2016.\nRoland Memisevic. Learning to relate images. IEEE transactions on pattern analysis and machine\nPublished as a conference paper at ICLR 2017\nTomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Ef\ufb01cient estimation of word represen-\ntations in vector space. arXiv preprint arXiv:1301.3781 , 2013.\nLili Mou, Rui Men, Ge Li, Yan Xu, Lu Zhang, Rui Yan, and Zhi Jin. Natural language inference by\ntree-based convolution and heuristic matching. arXiv preprint arXiv:1512.08422 , 2015a.\nLili Mou, Hao Peng, Ge Li, Yan Xu, Lu Zhang, and Zhi Jin. Discriminative neural sentence model-\ning by tree-based convolution. In Proceedings of the 2015 Conference on Empirical Methods in\nNatural Language Processing , pp. 2315\u20132325, Lisbon, Portugal, September 2015b. Association\nfor Computational Linguistics. URL http://aclweb.org/anthology/D15-1279 .\nTsendsuren Munkhdalai and Hong Yu. Neural tree indexers for text understanding. arXiv preprint\narXiv:1607.04492 , 2016a.\nTsendsuren Munkhdalai and Hong Yu. Neural semantic encoders. arXiv preprint arXiv:1607.04315 ,\n2016b.\nHamid Palangi, Li Deng, Yelong Shen, Jianfeng Gao, Xiaodong He, Jianshu Chen, Xinying Song,\nand Rabab Ward. Deep sentence embedding using long short-term memory networks: Analysis\nand application to information retrieval. IEEE/ACM Transactions on Audio, Speech, and Lan-\nguage Processing , 24(4):694\u2013707, 2016.\nAnkur P. Parikh, Oscar Tackstrom, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\nmodel for natural language inference. In Proceedings of EMNLP , 2016.\nJeffrey Pennington, Richard Socher, and Christopher D Manning. Glove: Global vectors for word\nrepresentation. In EMNLP , volume 14, pp. 1532\u201343, 2014.\nRichard Socher, Jeffrey Pennington, Eric H. Huang, Andrew Y . Ng, and Christopher D. Manning.\nSemi-supervised recursive autoencoders for predicting sentiment distributions. In Proceedings\nof the 2011 Conference on Empirical Methods in Natural Language Processing , pp. 151\u2013161,\nEdinburgh, Scotland, UK., July 2011. Association for Computational Linguistics. URL http:\n//www.aclweb.org/anthology/D11-1014 .\nRichard Socher, Alex Perelygin, Jean Y Wu, Jason Chuang, Christopher D Manning, Andrew Y Ng,\nand Christopher Potts. Recursive deep models for semantic compositionality over a sentiment\ntreebank. In Proceedings of the conference on empirical methods in natural language processing\n(EMNLP) , volume 1631, pp. 1642. Citeseer, 2013.\nKai Sheng Tai, Richard Socher, and Christopher D. Manning. Improved semantic representations\nfrom tree-structured long short-term memory networks. In Proceedings of ACL , pp. 1556\u20131566,\n2015.\nMing Tan, Cicero dos Santos, Bing Xiang, and Bowen Zhou. Improved representation learning for\nquestion answer matching. In Proceedings of ACL , pp. 464\u2013473, Berlin, Germany, August 2016.\nAssociation for Computational Linguistics. URL http://www.aclweb.org/anthology/\nP16-1044 .\nTheano Development Team. Theano: A fPythongframework for fast computation of mathemati-\ncal expressions. arXiv e-prints , abs/1605.0, 2016. URL http://arxiv.org/abs/1605.\n02688 .\nIvan Vendrov, Ryan Kiros, Sanja Fidler, and Raquel Urtasun. Order-embeddings of images and\nlanguage. arXiv preprint arXiv:1511.06361 , 2015.\nWenpeng Yin and Hinrich Sch \u00a8utze. Convolutional neural network for paraphrase identi\ufb01cation.\nInProceedings of the 2015 Conference of the North American Chapter of the Association for\nPublished as a conference paper at ICLR 2017\nAPPENDIX\nA P RUNED MLP FOR STRUCTURED MATRIX SENTENCE EMBEDDING\nAs a side effect of having multiple vectors to represent a sentence, the matrix sentence embedding\nis usually several times larger than vector sentence embeddings. This results in needing more pa-\nrameters in the subsequent fully connected layer, which connects every hidden units to every units\nin the matrix sentence embedding. Actually in the example shown in Figure 1, this fully connected\nlayer takes around 90% percent of the parameters. See Table 4. In this appendix we are going to\nintroduce a weight pruning method which, by utilizing the 2D structure of matrix embedding, is able\nto drastically reduce the number of parameters in the fully connected hidden layer.\nInheriting the notation used in the main paper, let the matrix embedding Mhas a shape of rbyu,\nand let the fully connected hidden layer has bunits. The normal fully connected hidden layer will\nrequire each hidden unit to be connected to every unit in the matrix embedding, as shown in Figure\n1. This ends up with r\u0002u\u0002bparameters in total.\nHowever there are 2-D structures in the matrix embedding, which we should make use of. Each\nrow (miin Figure 1) in the matrix is computed from a weighted sum of LSTM hidden states, which\nmeans they share some similarities\nTo re\ufb02ect these similarity in the fully connected layer, we split the hidden states into requally sized\ngroups, with each group having punits. The i-th group is only fully connected to the i-th row in\nthe matrix representation. All connections that connects the i-th group hidden units to other rows\nof the matrix are pruned away. In this way, Simillarity between different rows of matrix embedding\nare re\ufb02ected as symmetry of connecting type in the hidden layer. As a result, the hidden layer can\nbe interperated as also having a 2-D structute, with the number ( r) and size ( p) of groups as its\ntwo dimensions (The Mvin Figure 6). When the total number of hidden units are the same (i.e.,\nMh Mv\nuq\nru\nMm1m2miMh Mv\nuq\np\nrru\nFigure 6: Hidden layer with pruned weight connections. Mis the matrix sentence embedding, Mv\nandMhare the structured hidden representation computed by pruned weights.\nTable 4: Model Size Comparison Before and After Pruning\nHidden layer Softmax Other Parts Total Accuracy\nYelp, Original, b=3000 54M 15K 1.3M 55.3M 64.21%\nYelp, Pruned, p=150, q=10 2.7M 52.5K 1.3M 4.1M 63.86%\nAge, Original, b=4000 72M 20K 1.3M 73.2M 80.45%\nAge, Pruned, p=25,q=20 822K 63.75K 1.3M 2.1M 77.32%\nSNLI, Original, b=4000 72M 12K 22.9M 95.0M 84.43%\nSNLI, Pruned, p=300, q=10 5.6M 45K 22.9M 28.6MPublished as a conference paper at ICLR 2017\nr\u0002p=b), this process prunes away (r\u00001)=rof weight values, which is a fairly large portion when\nris large.\nOn the other dimension, another form of similarity exists too. For each vector representation miin\nM, thej-th element mijis a weighted sum of an LSTM hidden unit at different time steps. And for\na certain j-th element in all vector representations, they are summed up from a same LSTM hidden\nunit. We can also re\ufb02ect this similarity into the symmetry of weight connections by using the same\npruning method we did above. Thus we will have another 2-D structured hidden states sized u-by-q,\nnoted as Mhin Figure 6.\nTable 4 takes the model we use for yelp dataset as a concrete example, and compared the number of\nparameters in each part of the model, both before and after pruning. We can see the above pruning\nmethod drastically reduces the model size. Note that the pandqin this structure can be adjusted\nfreely as hyperparameters. Also, we can continue the corresponding pruning process on top of Mv\nandMhover and over again, and end up with having a stack of structured hidden layers, just like\nstacking fully connected layers.\nThe subsequent softmax layer will be fully connected to both MvandMh, i.e., each unit in the\nsoftmax layer is connected to all units in MvandMh. This is not a problem since the speed of\nsoftmax is largely dependent of the number of softmax units, which is not changed.In addition, for\napplications like sentiment analysis and textural entailment, the softmax layer is so tiny that only\ncontains several units.\nExperimental results in the three datasets has shown that, this pruning mechanism lowers perfor-\nmances a bit, but still allows all three models to perform comparable or better than other models\ncompared in the paper.\nB D ETAILED STRUCTURE OF THE MODEL FOR SNLI D ATASET\nIn Section 4.3 we tested our matrix sentence embedding model for the textual entailment task on\nthe SNLI dataset. Different from the former two tasks, the textual entailment task consists of a pair\nof sentences as input. We propose to use a set of multiplicative interactions to combine the two\n.........MhMpFh FpGated EncoderHypothesis Premise Fr.........\nw1 w2 w3 wn w4Mh\n... ... w1 w2 w3 wn w4Mp\n... ...Fh FpGated Encoder\nHypothesis Premise Fr\nPublished as a conference paper at ICLR 2017\nmatrix embeddings extracted for each sentence. The form of multiplicative interaction is inspired\nbyFactored Gated Autoencoder (Memisevic, 2013).\nThe overall structure of our model for SNLI is dipicted in Figure 7. For both hypothesis and premise,\nwe extract their embeddings ( MhandMpin the \ufb01gure) independently, with a same LSTM and\nattention mechanism. The parameters of this part of model are shared (rectangles with dashed orange\nline in the \ufb01gure).\nComparing the two matrix embeddings corresponds to the green dashed rectangle part in the \ufb01gure,\nwhich computes a single matrix embedding ( Fr) as the factor of semantic relation between the two\nsentences. To represent the relation between MhandMp,Frcan be connected to MhandMp\nthrough a three-way multiplicative interaction . In a three-way multiplicative interaction, the value\nof anyone of Fr,MhandMpis a function of the product of the others. This type of connection is\noriginally introduced to extract relation between images (Memisevic, 2013). Since here we are just\ncomputing the factor of relations ( Fr) from MhandMp, it corresponds to the encoder part in the\nFactored Gated Autoencoder in Memisevic (2013). We call it Gated Encoder in Figure 7.\nFirst we multiply each row in the matrix embedding by a different weight matrix. Repeating it\nover all rows, corresponds to a batched dot product between a 2-D matrix and a 3-D weight tensor.\nInheriting the name in (Memisevic, 2013), we call the resulting matrix as factor . Doing the batched\ndot for both hypothesis embedding and premise embedding, we have FhandFp, respectively.\nFh=batcheddot (Mh; Wfh) (10)\nFp=batcheddot (Mp; Wfp) (11)\nHereWfhandWfpare the two weight tensors for hypothesis embedding and premise embedding.\nThe factor of the relation ( Fr) is just an element-wise product of FhandFp(the triangle in the\nmiddle of Figure 7):\nFr=Fh\fFp (12)\nHere\fstands for element-wise product. After the Frlayer, we then use an MLP with softmax",
        "subsection": []
    },
    {
        "missing": []
    },
    {
        "references": []
    },
    {
        "title": "A Structured Self-attentive Sentence Embedding",
        "arxiv_id": "1703.03130"
    }
]