[
    {
        "id": "",
        "section": "Abstract",
        "text": "The existing machine translation systems,\nwhether phrase-based or neural, have\nrelied almost exclusively on word-level\nmodelling with explicit segmentation. In\nthis paper, we ask a fundamental question:\ncan neural machine translation generate\na character sequence without any explicit\nsegmentation? To answer this question,\nwe evaluate an attention-based encoder\u2013\ndecoder with a subword-level encoder and\na character-level decoder on four language\npairs\u2013En-Cs, En-De, En-Ru and En-Fi\u2013\nusing the parallel corpora from WMT\u201915.\nOur experiments show that the models\nwith a character-level decoder outperform\nthe ones with a subword-level decoder on\nall of the four language pairs. Further-\nmore, the ensembles of neural models with\na character-level decoder outperform the\nstate-of-the-art non-neural machine trans-\nlation systems on En-Cs, En-De and En-Fi\nand perform comparably on En-Ru.",
        "subsection": []
    },
    {
        "id": "1",
        "section": "Introduction",
        "text": "The existing machine translation systems have re-\nlied almost exclusively on word-level modelling\nwith explicit segmentation. This is mainly due\nto the issue of data sparsity which becomes much\nmore severe, especially for n-grams, when a sen-\ntence is represented as a sequence of characters\nrather than words, as the length of the sequence\ngrows signi\ufb01cantly. In addition to data sparsity,\nwe often have a priori belief that a word, or its\nsegmented-out lexeme, is a basic unit of meaning,\nmaking it natural to approach translation as map-\nping from a sequence of source-language words to\na sequence of target-language words.\nThis has continued with the more recently\nproposed paradigm of neural machine transla-tion, although neural networks do not suffer from\ncharacter-level modelling and rather suffer from\nthe issues speci\ufb01c to word-level modelling, such\nas the increased computational complexity from a\nvery large target vocabulary (Jean et al., 2015; Lu-\nong et al., 2015b). Therefore, in this paper, we ad-\ndress a question of whether neural machine trans-\nlation can be done directly on a sequence of char-\nacters without any explicit word segmentation .\nTo answer this question, we focus on represent-\ning the target side as a character sequence. We\nevaluate neural machine translation models with\na character-level decoder on four language pairs\nfrom WMT\u201915 to make our evaluation as convinc-\ning as possible. We represent the source side as\na sequence of subwords extracted using byte-pair\nencoding from Sennrich et al. (2015), and vary the\ntarget side to be either a sequence of subwords or\ncharacters. On the target side, we further design a\nnovel recurrent neural network (RNN), called bi-\nscale recurrent network , that better handles multi-\nple timescales in a sequence, and test it in addition\nto a naive, stacked recurrent neural network.\nOn all of the four language pairs\u2013En-Cs, En-De,\nEn-Ru and En-Fi\u2013, the models with a character-\nlevel decoder outperformed the ones with a\nsubword-level decoder. We observed a similar\ntrend with the ensemble of each of these con-\n\ufb01gurations, outperforming both the previous best\nneural and non-neural translation systems on En-\nCs, En-De and En-Fi, while achieving a compara-\nble result on En-Ru. We \ufb01nd these results to be\na strong evidence that neural machine translation\ncan indeed learn to translate at the character-level\nand that in fact, it bene\ufb01ts from doing so.",
        "subsection": []
    },
    {
        "id": "2",
        "section": "Neural machine translation",
        "text": "Neural machine translation refers to a recently\ncada and \u02dcNeco, 1997; Kalchbrenner and Blunsom,\n2013; Cho et al., 2014; Sutskever et al., 2014).\nThis approach aims at building an end-to-end neu-\nral network that takes as input a source sentence\nX= (x1;:::;x Tx)and outputs its translation\nY= (y1;:::;y Ty), wherextandyt0are respec-\ntively source and target symbols. This neural net-\nwork is constructed as a composite of an encoder\nnetwork and a decoder network.\nThe encoder network encodes the input sen-\ntenceXinto its continuous representation. In\nthis paper, we closely follow the neural transla-\ntion model proposed in Bahdanau et al. (2015)\nand use a bidirectional recurrent neural network,\nwhich consists of two recurrent neural networks.\nThe forward network reads the input sentence\nin a forward direction:\u0000 !zt=\u0000 !\u001e(ex(xt);\u0000 !zt\u00001);\nwhereex(xt)is a continuous embedding of the\nt-th input symbol, and \u001eis a recurrent activa-\ntion function. Similarly, the reverse network\nreads the sentence in a reverse direction (right\nto left): \u0000zt= \u0000\u001e(ex(xt); \u0000zt+1):At each loca-\ntion in the input sentence, we concatenate the hid-\nden states from the forward and reverse RNNs\nto form a context set C=fz1;:::; zTxg;where\nzt=\u0002\u0000 !zt; \u0000zt\u0003\n.\nThen the decoder computes the conditional dis-\ntribution over all possible translations based on\nthis context set. This is done by \ufb01rst rewrit-\ning the conditional probability of a translation:\nlogp(YjX) =PTy\nt0=1logp(yt0jy<t0;X):For each\nconditional term in the summation, the decoder\nRNN updates its hidden state by\nht0=\u001e(ey(yt0\u00001);ht0\u00001;ct0); (1)\nwhereeyis the continuous embedding of a target\nsymbol. ct0is a context vector computed by a soft-\nalignment mechanism:\nct0=falign(ey(yt0\u00001);ht0\u00001;C)): (2)\nThe soft-alignment mechanism falign weights\neach vector in the context set Caccording to its\nrelevance given what has been translated. The\nweight of each vector ztis computed by\n\u000bt;t0=1\nZefscore(ey(yt0\u00001);ht0\u00001;zt); (3)\nwherefscore is a parametric function returning an\nunnormalized score for ztgiven ht0\u00001andyt0\u00001.We use a feedforward network with a single hid-\nden layer in this paper.1Zis a normalization con-\nstant:Z=PTx\nk=1efscore(ey(yt0\u00001);ht0\u00001;zk):This\nprocedure can be understood as computing the\nalignment probability between the t0-th target\nsymbol and t-th source symbol.\nThe hidden state ht0, together with the previous\ntarget symbol yt0\u00001and the context vector ct0, is\nfed into a feedforward neural network to result in\nthe conditional distribution:\np(yt0jy<t0;X)/efyt0\nout(ey(yt0\u00001);ht0;ct0):(4)\nThe whole model, consisting of the encoder,\ndecoder and soft-alignment mechanism, is then\ntuned end-to-end to minimize the negative log-\nlikelihood using stochastic gradient descent.",
        "subsection": [
            {
                "id": "3.1",
                "section": "Motivation",
                "text": "Let us revisit how the source and target sen-\ntences (XandY) are represented in neural ma-\nchine translation. For the source side of any given\ntraining corpus, we scan through the whole cor-\npus to build a vocabulary Vxof unique tokens to\nwhich we assign integer indices. A source sen-\ntenceXis then built as a sequence of the indices\nof such tokens belonging to the sentence, i.e.,\nX= (x1;:::;x Tx), wherext2f1;2;:::;jVxjg.\nThe target sentence is similarly transformed into a\ntarget sequence of integer indices.\nEach token, or its index, is then transformed\ninto a so-called one-hot vector of dimensionality\njVxj. All but one elements of this vector are set to",
                "subsection": []
            },
            {
                "id": "3.2",
                "section": "Why word-level translation",
                "text": "(1) Word as a Basic Unit of Meaning A word\ncan be understood in two different senses. In the\nabstract sense, a word is a basic unit of mean-\ning (lexeme), and in the other sense, can be un-\nderstood as a \u201cconcrete word as used in a sen-\ntence.\u201d (Booij, 2012). A word in the former sense\nturns into that in the latter sense via a process\nof morphology, including in\ufb02ection, compound-\ning and derivation. These three processes do al-\nter the meaning of the lexeme, but often it stays\nclose to the original meaning. Because of this\nview of words as basic units of meaning (either\nin the form of lexemes or derived form) from lin-\nguistics, much of previous work in natural lan-\nguage processing has focused on using words as\nbasic units of which a sentence is encoded as a\nsequence. Also, the potential dif\ufb01culty in \ufb01nding\na mapping between a word\u2019s character sequence\nand meaning3has likely contributed to this trend\ntoward word-level modelling.\n(2) Data Sparsity There is a further technical\nreason why much of previous research on ma-\nchine translation has considered words as a ba-\nsic unit. This is mainly due to the fact that ma-\njor components in the existing translation systems,\nsuch as language models and phrase tables, are a\ncount-based estimator of probabilities. In other\nwords, a probability of a subsequence of sym-\nbols, or pairs of symbols, is estimated by count-\ning the number of its occurrences in a training\ncorpus. This approach severely suffers from the\nissue of data sparsity, which is due to a large\nstate space which grows exponentially w.r.t. the\nlength of subsequences while growing only lin-\nearly w.r.t. the corpus size. This poses a great chal-\nlenge to character-level modelling, as any subse-\nquence will be on average 4\u20135 times longer when\ncharacters, instead of words, are used. Indeed,\nVilar et al. (2007) reported worse performance\nwhen the character sequence was directly used by\na phrase-based machine translation system. More\n3For instance, \u201cquit\u201d, \u201cquite\u201d and \u201cquiet\u201d are one edit-\nrecently, Neubig et al. (2013) proposed a method\nto improve character-level translation with phrase-\nbased translation systems, however, with only a\nlimited success.\n(3) Vanishing Gradient Speci\ufb01cally to neural\nmachine translation, a major reason behind the\nwide adoption of word-level modelling is due to\nthe dif\ufb01culty in modelling long-term dependen-\ncies with recurrent neural networks (Bengio et al.,\n1994; Hochreiter, 1998). As the lengths of the\nsentences on both sides grow when they are repre-\nsented in characters, it is easy to believe that there\nwill be more long-term dependencies that must be\ncaptured by the recurrent neural network for suc-\ncessful translation.",
                "subsection": []
            },
            {
                "id": "3.3",
                "section": "Why character-level translation",
                "text": "Why notWord-Level Translation? The most\npressing issue with word-level processing is that\nwe do not have a perfect word segmentation al-\ngorithm for any one language. A perfect segmen-\ntation algorithm needs to be able to segment any\ngiven sentence into a sequence of lexemes and\nmorphemes. This problem is however a dif\ufb01cult\nproblem on its own and often requires decades of\nresearch (see, e.g., Creutz and Lagus (2005) for\nFinnish and other morphologically rich languages\nand Huang and Zhao (2007) for Chinese). There-\nfore, many opt to using either a rule-based tok-\nenization approach or a suboptimal, but still avail-\nable, learning based segmentation algorithm.\nThe outcome of this naive, sub-optimal segmen-\ntation is that the vocabulary is often \ufb01lled with\nmany similar words that share a lexeme but have\ndifferent morphology. For instance, if we apply\na simple tokenization script to an English corpus,\n\u201crun\u201d, \u201cruns\u201d, \u201cran\u201d and \u201crunning\u201d are all separate\nentries in the vocabulary, while they clearly share\nthe same lexeme \u201crun\u201d. This prevents any ma-\nchine translation system, in particular neural ma-\nchine translation, from modelling these morpho-\nlogical variants ef\ufb01ciently.\nMore speci\ufb01cally in the case of neural machine\ntranslation, each of these morphological variants\u2013\n\u201crun\u201d, \u201cruns\u201d, \u201cran\u201d and \u201crunning\u201d\u2013 will be as-\nsigned ad-dimensional word vector, leading to\nfour independent vectors, while it is clear that if\nwe can segment those variants into a lexeme and\nother morphemes, we can model them more ef\ufb01-\nciently. For instance, we can have a d-dimensional\nvector for the lexeme \u201crun\u201d and much smallervectors for \u201cs\u201d and\u201cing\u201d. Each of those variants\nwill be then a composite of the lexeme vector\n(shared across these variants) and morpheme vec-\ntors (shared across words sharing the same suf\ufb01x,\nfor example) (Botha and Blunsom, 2014). This\nmakes use of distributed representation, which\ngenerally yields better generalization, but seems\nto require an optimal segmentation, which is un-\nfortunately almost never available.\nIn addition to inef\ufb01ciency in modelling, there\nare two additional negative consequences from us-\ning (unsegmented) words. First, the translation\nsystem cannot generalize well to novel words,\nwhich are often mapped to a token reserved for\nan unknown word. This effectively ignores any\nmeaning or structure of the word to be incorpo-\nrated when translating. Second, even when a lex-\neme is common and frequently observed in the\ntraining corpus, its morphological variant may not\nbe. This implies that the model sees this speci\ufb01c,\nrare morphological variant much less and will not\nbe able to translate it well. However, if this rare\nmorphological variant shares a large part of its\nspelling with other more common words, it is de-\nsirable for a machine translation system to exploit\nthose common words when translating those rare\nvariants.\nWhy Character-Level Translation? All of\nthese issues can be addressed to certain extent by\ndirectly modelling characters. Although the issue\nof data sparsity arises in character-level transla-\ntion, it is elegantly addressed by using a paramet-\nric approach based on recurrent neural networks\ninstead of a non-parametric count-based approach.\nFurthermore, in recent years, we have learned how\nto build and train a recurrent neural network that\ncan well capture long-term dependencies by using\nmore sophisticated activation functions, such as\nlong short-term memory (LSTM) units (Hochre-\niter and Schmidhuber, 1997) and gated recurrent\nunits (Cho et al., 2014).\nKim et al. (2015) and Ling et al. (2015a) re-\ncently showed that by having a neural network that\nconverts a character sequence into a word vector,\nwe avoid the issues from having many morpho-\nlogical variants appearing as separate entities in\na vocabulary. This is made possible by sharing\nthe character-to-word neural network across all the\nunique tokens. A similar approach was applied to\nmachine translation by Ling et al. (2015b).\nthe availability of a good, if not optimal, segmen-\ntation algorithm. Ling et al. (2015b) indeed states\nthat \u201c[m]uch of the prior information regarding\nmorphology, cognates and rare word translation\namong others, should be incorporated\u201d.\nIt however becomes unnecessary to consider\nthese prior information, if we use a neural net-\nwork, be it recurrent, convolution or their combi-\nnation, directly on the unsegmented character se-\nquence. The possibility of using a sequence of un-\nsegmented characters has been studied over many\nyears in the \ufb01eld of deep learning. For instance,\nMikolov et al. (2012) and Sutskever et al. (2011)\ntrained a recurrent neural network language model\n(RNN-LM) on character sequences. The latter\nshowed that it is possible to generate sensible text\nsequences by simply sampling a character at a\ntime from this model. More recently, Zhang et\nal. (2015) and Xiao and Cho (2016) successfully\napplied a convolutional net and a convolutional-\nrecurrent net respectively to character-level docu-\nment classi\ufb01cation without any explicit segmenta-\ntion. Gillick et al. (2015) further showed that it\nis possible to train a recurrent neural network on\nunicode bytes, instead of characters or words, to\nperform part-of-speech tagging and named entity\nrecognition.\nThese previous works suggest the possibility of\napplying neural networks for the task of machine\ntranslation, which is often considered a substan-\ntially more dif\ufb01cult problem compared to docu-\nment classi\ufb01cation and language modelling.",
                "subsection": []
            },
            {
                "id": "3.4",
                "section": "Challenges and questions",
                "text": "There are two overlapping sets of challenges for\nthe source and target sides. On the source side, it\nis unclear how to build a neural network that learns\na highly nonlinear mapping from a spelling to the\nmeaning of a sentence.\nOn the target side, there are two challenges. The\n\ufb01rst challenge is the same one from the source\nside, as the decoder neural network needs to sum-\nmarize what has been translated. In addition to\nthis, the character-level modelling on the target\nside is more challenging, as the decoder network\nmust be able to generate a long, coherent sequence\nof characters. This is a great challenge, as the size\nof the state space grows exponentially w.r.t. the\nnumber of symbols, and in the case of characters,\nit is often 300-1000 symbols long.\nAll these challenges should \ufb01rst be framed as\n(a) Gating units (b) One-step processing\nFigure 1: Bi-scale recurrent neural network\nquestions; whether the current recurrent neural\nnetworks, which are already widely used in neu-\nral machine translation, are able to address these\nchallenges as they are. In this paper, we aim at an-\nswering these questions empirically and focus on\nthe challenges on the target side (as the target side\nshows both of the challenges).",
                "subsection": []
            }
        ]
    },
    {
        "id": "3",
        "section": "Towards character-level translation",
        "text": "",
        "subsection": [
            {
                "id": "4.1",
                "section": "Bi-scale recurrent neural network",
                "text": "In this proposed bi-scale recurrent neural network,\nthere are two sets of hidden units, h1andh2. They\ncontain the same number of units, i.e., dim (h1) =\ndim(h2). The \ufb01rst set h1models a fast-changing\ntimescale (thereby, a faster layer ), and h2a slower\ntimescale (thereby, a slower layer ). For each hid-\nwhich we refer by g1andg2. For the descrip-\ntion below, we use yt0\u00001andct0for the previous\ntarget symbol and the context vector (see Eq. (2)),\nrespectively.\nLet us start with the faster layer. The faster layer\noutputs two sets of activations, a normal output h1\nt0\nand its gated version \u0014h1\nt0. The activation of the\nfaster layer is computed by\nh1\nt0= tanh\u0010\nWh1h\ney(yt0\u00001);\u0014h1\nt0\u00001;^h2\nt0\u00001;ct0i\u0011\n;\nwhere \u0014h1\nt0\u00001and^h2\nt0\u00001are the gated activations of\nthe faster and slower layers respectively. These\ngated activations are computed by\n\u0014h1\nt0= (1\u0000g1\nt0)\fh1\nt0;^h2\nt0=g1\nt0\fh2\nt0:\nIn other words, the faster layer\u2019s activation is\nbased on the adaptive combination of the faster\nand slower layers\u2019 activations from the previous\ntime step. Whenever the faster layer determines\nthat it needs to reset, i.e., g1\nt0\u00001\u00191, the next\nactivation will be determined based more on the\nslower layer\u2019s activation.\nThe faster layer\u2019s gating unit is computed by\ng1\nt0=\u001b\u0010\nWg1h\ney(yt0\u00001);\u0014h1\nt0\u00001;^h2\nt0\u00001;ct0i\u0011\n;\nwhere\u001bis a sigmoid function.\nThe slower layer also outputs two sets of acti-\nvations, a normal output h2\nt0and its gated version\n\u0014h2\nt0. These activations are computed as follows:\nh2\nt0= (1\u0000g1\nt0)\fh2\nt0\u00001+g1\nt0\f~h2\nt0;\n\u0014h2\nt0= (1\u0000g2\nt0)\fh2\nt0;\nwhere ~h2\nt0is a candidate activation. The slower\nlayer\u2019s gating unit g2\nt0is computed by\ng2\nt0=\u001b\u0010\nWg2\u0002\n(g1\nt0\fh1\nt0);\u0014h2\nt0\u00001;ct0\u0003\u0011\n:\nThis adaptive leaky integration based on the gat-\ning unit from the faster layer has a consequence\nthat the slower layer updates its activation only\nwhen the faster layer resets. This puts a soft con-\nstraint that the faster layer runs at a faster rate by\npreventing the slower layer from updating while\nthe faster layer is processing a current chunk.\nThe candidate activation is then computed by\n~h2\nt0= tanh\u0010\nWh2\u0002\n(g1\nt0\fh1\nt0);\u0014h2\nt0\u00001;ct0\u0003\u0011\n:(5)\nFigure 2: (left) The BLEU scores on En-Cs\nw.r.t. the length of source sentences. (right) The\ndifference of word negative log-probabilities be-\ntween the subword-level decoder and either of the\ncharacter-level base or bi-scale decoder.\n\u0014h2\nt0\u00001indicates the reset activation from the pre-\nvious time step, similarly to what happened in the\nfaster layer, and ct0is the input from the context.\nAccording to g1\nt0\fh1\nt0in Eq. (5), the faster layer\nin\ufb02uences the slower layer, only when the faster\nlayer has \ufb01nished processing the current chunk\nand is about to reset itself ( g1\nt0\u00191). In other\nwords, the slower layer does not receive any in-\nput from the faster layer, until the faster layer has\nquickly processed the current chunk, thereby run-\nning at a slower rate than the faster layer does.\nAt each time step, the \ufb01nal output of the pro-\nposed bi-scale recurrent neural network is the con-\ncatenation of the output vectors of the faster and\nslower layers, i.e.,\u0002\nh1;h2\u0003\n. This concatenated\nvector is used to compute the probability distribu-\ntion over all the symbols in the vocabulary, as in\nEq. (4). See Fig. 1 for graphical illustration.",
                "subsection": []
            }
        ]
    },
    {
        "id": "0.",
        "section": "The only element whose index corresponds to",
        "text": "the token\u2019s index is set to 1. This one-hot vector\nis the one which any neural machine translation\nmodel sees. The embedding function, exorey, is\nsimply the result of applying a linear transforma-\ntion (the embedding matrix) to this one-hot vector.\nThe important property of this approach based\non one-hot vectors is that the neural network is\noblivious to the underlying semantics of the to-\nkens. To the neural network, each and every token\nin the vocabulary is equal distance away from ev-\nery other token. The semantics of those tokens are\nsimply learned (into the embeddings) to maximize\nthe translation quality, or the log-likelihood of the\nmodel.\nThis property allows us great freedom in the\nchoice of tokens\u2019 unit. Neural networks have been\n1shown to work well with word tokens (Bengio et\nal., 2001; Schwenk, 2007; Mikolov et al., 2010)\nbut also with \ufb01ner units, such as subwords (Sen-\nnrich et al., 2015; Botha and Blunsom, 2014; Lu-\nong et al., 2013) as well as symbols resulting\nfrom compression/encoding (Chitnis and DeNero,\n2015). Although there have been a number of\nprevious research reporting the use of neural net-\nworks with characters (see, e.g., Mikolov et al.\n(2012) and Santos and Zadrozny (2014)), the dom-\ninant approach has been to preprocess the text into\na sequence of symbols, each associated with a se-\nquence of characters, after which the neural net-\nwork is presented with those symbols rather than\nwith characters.\nMore recently in the context of neural machine\ntranslation, two research groups have proposed to\ndirectly use characters. Kim et al. (2015) proposed\nto represent each word not as a single integer index\nas before, but as a sequence of characters, and use\na convolutional network followed by a highway\nnetwork (Srivastava et al., 2015) to extract a con-\ntinuous representation of the word. This approach,\nwhich effectively replaces the embedding func-\ntionex, was adopted by Costa-Juss `a and Fonollosa\n(2016) for neural machine translation. Similarly,\nLing et al. (2015b) use a bidirectional recurrent\nneural network to replace the embedding functions\nexandeyto respectively encode a character se-\nquence to and from the corresponding continuous\nword representation. A similar, but slightly differ-\nent approach was proposed by Lee et al. (2015),\nwhere they explicitly mark each character with its\nrelative location in a word (e.g., \u201cB\u201deginning and\n\u201cI\u201dntermediate).\nDespite the fact that these recent approaches\nwork at the level of characters, it is less satisfying\nthat they all rely on knowing how to segment char-\nacters into words. Although it is generally easy\nfor languages like English, this is not always the\ncase. This word segmentation procedure can be\nas simple as tokenization followed by some punc-\ntuation normalization, but also can be as compli-\ncated as morpheme segmentation requiring a sep-\narate model to be trained in advance (Creutz and\nLagus, 2005; Huang and Zhao, 2007). Further-\nmore, these segmentation2steps are often tuned\nor designed separately from the ultimate objective\nof translation quality, potentially contributing to a\n2From here on, the term segmentation broadly refers to\nany method that splits a given character sequence into a se-\nquence of subword symbols.suboptimal quality.\nBased on this observation and analysis, in this\npaper, we ask ourselves and the readers a question\nwhich should have been asked much earlier: Is it\npossible to do character-level translation without\nany explicit segmentation?",
        "subsection": []
    },
    {
        "id": "4",
        "section": "Character-level translation",
        "text": "In this paper, we try to answer the questions posed\nearlier by testing two different types of recurrent\nneural networks on the target side (decoder).\nFirst, we test an existing recurrent neural net-\nwork with gated recurrent units (GRUs). We call\nthis decoder a base decoder.\nSecond, we build a novel two-layer recurrent\nneural network, inspired by the gated-feedback\nnetwork from Chung et al. (2015), called a bi-\nscale recurrent neural network. We design this\nnetwork to facilitate capturing two timescales, mo-\ntivated by the fact that characters and words may\nwork at two separate timescales.\nWe choose to test these two alternatives for the\nfollowing purposes. Experiments with the base\ndecoder will clearly answer whether the existing\nneural network is enough to handle character-level\ndecoding, which has not been properly answered\nin the context of machine translation. The alterna-\ntive, the bi-scale decoder, is tested in order to see\nwhether it is possible to design a better decoder, if\nthe answer to the \ufb01rst question is positive.",
        "subsection": []
    },
    {
        "id": "5",
        "section": "Experiment settings",
        "text": "For evaluation, we represent a source sentence as\na sequence of subword symbols extracted by byte-\npair encoding (BPE, Sennrich et al. (2015)) and a\ntarget sentence either as a sequence of BPE-based\nsymbols or as a sequence of characters.\nCorpora and Preprocessing We use all avail-\nable parallel corpora for four language pairs from\nWMT\u201915: En-Cs, En-De, En-Ru and En-Fi. They\nconsist of 12.1M, 4.5M, 2.3M and 2M sentence\npairs, respectively. We tokenize each corpus using\na tokenization script included in Moses.4We only\nuse the sentence pairs, when the source side is up\nto 50 subword symbols long and the target side is\neither up to 100 subword symbols or 500 charac-\nters. We do not use any monolingual corpus.\n4Although tokenization is not necessary for character-\nlevel modelling, we tokenize the all target side corpora to\nSrcDepthAttention\nModelDevelopment Test 1 Test 2\nTrgt h1h2Single Ens Single Ens Single EnsEn-De(a)\nBPEBPE1 DBase20.78 \u2013 19.98 \u2013 21.72 \u2013\n(b) 2 D D 21:2621:45\n20:62 23.49 20:4720:88\n19:30 23.10 22:0222:21\n21:35 24.83\n(c)\nChar2 DBase21:5721:88\n20:88 23.14 21 :3321:56\n19:82 23.11 23 :4523:91\n21:72 25.24\n(d) 2 D D 20.31 \u2013 19.70 \u2013 21.30 \u2013\n(e) 2 D\nBi-S21:2921:43\n21:13 23.05 21:2521:47\n20:62 23.04 23:0623:47\n22:85 25.44\n(f) 2 D D 20.78 \u2013 20.19 \u2013 22.26 \u2013\n(g) 2 D 20.08 \u2013 19.39 \u2013 20.94 \u2013\nState-of-the-art Non-Neural Approach\u0003\u2013 20.60(1)24.00(2)En-Cs(h)\nBPEBPE 2 D D Base 16:1216:96\n15:96 19.21 17:1617:68\n16:38 20.79 14:6315:09\n14:26 17.61\n(i)Char2 D Base 17:6817:78\n17:39 19.52 19:2519:55\n18:89 21.95 16 :9817:17\n16:81 18.92\n(j) 2 D Bi-S 17:6217:93\n17:43 19.83 19 :2719:53\n19:15 22.15 16:8617:10\n16:68 18.93\nState-of-the-art Non-Neural Approach\u0003\u2013 21.00(3)18.20(4)En-Ru(k)\nBPEBPE 2 D D Base 18:5618:70\n18:26 21.17 25:3025:40\n24:95 29.26 19:7220:29\n19:02 22.96\n(l)Char2 D Base 18:5618:87\n18:39 20.53 26 :0026:07\n25:04 29.37 21 :1021:24\n20:14 23.51\n(m) 2 D Bi-S 18:3018:54\n17:88 20.53 25:5925:76\n24:57 29.26 20:7321:02\n19:97 23.75\nState-of-the-art Non-Neural Approach\u0003\u2013 28.70(5)24.30(6)En-Fi(n)\nBPEBPE 2 D D Base 9:6110:02\n9:24 11.92 \u2013 \u2013 8:979:17\n8:88 11.73\n(o)Char2 D Base 11:1911:55\n11:09 13.72 \u2013 \u2013 10 :9311:56\n10:11 13.48\n(p) 2 D Bi-S 10:7311:04\n10:40 13.39 \u2013 \u2013 10:2410:63\n9:71 13.32\nState-of-the-art Non-Neural Approach\u0003\u2013 \u2013 12.70(7)\nTable 1: BLEU scores of the subword-level, character-level base and character-level bi-scale decoders\nfor both single models and ensembles. The best scores among the single models per language pair\nare bold-faced, and those among the ensembles are underlined. When available, we report the median\nvalue, and the minimum and maximum values as a subscript and a superscript, respectively. (\u0003)http:\n//matrix.statmt.org/ as of 11 March 2016 (constrained only). (1) Freitag et al. (2014). (2, 6) Williams et al. (2015).\n(3, 5) Durrani et al. (2014). (4) Haddow et al. (2015). (7) Rubino et al. (2015).\nFor all the pairs other than En-Fi, we use\nnewstest-2013 as a development set, and newstest-\n2014 (Test 1) and newstest-2015 (Test 2) as test sets.\nFor En-Fi, we use newsdev-2015 and newstest-\n2015 as development and test sets, respectively.\nModels and Training We test three models set-\ntings: (1) BPE!BPE, (2) BPE!Char (base) and\n(3) BPE!Char (bi-scale). The latter two differ by\nthe type of recurrent neural network we use. We\nuse GRUs for the encoder in all the settings. We\nused GRUs for the decoders in the \ufb01rst two set-\ntings, (1) and (2), while the proposed bi-scale re-\ncurrent network was used in the last setting, (3).\nThe encoder has 512hidden units for each direc-\ntion (forward and reverse), and the decoder has\n1024 hidden units per layer.\nWe train each model using stochastic gradient\ndescent with Adam (Kingma and Ba, 2014). Each\nupdate is computed using a minibatch of 128 sen-\ntence pairs. The norm of the gradient is clipped\nwith a threshold 1(Pascanu et al., 2013).\nDecoding and Evaluation We use beamsearch\nto approximately \ufb01nd the most likely translationgiven a source sentence. The beam widths are\n5and15respectively for the subword-level and\ncharacter-level decoders. They were chosen based\non the translation quality on the development set.\nThe translations are evaluated using BLEU.5\nMultilayer Decoder and Soft-Alignment Mech-\nanism When the decoder is a multilayer re-\ncurrent neural network (including a stacked net-\nwork as well as the proposed bi-scale network),\nthe decoder outputs multiple hidden vectors\u2013\b\nh1;:::; hL\t\nforLlayers, at a time. This allows\nan extra degree of freedom in the soft-alignment\nmechanism ( fscore in Eq. (3)). We evaluate using\nalternatives, including (1) using only hL(slower\nlayer) and (2) using all of them (concatenated).\nEnsembles We also evaluate an ensemble of\nneural machine translation models and compare\nits performance against the state-of-the-art phrase-\nbased translation systems on all four language\npairs. We decode from an ensemble by taking the\naverage of the output probabilities at each step.\n5Figure 3: Alignment matrix of a test example from En-De using the BPE !Char (bi-scale) model.",
        "subsection": []
    },
    {
        "id": "6",
        "section": "Quantitative analysis",
        "text": "Slower Layer for Alignment On En-De, we\ntest which layer of the decoder should be used\nfor computing soft-alignments. In the case of\nsubword-level decoder, we observed no difference\nbetween choosing any of the two layers of the de-\ncoder against using the concatenation of all the\nlayers (Table 1 (a\u2013b)) On the other hand, with the\ncharacter-level decoder, we noticed an improve-\nment when only the slower layer ( h2) was used\nfor the soft-alignment mechanism (Table 1 (c\u2013g)).\nThis suggests that the soft-alignment mechanism\nbene\ufb01ts by aligning a larger chunk in the target\nwith a subword unit in the source, and we use only\nthe slower layer for all the other language pairs.\nSingle Models In Table 1, we present a com-\nprehensive report of the translation qualities of\n(1) subword-level decoder, (2) character-level base\ndecoder and (3) character-level bi-scale decoder,\nfor all the language pairs. We see that the both\ntypes of character-level decoder outperform the\nsubword-level decoder for En-Cs and En-Fi quite\nsigni\ufb01cantly. On En-De, the character-level base\ndecoder outperforms both the subword-level de-\ncoder and the character-level bi-scale decoder,\nvalidating the effectiveness of the character-level\nmodelling. On En-Ru, among the single mod-\nels, the character-level decoders outperform the\nsubword-level decoder, but in general, we observe\nthat all the three alternatives work comparable to\neach other.\nThese results clearly suggest that it is indeed\npossible to do character-level translation without\nexplicit segmentation. In fact, what we observed is\nthat character-level translation often surpasses the\ntranslation quality of word-level translation. Of\ncourse, we note once again that our experiment is\nrestricted to using an unsegmented character se-\nquence at the decoder only, and a further explo-\nration toward replacing the source sentence with\nan unsegmented character sequence is needed.Ensembles Each ensemble was built using eight\nindependent models. The \ufb01rst observation we\nmake is that in all the language pairs, neural ma-\nchine translation performs comparably to, or often\nbetter than, the state-of-the-art non-neural transla-\ntion system. Furthermore, the character-level de-\ncoders outperform the subword-level decoder in\nall the cases.",
        "subsection": []
    },
    {
        "id": "7",
        "section": "Qualitative analysis",
        "text": "(1) Can the character-level decoder generate\na long, coherent sentence? The translation in\ncharacters is dramatically longer than that in\nwords, likely making it more dif\ufb01cult for a recur-\nrent neural network to generate a coherent sen-\ntence in characters. This belief turned out to be\nfalse. As shown in Fig. 2 (left), there is no sig-\nni\ufb01cant difference between the subword-level and\ncharacter-level decoders, even though the lengths\nof the generated translations are generally 5\u201310\ntimes longer in characters.\n(2) Does the character-level decoder help with\nrare words? One advantage of character-level\nmodelling is that it can model the composition of\nany character sequence, thereby better modelling\nrare morphological variants. We empirically con-\n\ufb01rm this by observing the growing gap in the aver-\nage negative log-probability of words between the\nsubword-level and character-level decoders as the\nfrequency of the words decreases. This is shown\nin Fig. 2 (right) and explains one potential cause\nbehind the success of character-level decoding in\nour experiments (we de\ufb01ne di\u000b(x;y) =x\u0000y).\n(3) Can the character-level decoder soft-align\nbetween a source word and a target charac-\nter? In Fig. 3 (left), we show an example soft-\nalignment of a source sentence, \u201cTwo sets of light\nso close to one another\u201d. It is clear that the\ncharacter-level translation model well captured the\nget characters. We observe that the character-\nlevel decoder correctly aligns to \u201clights\u201d and \u201csets\nof\u201d when generating a German compound word\n\u201cLichtersets\u201d (see Fig. 3 (right) for the zoomed-\nin version). This type of behaviour happens simi-\nlarly between \u201cone another\u201d and \u201ceinander\u201d. Of\ncourse, this does not mean that there exists an\nalignment between a source word and a target\ncharacter. Rather, this suggests that the internal\nstate of the character-level decoder, the base or bi-\nscale, well captures the meaningful chunk of char-\nacters, allowing the model to map it to a larger\nchunk (subword) in the source.\n(4) How fast is the decoding speed of the\ncharacter-level decoder? We evaluate the de-\ncoding speed of subword-level base, character-\nlevel base and character-level bi-scale decoders on\nnewstest-2013 corpus (En-De) with a single Titan\nX GPU. The subword-level base decoder gener-\nates 31.9 words per second, and the character-level\nbase decoder and character-level bi-scale decoder\ngenerate 27.5 words per second and 25.6 words\nper second, respectively. Note that this is evalu-\nated in an online setting, performing consecutive\ntranslation, where only one sentence is translated\nat a time. Translating in a batch setting could dif-\nfer from these results.",
        "subsection": []
    },
    {
        "id": "8",
        "section": "Conclusion",
        "text": "In this paper, we addressed a fundamental ques-\ntion on whether a recently proposed neural ma-\nchine translation system can directly handle trans-\nlation at the level of characters without any word\nsegmentation. We focused on the target side, in\nwhich a decoder was asked to generate one char-\nacter at a time, while soft-aligning between a tar-\nget character and a source subword. Our extensive\nexperiments, on four language pairs\u2013En-Cs, En-\nDe, En-Ru and En-Fi\u2013 strongly suggest that it is\nindeed possible for neural machine translation to\ntranslate at the level of characters, and that it actu-\nally bene\ufb01ts from doing so.\nOur result has one limitation that we used sub-\nword symbols in the source side. However, this\nhas allowed us a more \ufb01ne-grained analysis, but in\nthe future, a setting where the source side is also\nrepresented as a character sequence must be inves-\ntigated.Acknowledgments\nThe authors would like to thank the developers\nof Theano (Team et al., 2016). We acknowledge\nthe support of the following agencies for research\nfunding and computing support: NSERC, Calcul\nQu\u00b4ebec, Compute Canada, the Canada Research\nChairs, CIFAR and Samsung. KC thanks the sup-\nport by Facebook, Google (Google Faculty Award\n2016) and NVIDIA (GPU Center of Excellence\n2015-2016). JC thanks Orhan Firat for his con-\nstructive feedbacks.",
        "subsection": []
    },
    {
        "missing": []
    },
    {
        "references": []
    },
    {
        "title": "A Character-Level Decoder without Explicit Segmentation for Neural\n  Machine Translation",
        "arxiv_id": "1603.06147"
    }
]