[
    {
        "id": "",
        "section": "Abstract",
        "text": "We study the topmost weight matrix of\nneural network language models. We\nshow that this matrix constitutes a valid\nword embedding. When training language\nmodels, we recommend tying the input\nembedding and this output embedding.\nWe analyze the resulting update rules and\nshow that the tied embedding evolves in\na more similar way to the output embed-\nding than to the input embedding in the\nuntied model. We also offer a new method\nof regularizing the output embedding. Our\nmethods lead to a signi",
        "subsection": []
    },
    {
        "id": "1",
        "section": "Introduction",
        "text": "In a common family of neural network language\nmodels, the current input word is represented as\nthe vector c\u2208IRCand is projected to a dense\nrepresentation using a word embedding matrix U.\nSome computation is then performed on the word\nembedding U\u22a4c, which results in a vector of ac-\ntivationsh2. A second matrix Vthen projects h2\nto a vector h3containing one score per vocabulary\nword:h3=Vh2. The vector of scores is then con-\nverted to a vector of probability values p, which\nrepresents the models\u2019 prediction of the next word,\nusing the softmax function.\nFor example, in the LSTM-based lan-\nguage models of (Sundermeyer et al., 2012;\nZaremba et al., 2014), for vocabulary of size C,\nthe one-hot encoding is used to represent the input\ncandU\u2208IRC\u00d7H. An LSTM is then employed,which results in an activation vector h2that\nsimilarly to U\u22a4c, is also in IRH. In this case, U\nandVare of exactly the same size.\nWe callUthe input embedding, and Vthe\noutput embedding. In both matrices, we expect\nrows that correspond to similar words to be sim-\nilar: for the input embedding, we would like the\nnetwork to react similarly to synonyms, while in\nthe output embedding, we would like the scores\nof words that are interchangeable to be simi-\nlar (Mnih and Teh, 2012).\nWhileUandVcan both serve as word embed-\ndings, in the literature, only the former serves this\nrole. In this paper, we compare the quality of the\ninput embedding to that of the output embedding,\nand we show that the latter can be used to improve\nneural network language models. Our main results\nare as follows: (i) We show that in the word2vec\nskip-gram model, the output embedding is only\nslightly inferior to the input embedding. This is\nshown using metrics that are commonly used in or-\nder to measure embedding quality. (ii) In recurrent\nneural network based language models, the output\nembedding outperforms the input embedding. (iii)\nBy tying the two embeddings together, i.e., enforc-\ningU=V, the joint embedding evolves in a more\nsimilar way to the output embedding than to the in-\nput embedding of the untied model. (iv) Tying the\ninput and output embeddings leads to an improve-\nment in the perplexity of various language mod-\nels. This is true both when using dropout or when\nnot using it. (v) When not using dropout, we pro-\npose adding an additional projection PbeforeV,\nand apply regularization to P. (vi) Weight tying\nin neural translation models can reduce their size\n(number of parameters) to less than half of their\n2 Related Work\nNeural network language models (NNLMs)\nassign probabilities to word sequences. Their\nresurgence was initiated by (Bengio et al., 2003).\nRecurrent neural networks were \ufb01rst used for\nlanguage modeling in (Mikolov et al., 2010)\nand (Pascanu et al., 2013). The \ufb01rst model\nthat implemented language modeling with\nLSTMs (Hochreiter and Schmidhuber, 1997)\nwas (Sundermeyer et al., 2012). Follow-\ning that, (Zaremba et al., 2014) introduced\na dropout (Srivastava, 2013) augmented\nNNLM. (Gal, 2015; Gal and Ghahramani, 2016)\nproposed a new dropout method, which is referred\nto as Bayesian Dropout below, that improves on\nthe results of (Zaremba et al., 2014).\nThe skip-gram word2vec model introduced\nin (Mikolov et al., 2013a; Mikolov et al., 2013b)\nlearns representations of words. This model learns\na representation for each word in its vocabulary,\nboth in an input embedding matrix and in an\noutput embedding matrix. When training is\ncomplete, the vectors that are returned are the\ninput embeddings. The output embedding is\ntypically ignored, although (Mitra et al., 2016;\nMnih and Kavukcuoglu, 2013) use both the\noutput and input embeddings of words\nin order to compute word similarity. Re-\ncently, (Goldberg and Levy, 2014) argued that\nthe output embedding of the word2vec skip-\ngram model needs to be different than the input\nembedding.\nAs we show, tying the input and the output em-\nbeddings is indeed detrimental in word2vec. How-\never, it improves performance in NNLMs.\nIn neural machine translation (NMT)\nmodels (Kalchbrenner and Blunsom, 2013;\nCho et al., 2014; Sutskever et al., 2014;\nBahdanau et al., 2014), the decoder, which\ngenerates the translation of the input sentence in\nthe target language, is a language model that is\nconditioned on both the previous words of the out-\nput sentence and on the source sentence. State of\nthe art results in NMT have recently been achieved\nby systems that segment the source and target\nwords into subword units (Sennrich et al., 2016a).\nOne such method (Sennrich et al., 2016b) is based\non the byte pair encoding (BPE) compression al-\ngorithm (Gage, 1994). BPE segments rare words\ninto their more commonly appearing subwords.\nWeight tying was previously used in the log-bilinear model of (Mnih and Hinton, 2009), but\nthe decision to use it was not explained, and\nits effect on the model\u2019s performance was not\ntested. Independently and concurrently with\nour work (Inan et al., 2016) presented an ex-\nplanation for weight tying in NNLMs based\non (Hinton et al., 2015).",
        "subsection": []
    },
    {
        "id": "3",
        "section": "Weight tying",
        "text": "In this work, we employ three different model cat-\negories: NNLMs, the word2vec skip-gram model,\nand NMT models. Weight tying is applied sim-\nilarly in all models. For translation models, we\nalso present a three-way weight tying method.\nNNLM models contain an input embedding ma-\ntrix, two LSTM layers ( h1andh2), a third hidden\nscores/logits layer h3, and a softmax layer. The\nloss used during training is the cross entropy loss\nwithout any regularization terms.\nFollowing (Zaremba et al., 2014), we employ\ntwo models: large and small. The large model em-\nploys dropout for regularization. The small model\nis not regularized. Therefore, we propose the fol-\nlowing regularization scheme. A projection matrix\nP\u2208IRH\u00d7His inserted before the output embed-\nding, i.e., h3=VPh2. The regularizing term\n\u03bb/bardblP/bardbl2is then added to the small model\u2019s loss\nfunction. In all of our experiments, \u03bb= 0.15.\nProjection regularization allows us to use the\nsame embedding (as both the input/output embed-\nding) with some adaptation that is under regular-\nization. It is, therefore, especially suited for WT.\nWhile training a vanilla untied NNLM , at\ntimestep t, with current input word sequence\ni1:t= [i1,...,it]and current target output word\not, the negative log likelihood loss is given by:\nLt=\u2212logpt(ot|i1:t), where pt(ot|i1:t) =\nexp(V\u22a4\noth(t)\n2)\n/summationtextC\nx=1exp(V\u22a4xh(t)\n2),Uk(Vk) is thekth row of U(V),\nwhich corresponds to word k, andh(t)\n2is the vector\nof activations of the topmost LSTM layer\u2019s output\nat timet. For simplicity, we assume that at each\ntimestept,it/ne}ationslash=ot. Optimization of the model is\nperformed using stochastic gradient descent.\nThe update for row kof the input embedding is:\n\u2202Lt\n\u2202Uk=/braceleftBigg\n(/summationtextC\nx=1pt(x|i1:t)\u00b7V\u22a4\nx\u2212V\u22a4\not)\u2202h(t)\n2\n\u2202Uitk=it\n0 k/negationslash=it\nFor the output embedding, row k\u2019s update is:\n\u2202Lt\n\u2202Vk=/braceleftBigg\n(pt(ot|i1:t)\u22121)h(t)\n2k=ot\npt(k|i1:t)\u00b7h(t)\n2 k/negationslash=oTherefore, in the untied model, at every timestep,\nthe only row that is updated in the input embed-\nding is the row Uitrepresenting the current input\nword. This means that vectors representing rare\nwords are updated only a small number of times.\nThe output embedding updates every row at each\ntimestep.\nIntied NNLMs , we setU=V=S. The\nupdate for each row in Sis the sum of the updates\nobtained for the two roles of S as both an input and\noutput embedding.\nThe update for row k/ne}ationslash=itis similar to the up-\ndate of row kin the untied NNLM\u2019s output embed-\nding (the only difference being that U and V are\nboth replaced by a single matrix S). In this case,\nthere is no update from the input embedding role\nofS.\nThe update for row k=it, is made up of a term\nfrom the input embedding (case k=it) and a term\nfrom the output embedding (case k/ne}ationslash=ot). The\nsecond term grows linearly with pt(it|i1:t), which\nis expected to be close to zero, since words sel-\ndom appear twice in a row (the low probability\nin the network was also veri\ufb01ed experimentally).\nThe update that occurs in this case is, therefore,\nmostly impacted by the update from the input em-\nbedding role of S.\nTo conclude, in the tied NNLM, every row of S\nis updated during each iteration, and for all rows\nexcept one, this update is similar to the update of\nthe output embedding of the untied model. This\nimplies a greater degree of similarity of the tied\nembedding to the untied model\u2019s output embed-\nding than to its input embedding.\nThe analysis above focuses on NNLMs for\nbrevity. In word2vec , the update rules are simi-\nlar, just that h(t)\n2is replaced by the identity func-\ntion. As argued by (Goldberg and Levy, 2014), in\nthis case weight tying is not appropriate, because\nifpt(it|i1:t)is close to zero then so is the norm\nof the embedding of it. This argument does not\nhold for NNLMs, since the LSTM layers cause a\ndecoupling of the input and output embedddings.\nFinally, we evaluate the effect of weight ty-\ning in neural translation models . In this model:\npt(ot|i1:t,r) =exp(V\u22a4\notG(t))\n/summationtextCt\nx=1exp(V\u22a4xG(t))wherer=\n(r1,...,rN)is the set of words in the source sen-\ntence,UandVare the input and output embed-\ndings of the decoder and Wis the input embed-\nding of the encoder (in translation models U,V\u2208\nIRCt\u00d7HandW\u2208IRCs\u00d7H, whereCs/Ctis theLanguage Subwords Subwords Subwords\npairs only in source only in target in both\nEN\u2192FR 2K 7K 85K\nEN\u2192DE 3K 11K 80K\nTable 1: Shared BPE subwords between pairs of languages.\nsize of the vocabulary of the source / target). G(t)\nis the decoder, which receives the context vector,\nthe embedding of the input word ( it) inU, and its\nprevious state at each timestep. ctis the context\nvector at timestep t,ct=/summationtext\nj\u2208ratjhj, whereatj\nis the weight given to the jth annotation at time t:\natj=exp(etj)/summationtext\nk\u2208rexp(eik), andetj=at(hj), whereais\nthe alignment model. Fis the encoder which pro-\nduces the sequence of annotations (h1,...,hN).\nThe output of the decoder is then projected to\na vector of scores using the output embedding:\nlt=VG(t). The scores are then converted to prob-\nability values using the softmax function.\nIn our weight tied translation model, we tie the\ninput and output embeddings of the decoder.\nWe observed that when preprocessing the ACL\nWMT 2014 EN \u2192FR1and WMT 2015 EN \u2192DE2\ndatasets using BPE, many of the subwords ap-\npeared in the vocabulary of both the source and\nthe target languages. Tab. 1 shows that up to\n90% (85%) of BPE subwords between English and\nFrench (German) are shared.\nBased on this observation, we propose three-\nway weight tying (TWWT), where the input em-\nbedding of the decoder, the output embedding of\nthe decoder and the input embedding of the en-\ncoder are all tied. The single source/target vocab-\nulary of this model is the union of both the source\nand target vocabularies. In this model, both in the\nencoder and decoder, all subwords are embedded\nin the same duo-lingual space.",
        "subsection": []
    },
    {
        "id": "4",
        "section": "Results",
        "text": "Our experiments study the quality of various em-\nbeddings, the similarity between them, and the\nimpact of tying them on the word2vec skip-gram\nmodel, NNLMs, and NMT models.",
        "subsection": [
            {
                "id": "4.1",
                "section": "Quality of obtained embeddings",
                "text": "In order to compare the various embeddings,\nwe pooled \ufb01ve embedding evaluation meth-\nods from the literature. These evaluation\nmethods involve calculating pairwise (cosine)\n1http://statmt.org/wmt14/translation-task.html\n2Input Output Tied\nSimlex999 0.30 0.29 0.17\nVerb-143 0.41 0.34 0.12\nMEN 0.66 0.61 0.50\nRare-Word 0.34 0.34 0.23\nMTurk-771 0.59 0.54 0.37\nTable 2: Comparison of input and output embeddings\nlearned by a word2vec skip-gram model. Results are also\nshown for the tied word2vec model. Spearman\u2019s correlation \u03c1\nis reported for \ufb01ve word embedding evaluation benchmarks.\nPTB text8\nEmbedding In Out Tied In Out Tied\nSimlex999 0.02 0.13 0.14 0.17 0.27 0.28\nVerb143 0.12 0.37 0.32 0.20 0.35 0.42\nMEN 0.11 0.21 0.26 0.26 0.50 0.50\nRare-Word 0.28 0.38 0.36 0.14 0.15 0.17\nMTurk771 0.17 0.28 0.30 0.26 0.48 0.45\nTable 3: Comparison of the input/output embeddings of the\nsmall model from (Zaremba et al., 2014) and the embeddings\nfrom our weight tied variant. Spearman\u2019s correlation \u03c1is pre-\nsented.\ndistances between embeddings and correlat-\ning these distances with human judgments\nof the strength of relationships between con-\ncepts. We use: Simlex999 (Hill et al., 2016),\nVerb-143 (Baker et al., 2014),\nMEN (Bruni et al., 2014), Rare-\nWord (Luong et al., 2013) and MTurk-\n771 (Halawi et al., 2012).\nWe begin by training both the tied and un-\ntied word2vec models on the text83dataset, us-\ning a vocabulary consisting only of words that\nappear at least \ufb01ve times. As can be seen\nin Tab. 2, the output embedding is almost as\ngood as the input embedding. As expected,\nthe embedding of the tied model is not com-\npetitive. The situation is different when train-\ning the small NNLM model on either the Penn\nTreebank (Marcus et al., 1993) or text8 datasets\n(for PTB, we used the same train/validation/test\nset split and vocabulary as (Mikolov et al., 2011),\nwhile on text8 we used the split/vocabulary\nfrom (Mikolov et al., 2014)). These results are\npresented in Tab. 3. In this case, the input embed-\nding is far inferior to the output embedding. The\ntied embedding is comparable to the output em-\nbedding.\nA natural question given these results and the\nanalysis in Sec. 3 is whether the word embedding\nin the weight tied NNLM model is more similar to\nthe input embedding or to the output embedding\n3http://mattmahoney.net/dc/textdataA B \u03c1(A,B)\u03c1(A,B)\u03c1(A,B)\nword2vec NNLM(S) NNLM(L)\nIn Out 0.77 0.13 0.16\nIn Tied 0.19 0.31 0.45\nOut Tied 0.39 0.65 0.77\nTable 4: Spearman\u2019s rank correlation \u03c1of similarity values\nbetween all pairs of words evaluated for the different embed -\ndings: input/output embeddings (of the untied model) and th e\nembeddings of our tied model. We show the results for both\nthe word2vec models and the small and large NNLM models\nfrom (Zaremba et al., 2014).\nModel Size Train Val. Test\nLarge (Zaremba et al., 2014) 66M 37.8 82.2 78.4\nLarge + Weight Tying 51M 48.5 77.7 74.3\nLarge + BD (Gal, 2015) + WD 66M 24.3 78.1 75.2\nLarge + BD + WT 51M 28.2 75.8 73.2\nRHN (Zilly et al., 2016) + BD 32M 67.4 71.2 68.5\nRHN + BD + WT 24M 74.1 68.1 66.0\nTable 5: Word level perplexity (lower is better) on PTB\nand size (number of parameters) of models that use either\ndropout (baseline model) or Bayesian dropout (BD). WD \u2013\nweight decay.\nof the original model. We, therefore, run the fol-\nlowing experiment: First, for each embedding, we\ncompute the cosine distances between each pair of\nwords. We then compute Spearman\u2019s rank corre-\nlation between these vectors of distances. As can\nbe seen in Tab. 4, the results are consistent with\nour analysis and the results of Tab. 2 and Tab. 3:\nfor word2vec the input and output embeddings are\nsimilar to each other and differ from the tied em-\nbedding; for the NNLM models, the output em-\nbedding and the tied embeddings are similar, the\ninput embedding is somewhat similar to the tied\nembedding, and differs considerably from the out-\nput embedding.",
                "subsection": []
            },
            {
                "id": "4.2",
                "section": "Neural network language models",
                "text": "We next study the effect of tying the embeddings\non the perplexity obtained by the NNLM mod-\nels. Following (Zaremba et al., 2014), we study\ntwo NNLMs. The two models differ mostly in the\nsize of the LSTM layers. In the small model, both\nLSTM layers contain 200 units and in the large\nmodel, both contain 1500 units. In addition, the\nlarge model uses three dropout layers, one placed\nright before the \ufb01rst LSTM layer, one between h1\nandh2and one right after h2. The dropout proba-\nbility is0.65. For both the small and large models,\nwe use the same hyperparameters (i.e. weight ini-\ntialization, learning rate schedule, batch size) as\nModel Size Train Val. Test\nKN 5-gram 141\nRNN 123\nLSTM 117\nStack RNN 8.48M 110\nFOFE-FNN 108\nNoisy LSTM 4.65M 111.7 108.0\nDeep RNN 6.16M 107.5\nSmall model 4.65M 38.0 120.7 114.5\nSmall + WT 2.65M 36.4 117.5 112.4\nSmall + PR 4.69M 50.8 116.0 111.7\nSmall + WT + PR 2.69M 53.5 104.9 100.9\nTable 6: Word level perplexity on PTB and size\nfor models that do not use dropout. The com-\npared models are: KN 5-gram (Mikolov et al., 2011),\nRNN (Mikolov et al., 2011), LSTM (Graves, 2013),\nStack / Deep RNN (Pascanu et al., 2013),\nFOFE-FNN (Zhang et al., 2015), Noisy\nLSTM (G\u00a8 ulc \u00b8ehre et al., 2016), and the small model\nfrom (Zaremba et al., 2014). The last three models are our\nmodels, which extend the small model. PR \u2013 projection\nregularization.\nModel Small S + WT S + PR S + WT + PRtext8Train 90.4 95.6 92.6 95.3\nVal. - - - -\nTest 195.3 187.1 199.0 183.2IMDBTrain 71.3 75.4 72.0 72.9\nVal. 94.1 94.6 94.0 91.2\nTest 94.3 94.8 94.4 91.5BBCTrain 28.6 30.1 42.5 45.7\nVal. 103.6 99.4 104.9 96.4\nTest 110.8 106.8 108.7 98.9\nTable 7: Word level perplexity on the text8, IMDB and\nBBC datasets. The last three models are our models, which\nextend the small model (S) of (Zaremba et al., 2014).\nIn addition to training our models on PTB and\ntext8, following (Miyamoto and Cho, 2016), we\nalso compare the performance of the NNLMs\non the BBC (Greene and Cunningham, 2006)\nand IMDB (Maas et al., 2011) datasets,\neach of which we process and split into a\ntrain/validation/test split (we use the same\nvocabularies as (Miyamoto and Cho, 2016)).\nIn the \ufb01rst experiment, which was conducted on\nthe PTB dataset, we compare the perplexity ob-\ntained by the large NNLM model and our ver-\nsion in which the input and output embeddings\nare tied. As can be seen in Tab. 5, weight tying\nsigni\ufb01cantly reduces perplexity on both the val-\nidation set and the test set, but not on the train-\ning set. This indicates less over\ufb01tting, as expected\ndue to the reduction in the number of parameters.\nRecently, (Gal and Ghahramani, 2016), proposed\na modi\ufb01ed model that uses Bayesian dropout and\nweight decay. They obtained improved perfor-\nmance. When the embeddings of this model areSize Validation Test\nEN\u2192FR Baseline 168M 29.49 33.13\nDecoder WT 122M 29.47 33.26\nTWWT 80M 29.43 33.46\nEN\u2192DE Baseline 165M 20.96 16.79\nDecoder WT 119M 21.09 16.54\nTWWT 79M 21.02 17.15\nTable 8: Size (number of parameters) and BLEU score of\nvarious translation models. TWWT \u2013 three-way weight tying.\ntied, a similar amount of improvement is gained.\nWe tried this with and without weight decay and\ngot similar results in both cases, with slight im-\nprovement in the latter model. Finally, by re-\nplacing the LSTM with a recurrent highway net-\nwork (Zilly et al., 2016), state of the art results are\nachieved when applying weight tying. The contri-\nbution of WT is also signi\ufb01cant in this model.\nPerplexity results are often reported separately\nfor models with and without dropout. In Tab. 6, we\nreport the results of the small NNLM model, that\ndoes not utilize dropout, on PTB. As can be seen,\nboth WT and projection regularization (PR) im-\nprove the results. When combining both methods\ntogether, state of the art results are obtained. An\nanalog table for text8, IMDB and BBC is Tab. 7,\nwhich shows a signi\ufb01cant reduction in perplexity\nacross these datasets when both PR and WT are\nused. PR does not help the large models, which\nemploy dropout for regularization.",
                "subsection": []
            },
            {
                "id": "4.3",
                "section": "Neural machine translation",
                "text": "Finally, we study the impact of weight tying in at-\ntention based NMT models, using the DL4MT4\nimplementation. We train our EN \u2192FR mod-\nels on the parallel corpora provided by ACL\nWMT 2014. We use the data as processed\nby (Cho et al., 2014) using the data selection\nmethod of (Axelrod et al., 2011). For EN \u2192DE\nwe train on data from the translation task of\nWMT 2015, validate on newstest2013 and test\non newstest2014 and newstest2015. Follow-\ning (Sennrich et al., 2016b) we learn the BPE seg-\nmentation on the union of the vocabularies that\nwe are translating from and to (we use BPE with\n89500 merge operations). All models were trained\nusing Adadelta (Zeiler, 2012) for 300K updates,\nhave a hidden layer size of 1000 and all embed-\nding layers are of size 500.\nTab. 8 shows that even though the weight tied\nmodels have about 28% fewer parameters than the\n4baseline models, their performance is similar. This\nis also the case for the three-way weight tied mod-\nels, even though they have about 52% fewer pa-\nrameters than their untied counterparts.",
                "subsection": []
            }
        ]
    },
    {
        "missing": []
    },
    {
        "references": []
    },
    {
        "title": "Using the Output Embedding to Improve Language Models",
        "arxiv_id": "1608.05859"
    }
]