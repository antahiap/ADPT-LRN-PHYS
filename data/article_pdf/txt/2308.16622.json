[
    {
        "id": "",
        "section": "Abstract",
        "text": "As the field of Large Language Models (LLMs) evolves at an accelerated pace, the critical need to assess\nand monitor their performance emerges. We introduce a benchmarking framework focused on knowledge\ngraph engineering (KGE) accompanied by three challenges addressing syntax and error correction, facts\nextraction and dataset generation. We show that while being a useful tool, LLMs are yet unfit to assist in\nknowledge graph generation with zero-shot prompting. Consequently, our LLM-KG-Bench framework\nprovides automatic evaluation and storage of LLM responses as well as statistical data and visualization\ntools to support tracking of prompt engineering and model performance.\nKeywords\nLarge Language Model, Knowledge Graph Engineering, Large Language Model Benchmark",
        "subsection": []
    },
    {
        "id": "1.",
        "section": "Introduction",
        "text": "Large Language Models (LLMs) hold the potential to change the way how we interact with data\nand technology. Especially models like GPT-3 and GPT-4 have shown proficient capabilities\nin solving textual assignments [ 1] and spawned a wave of subsequent models and the field of\nprompt engineering .\nBut the fast evolution and rapidly growing landscape of different LLMs make it challenging\nto keep track of their individual capabilities and to choose the best model and best prompt\nfor the job. There exist efforts on generic LLM benchmarks (e.g. [ 2]). However, despite these\nadvancements, the application and (automated) assessment of LLMs in the context of knowledge\ngraph engineering (KGE) and the Semantic Web is still a highly under-explored area. In response\nto this gap, this paper proposes a first LLM KGE benchmarking framework LLM-KG-Bench1\nthat follows our vision of an automated and continuous evaluation platform for different tasks\nSEMANTICS 2023 EU: 19th International Conference on Semantic Systems, September 20-22, 2023, Leipzig, Germany\n*Corresponding author.\n\u2020These authors contributed equally.\n/envel\u2322pe-\u2322penlpmeyer@infai.org (L. Meyer)\n/orcid0000-0001-5260-5181 (L. Meyer); 0000-0003-3127-0815 (J. Frey); 0000-0003-1337-2770 (K. Junghanns);\n0009-0008-5245-6655 (F. Brei); 0000-0002-1459-3754 (K. Bulert); 0000-0003-0054-5003 (S. Gr\u00fcnder-Fahrer);\n0000-0003-0762-8688 (M. Martin)\n\u00a92023 Copyright for this paper by its authors. Use permitted under Creative Commons License Attribution 4.0 International (CC BY 4.0).\nCEUR\nWorkshophttp://ceur-ws.orgCEUR\nWorkshop\nProceedingshttp://ceur-ws.org\nISSN 1613-0073\nCEUR Workshop Proceedings (CEUR-WS.org)\n1in KGE scenarios. A test of the framework is presented by comparing three LLMs for three\nexemplary KGE tasks.",
        "subsection": []
    },
    {
        "id": "2.",
        "section": "Related Work",
        "text": "The utilization of an LLM in the semantic web domain benefits from its capability to handle RDF-\nrelated syntaxes such as JSON-LD, Turtle and SPARQL. A comprehensive amalgamation of LLMs\nand knowledge graphs (KGs) is described in Dagstuhl Seminar [3] and [ 4]. The Knowledge Base\nConstruction from Pre-trained Language Models (LM-KBC) Challenge2emphasises the relevance\nof this combination.\nThe basis of this study is [ 5], where ChatGPT\u2019s use in knowledge graph engineering is\nassessed. Impressive capabilities were revealed, suggesting two conclusions: Firstly, such\nstudies offer insight into LLMs\u2019 potential and limitations, aiding knowledge graph engineers.\nSecondly, comparing different LLMs can lead to superior results by addressing inherent model\nissues.\nRecognizing the potential of Large Language Models (LLMs) in knowledge graph engineering,\nit\u2019s vital to evaluate their performance across diverse tasks. Google\u2019s Beyond the Imitation Game\n(BIG-bench) Benchmark3[2] and the Large Model Systems (LMSys) leaderboard4are community\nefforts that assess the performance of various models with regard to a plethora of tasks. The\nLanguage Model Evaluation Harness5offers further testing of generative language models on\nvarious evaluation tasks. However all of them are not perfect for assessing an LLM\u2019s use for\nKGE. They are missing KGE specific scoring and do not evaluate scores relative to problem size.\nThe size seems to be relevant for KGE as KGs get quite big in relation to current LLMs context\nsizes[5]. Acknowledging the existing appraoches limitations we introduce the LLM-KG-Bench\nframework.",
        "subsection": []
    },
    {
        "id": "3.",
        "section": "The LLM-KG-Bench Framework",
        "text": "Our current (and ongoing) work presented in this paper is comprising the design and imple-\nmentation of the modular LLM-KG-Bench framework1for benchmarking LLMs in the context\nof knowledge graph engineering. The main focus is on automated evaluation procedures to\nallow for many repeated test executions. The framework supports configurable task sizing, as\nprior work[5] suggest the relevance of the LLM\u2019s context size for KGE tasks.\nAs we aim for as much compatibility as possible, especially in the direction of BIG-bench3,\ntheLLM-KG-Bench framework is organized around benchmark tasks andLLM model connectors ,\nglued together by some code for execution organisation and result persistence. LLM model\nconnectors encapsulate the connection to a specific LLM and offer the function generate_text .\nWith this function a benchmark task can send a prompt to LLM and get its answer. Benchmark\ntasks handle the LLM evaluation for a single task. In the function evaluate_model they\n2Website: https://lm-kbc.github.io/challenge2023/\n3Repository: https://github.com/google/BIG-bench\n4Blogpost: https://lmsys.org/blog/2023-06-22-leaderboard/\n5\nBench Task (connector , size)\nQuery generator\nAI-Model-connector\nAnswer Evaluator\nplotStats\nStorageAIText\nTextAPI\nStatsTask-Info\naddon queries\nBenchmark\nCollectionConnector\nCollection\nIterate (according to config):\nSizes x Iterations x Connectors x Benchmarks\nIterations=10\nSizes={1k, 10k, 1m}\nConnectors={1,2}\nBenchmarks= {1,3,4}Figure 1: Basic LLM-KG-Bench framework architecture. The Benchmark runner takes a benchmark\nconfiguration and organizes the repeated execution of benchmark tasks with LLM model connectors and\ngiven size parameters. Results generated get stored and can be visualized.\nTable 1\nSetup used for testing the LLM-KG-Bench framework.\nModel Version\nClaude claude-1.3-100k\nGPT 3.5 gpt-3.5-turbo-0613 (4k)\nGPT 4 gpt-4-0613 (8k)\n(a) LLMs evaluatedTask a Task b Task c\nRepetitions: 20 x 1 size 20 x 1 size 20 x 8 sizes\nplot type: F1 measure F1 measure Mean error\nplot generated: Figure 2a Figure 2b Figure 2c\n(b) test configuration per task\nusually build a prompt or task description for the LLM, hand this task over to a given LLM via\nanLLM model connector and evaluate the given answer. If necessary the benchmark task could\nsend additional prompts to the LLM in the evaluation process. The evaluation results in score\nvalues for the task specific defined score types and additional information.\nDue to LLM-KG-Bench \u2019s modularization, as shown in Figure 1, additional benchmark tasks\nand LLM model connectors can be added by just adding corresponding python class definitions.\nThe framework supports basic result visualization with the help of seaborn6. The plots shown\nin Figure 2 are generated this way.",
        "subsection": []
    },
    {
        "id": "4.",
        "section": "Initial Evaluation of the Framework with first Tasks",
        "text": "To test the LLM-KG-Bench framework we added a couple of benchmark tasks and evaluated\nthree of the currently highest ranking LLMs at the LLMSYS Chatbot Arena Leaderboard4. The\ntest setup is detailed in Table 1.\n6Task a: Fixing of Errors in Turtle Files: Turtle is a common serialization format for\nknowledge graphs. By asking the LLMs to fix errors in given manipulated turtle files we test\nthe knowledge of turtle syntax as well as strict adhering to the given task and facts. One of the\nscores calculated during evaluation is the F1 measure on parsable normalized triples, comparing\nLLM\u2019s answer with a perfect answer. A plot on the F1 measure results for this task is shown in\nFigure 2a. GPT-3.5 often claims that file would be correct and returns no turtle. This accounts\nfor the high frequency of zero-value F1 scores. The answers given by Claude-1.3 and GPT-4\nscore better.\nTask b: KG Creation from Factsheet Plaintext: To evaluate knowledge extraction and\nmodelling capabilities, we use a plaintext excerpt of a PDF factsheet. The text describes various\nspecifications of a 3D printer in a key-value style, including usual formatting irregularities\nassociated with PDF extraction. We ask the model to generate a Turtle file, that captures a subset\nof the information. The prompt is engineered very specific with regard to which properties\nor ontologies have to be used and how IRI identifiers and Literals should be represented.\nSubsequently, we can evaluate the quality of a single response using the F1 measure, counting\nthe set of parsable triples that (mis)match or are missing compared to a manually curated\nreference document. Fig. 2b shows that the GPT models outperform Claude in this task.\nWhile GPT4 has a better mean, due to one very good response, it however replied often with\nunparseable content, which in turn did not happen for GPT3.5, leading to a slightly better\nmedian for that.\nTask c: Synthetic Dataset Generation: Creating example data is an important task and\nthe help of LLMs would be highly appreciated. We created a basic test for this capability. We ask\nthe LLM to generate some synthetic dataset using well known foaf:Person andfoaf:knows\nwith a varying number of desired objects and links in the final KG. In the evaluation we used\nbeside other scores the persons_relative_error indicating the difference between the actual\nnumber person objects generated and the number asked for. This value is normalized to be = 0\nif they match, >0if there are more persons than asked for and <0if there are less persons,\nwith the special case of \u22121meaning an empty graph. The results presented in Figure 2c show a\nrelation between the persons_relative_error and the problem size, in this case number of person\nobjects to generate.",
        "subsection": []
    },
    {
        "id": "5.",
        "section": "Conclusion and Future Work",
        "text": "We showed that there is a need for measuring the knowledge graph engineering capabilities\nof the rapidly evolving LLMs. We proposed and describe the novel LLM-KG-Bench framework\nfor this task. A first evaluation of three high ranking LLMs with first benchmarks shows the\nbenefit of the automated evaluation with the new framework.\nThe LLM-KG-Bench framework is prepared to enable dialogs between benchmark tasks and\nLLMs. It will be interesting to evaluate LLMs capabilities to fix their answers with some feedback\nlike e.g. error codes in improved or additional tasks. We are looking forward to extending to\n(a) Turtle Fixing\n (b) Fact Extraction\n (c) Mean Error Dataset Generation\nFigure 2: Subset of metrics from initial tasks. Shown are the F1 scores and mean error of person count\nAcknowledgments\nThis work was partially supported by grants from the German Federal Ministry for Economic\nAffairs and Climate Action (BMWK) to the CoyPu project (01MK21007A) and KISS project\n(01MK22001A) as well as from the German Federal Ministry of Education and Research (BMBF)\nto the projects StahlDigital (13XP5116B) and KupferDigital (F13XP5119F).\nReferences\n[1] OpenAI, Gpt-4 technical report, 2023. arXiv:2303.08774 .\n[2]A. Srivastava, et al., Beyond the imitation game: Quantifying and extrapolating the\ncapabilities of language models, Transactions on Machine Learning Research (2023).\narXiv:2206.04615 .\n[3]P. Groth, E. Simperl, M. van Erp, D. Vrande\u010di\u0107, Knowledge graphs and their role in the\nknowledge engineering of the 21st century (dagstuhl seminar 22372) (2023). doi: 10.4230/\nDAGREP.12.9.60 .\n[4]S. Pan, L. Luo, Y. Wang, C. Chen, J. Wang, X. Wu, Unifying large language models and\nknowledge graphs: A roadmap, 2023. arXiv:2306.08302 .\n[5]L.-P. Meyer, C. Stadler, J. Frey, N. Radtke, K. Junghanns, R. Meissner, G. Dziwis, K. Bulert,\nM. Martin, Llm-assisted knowledge graph engineering: Experiments with chatgpt, 2023.\narXiv:2307.06917 , to appear in proceedings of AI-Tomorrow track on Data Week 2023\nin Leipzig.\nA. Online Resources\n\u2022LLM-KG-Bench repository: https://github.com/AKSW/LLM-KG-Bench\nor doi:10.5281/zenodo.8251944\n\u2022experiment data: https://github.com/AKSW/LLM-KG-Bench-Results/tree/main/",
        "subsection": []
    },
    {
        "id": "",
        "section": "Abstract",
        "text": "As the field of Large Language Models (LLMs) evolves at an accelerated pace, the critical need to assess\nand monitor their performance emerges. We introduce a benchmarking framework focused on knowledge\ngraph engineering (KGE) accompanied by three challenges addressing syntax and error correction, facts\nextraction and dataset generation. We show that while being a useful tool, LLMs are yet unfit to assist in\nknowledge graph generation with zero-shot prompting. Consequently, our LLM-KG-Bench framework\nprovides automatic evaluation and storage of LLM responses as well as statistical data and visualization\ntools to support tracking of prompt engineering and model performance.\nKeywords\nLarge Language Model, Knowledge Graph Engineering, Large Language Model Benchmark\n1. Introduction\nLarge Language Models (LLMs) hold the potential to change the way how we interact with data\nand technology. Especially models like GPT-3 and GPT-4 have shown proficient capabilities\nin solving textual assignments ",
        "subsection": []
    },
    {
        "missing": []
    },
    {
        "title": "Developing a Scalable Benchmark for Assessing Large\nLanguage Models in Knowledge Graph Engineeringhttp://ceur-ws.org"
    }
]