[
    {
        "id": "",
        "section": "Abstract",
        "text": "IVE\nSUMMARIZATION\nRomain Paulus, Caiming Xiong ",
        "subsection": []
    },
    {
        "id": "172",
        "section": "University avenue",
        "text": "Palo Alto, CA 94301, USA\nfrpaulus,cxiong,rsocher g@salesforce.com\nABSTRACT\nAttentional, RNN-based encoder-decoder models for abstractive summarization\nhave achieved good performance on short input and output sequences. For longer\ndocuments and summaries however these models often include repetitive and\nincoherent phrases. We introduce a neural network model with a novel intra-\nattention that attends over the input and continuously generated output separately,\nand a new training method that combines standard supervised word prediction and\nreinforcement learning (RL). Models trained only with supervised learning often\nexhibit \u201cexposure bias\u201d \u2013 they assume ground truth is provided at each step during\ntraining. However, when standard word prediction is combined with the global se-\nquence prediction training of RL the resulting summaries become more readable.\nWe evaluate this model on the CNN/Daily Mail and New York Times datasets.\nOur model obtains a 41.16 ROUGE-1 score on the CNN/Daily Mail dataset, an\nimprovement over previous state-of-the-art models. Human evaluation also shows\nthat our model produces higher quality summaries.",
        "subsection": [
            {
                "id": "2.1",
                "section": "Intra -temporal attention on input sequence",
                "text": "At each decoding step t, we use an intra-temporal attention function to attend over speci\ufb01c parts\nof the encoded input sequence in addition to the decoder\u2019s own hidden state and the previously-\ngenerated word (Sankaran et al., 2016). This kind of attention prevents the model from attending\nover the sames parts of the input on different decoding steps. Nallapati et al. (2016) have shown\nthat such an intra-temporal attention can reduce the amount of repetitions when attending over long\ndocuments.\nWe de\ufb01neetias the attention score of the hidden input state he\niat decoding time step t:\neti=f(hd\nt;he\ni); (1)\nwherefcan be any function returning a scalar etifrom thehd\ntandhe\nivectors. While some attention\nmodels use functions as simple as the dot-product between the two vectors, we choose to use a\nbilinear function:\nf(hd\nt;he\ni) =hd\ntTWe\nattnhe\niWe normalize the attention weights with the following temporal attention function, penalizing input\ntokens that have obtained high attention scores in past decoding steps. We de\ufb01ne new temporal\nscorese0\nti:\ne0\nti=(\nexp(eti) ift= 1\nexp(eti)Pt\u00001\nj=1exp(eji)otherwise:(3)\nFinally, we compute the normalized attention scores \u000be\ntiacross the inputs and use these weights to\nobtain the input context vector ce\nt:\n\u000be\nti=e0\ntiPn\nj=1e0\ntj(4) ce\nt=nX\ni=1\u000be\ntihe\ni: (5)",
                "subsection": []
            },
            {
                "id": "2.2",
                "section": "Intra -decoder attention",
                "text": "While this intra-temporal attention function ensures that different parts of the encoded input se-\nquence are used, our decoder can still generate repeated phrases based on its own hidden states,\nespecially when generating long sequences. To prevent that, we can incorporate more information\nabout the previously decoded sequence into the decoder. Looking back at previous decoding steps\nwill allow our model to make more structured predictions and avoid repeating the same information,\neven if that information was generated many steps away. To achieve this, we introduce an intra-\ndecoder attention mechanism. This mechanism is not present in existing encoder-decoder models\nfor abstractive summarization.\nFor each decoding step t, our model computes a new decoder context vector cd\nt. We setcd\n1to a vector\nof zeros since the generated sequence is empty on the \ufb01rst decoding step. For t >1, we use the\nfollowing equations:\ned\ntt0=hd\ntTWd\nattnhd\nt0(6)\u000bd\ntt0=exp(ed\ntt0)Pt\u00001\nj=1exp(ed\ntj)(7) cd\nt=t\u00001X\nj=1\u000bd\ntjhd\nj (8)\nFigure 1 illustrates the intra-attention context vector computation cd\nt, in addition to the encoder\ntemporal attention, and their use in the decoder.\nA closely-related intra-RNN attention function has been introduced by Cheng et al. (2016) but their\nimplementation works by modifying the underlying LSTM function, and they do not apply it to\nlong sequence generation problems. This is a major difference with our method, which makes no\nassumptions about the type of decoder RNN, thus is more simple and widely applicable to other\ntypes of recurrent networks.",
                "subsection": []
            },
            {
                "id": "2.3",
                "section": "Token generation and pointer",
                "text": "To generate a token, our decoder uses either a token-generation softmax layer or a pointer mecha-\nnism to copy rare or unseen from the input sequence. We use a switch function that decides at each\ndecoding step whether to use the token generation or the pointer (Gulcehre et al., 2016; Nallapati\net al., 2016). We de\ufb01ne utas a binary value, equal to 1 if the pointer mechanism is used to output\nyt, and 0 otherwise. In the following equations, all probabilities are conditioned on y1;:::;y t\u00001;x,\neven when not explicitly stated.\nOur token-generation layer generates the following probability distribution:\np(ytjut= 0) = softmax( Wout[hd\ntkce\ntkcd\nt] +bout) (9)\nOn the other hand, the pointer mechanism uses the temporal attention weights \u000be\ntias the probability\ndistribution to copy the input token xi.\np(yt=xijut= 1) =\u000be\nti (10)\nWe also compute the probability of using the copy mechanism for the decoding step t:\np(ut= 1) =\u001b(Wu[hd\ntkce\ntkcd\nt] +bu)where\u001bis the sigmoid activation function.\nPutting Equations 9 , 10 and 11 together, we obtain our \ufb01nal probability distribution for the output\ntokenyt:\np(yt) =p(ut= 1)p(ytjut= 1) +p(ut= 0)p(ytjut= 0): (12)\nThe ground-truth value for utand the corresponding iindex of the target input token when ut= 1\nare provided at every decoding step during training. We set ut= 1 either when ytis an out-of-\nvocabulary token or when it is a pre-de\ufb01ned named entity (see Section 5).",
                "subsection": []
            },
            {
                "id": "2.4",
                "section": "Sharing decoder weights",
                "text": "In addition to using the same embedding matrix Wembfor the encoder and the decoder sequences,\nwe introduce some weight-sharing between this embedding matrix and the Woutmatrix of the token-\ngeneration layer, similarly to Inan et al. (2017) and Press & Wolf (2016). This allows the token-\ngeneration function to use syntactic and semantic information contained in the embedding matrix.\nWout= tanh(WembWproj) (13)",
                "subsection": []
            },
            {
                "id": "2.5",
                "section": "Repetition avoidance at test time",
                "text": "Another way to avoid repetitions comes from our observation that in both the CNN/Daily Mail and\nNYT datasets, ground-truth summaries almost never contain the same trigram twice. Based on this\nobservation, we force our decoder to never output the same trigram more than once during testing.\nWe do this by setting p(yt) = 0 during beam search, when outputting ytwould create a trigram that\nalready exists in the previously decoded sequence of the current beam.",
                "subsection": []
            }
        ]
    },
    {
        "id": "1",
        "section": "Introduction",
        "text": "Text summarization is the process of automatically generating natural language summaries from an\ninput document while retaining the important points. By condensing large quantities of information\ninto short, informative summaries, summarization can aid many downstream applications such as\ncreating news digests, search, and report generation.\nThere are two prominent types of summarization algorithms. First, extractive summarization sys-\ntems form summaries by copying parts of the input (Dorr et al., 2003; Nallapati et al., 2017). Second,\nabstractive summarization systems generate new phrases, possibly rephrasing or using words that\nwere not in the original text (Chopra et al., 2016; Nallapati et al., 2016).\nNeural network models (Nallapati et al., 2016) based on the attentional encoder-decoder model for\nmachine translation (Bahdanau et al., 2014) were able to generate abstractive summaries with high\nROUGE scores. However, these systems have typically been used for summarizing short input\nsequences (one or two sentences) to generate even shorter summaries. For example, the summaries\non the DUC-2004 dataset generated by the state-of-the-art system by Zeng et al. (2016) are limited\nto 75 characters.\nNallapati et al. (2016) also applied their abstractive summarization model on the CNN/Daily Mail\ndataset (Hermann et al., 2015), which contains input sequences of up to 800 tokens and multi-\nsentence summaries of up to 100 tokens. But their analysis illustrates a key problem with attentional\nencoder-decoder models: they often generate unnatural summaries consisting of repeated phrases.\nWe present a new abstractive summarization model that achieves state-of-the-art results on the\nCNN/Daily Mail and similarly good results on the New York Times dataset (NYT) (Sandhaus,\n2008). To our knowledge, this is the \ufb01rst end-to-end model for abstractive summarization on the\nNYT dataset. We introduce a key attention mechanism and a new learning objective to address the\nrepeating phrase problem: (i) we use an intra-temporal attention in the encoder that records previous\nFigure 1: Illustration of the encoder and decoder attention functions combined. The two context\nvectors (marked \u201cC\u201d) are computed from attending over the encoder hidden states and decoder\nhidden states. Using these two contexts and the current decoder hidden state (\u201cH\u201d), a new word is\ngenerated and added to the output sequence.\ntakes into account which words have already been generated by the decoder. (ii) we propose a new\nobjective function by combining the maximum-likelihood cross-entropy loss used in prior work with\nrewards from policy gradient reinforcement learning to reduce exposure bias.\nOur model achieves 41.16 ROUGE-1 on the CNN/Daily Mail dataset. Moreover, we show, through\nhuman evaluation of generated outputs, that our model generates more readable summaries com-\npared to other abstractive approaches.",
        "subsection": [
            {
                "id": "3.1",
                "section": "Supervised learning with teacher forcing",
                "text": "The most widely used method to train a decoder RNN for sequence generation, called the\nteacher forcing\u201d algorithm (Williams & Zipser, 1989), minimizes a maximum-likelihood loss at each\ndecoding step. We de\ufb01ne y\u0003=fy\u0003\n1;y\u0003\n2;:::;y\u0003\nn0gas the ground-truth output sequence for a given\ninput sequence x. The maximum-likelihood training objective is the minimization of the following\nloss:\nLml=\u0000n0X\nt=1logp(y\u0003\ntjy\u0003\n1;:::;y\u0003\nt\u00001;x) (14)\nHowever, minimizing Lmldoes not always produce the best results on discrete evaluation metrics\nsuch as ROUGE (Lin, 2004). This phenomenon has been observed with similar sequence generation\ntasks like image captioning with CIDEr (Rennie et al., 2016) and machine translation with BLEU\n(Wu et al., 2016; Norouzi et al., 2016). There are two main reasons for this discrepancy. The \ufb01rst\none, called exposure bias (Ranzato et al., 2015), comes from the fact that the network has knowledge\nof the ground truth sequence up to the next token during training but does not have such supervision\nwhen testing, hence accumulating errors as it predicts the sequence. The second reason is due to\nthe large number of potentially valid summaries, since there are more ways to arrange tokens to\nproduce paraphrases or different sentence orders. The ROUGE metrics take some of this \ufb02exibility\ninto account, but the maximum-likelihood objective does not.",
                "subsection": []
            },
            {
                "id": "3.2",
                "section": "Policy learning",
                "text": "One way to remedy this is to learn a policy that maximizes a speci\ufb01c discrete metric instead of\nminimizing the maximum-likelihood loss, which is made possible with reinforcement learning. In\nFor this training algorithm, we produce two separate output sequences at each training iteration: ys,\nwhich is obtained by sampling from the p(ys\ntjys\n1;:::;ys\nt\u00001;x)probability distribution at each decod-\ning time step, and ^y, the baseline output, obtained by maximizing the output probability distribution\nat each time step, essentially performing a greedy search. We de\ufb01ne r(y)as the reward function for\nan output sequence y, comparing it with the ground truth sequence y\u0003with the evaluation metric of\nour choice.\nLrl= (r(^y)\u0000r(ys))n0X\nt=1logp(ys\ntjys\n1;:::;ys\nt\u00001;x) (15)\nWe can see that minimizing Lrlis equivalent to maximizing the conditional likelihood of the sam-\npled sequence ysif it obtains a higher reward than the baseline ^y, thus increasing the reward expec-\ntation of our model.",
                "subsection": []
            },
            {
                "id": "3.3",
                "section": "Mixed training objective function",
                "text": "One potential issue of this reinforcement training objective is that optimizing for a speci\ufb01c discrete\nmetric like ROUGE does not guarantee an increase in quality and readability of the output. It\nis possible to game such discrete metrics and increase their score without an actual increase in\nreadability or relevance (Liu et al., 2016). While ROUGE measures the n-gram overlap between our\ngenerated summary and a reference sequence, human-readability is better captured by a language\nmodel, which is usually measured by perplexity.\nSince our maximum-likelihood training objective (Equation 14) is essentially a conditional lan-\nguage model, calculating the probability of a token ytbased on the previously predicted sequence\nfy1;:::;y t\u00001gand the input sequence x, we hypothesize that it can assist our policy learning algo-\nrithm to generate more natural summaries. This motivates us to de\ufb01ne a mixed learning objective\nfunction that combines equations 14 and 15:\nLmixed =\rLrl+ (1\u0000\r)Lml; (16)\nwhere\ris a scaling factor accounting for the difference in magnitude between LrlandLml. A\nsimilar mixed-objective learning function has been used by Wu et al. (2016) for machine translation\non short sequences, but this is its \ufb01rst use in combination with self-critical policy learning for long\nsummarization to explicitly improve readability in addition to evaluation metrics.",
                "subsection": []
            }
        ]
    },
    {
        "id": "2",
        "section": "Neural intra -attention model",
        "text": "In this section, we present our intra-attention model based on the encoder-decoder network\n(Sutskever et al., 2014). In all our equations, x=fx1;x2;:::;x ngrepresents the sequence of input\n(article) tokens, y=fy1;y2;:::;y n0gthe sequence of output (summary) tokens, and kdenotes the\nvector concatenation operator.\nOur model reads the input sequence with a bi-directional LSTM encoder fRNNefwd;RNNebwdg\ncomputing hidden states he\ni= [hefwd\nikhebwd\ni]from the embedding vectors of xi. We use a single\nLSTM decoder RNNd, computing hidden states hd\ntfrom the embedding vectors of yt. Both input\nand output embeddings are taken from the same matrix Wemb. We initialize the decoder hidden state\nwithhd\n0=he\nn.",
        "subsection": [
            {
                "id": "4.1",
                "section": "Neural encoder -decoder sequence models",
                "text": "Neural encoder-decoder models are widely used in NLP applications such as machine translation\n(Sutskever et al., 2014), summarization (Chopra et al., 2016; Nallapati et al., 2016), and question\nanswering (Hermann et al., 2015). These models use recurrent neural networks (RNN), such as\nlong-short term memory network (LSTM) (Hochreiter & Schmidhuber, 1997) to encode an input\nsentence into a \ufb01xed vector, and create a new output sequence from that vector using another RNN.\nTo apply this sequence-to-sequence approach to natural language, word embeddings (Mikolov et al.,\n2013; Pennington et al., 2014) are used to convert language tokens to vectors that can be used as\ninputs for these networks. Attention mechanisms (Bahdanau et al., 2014) make these models more\nperformant and scalable, allowing them to look back at parts of the encoded input sequence while\nthe output is generated. These models often use a \ufb01xed input and output vocabulary, which prevents\nthem from learning representations for new words. One way to \ufb01x this is to allow the decoder\nnetwork to point back to some speci\ufb01c words or sub-sequences of the input and copy them onto the\noutput sequence (Vinyals et al., 2015). Gulcehre et al. (2016) and Merity et al. (2017) combine this\npointer mechanism with the original word generation layer in the decoder to allow the model to use\neither method at each decoding step.",
                "subsection": []
            },
            {
                "id": "4.2",
                "section": "Reinforcement learning for sequence generation",
                "text": "Reinforcement learning (RL) is a way of training an agent to interact with a given environment in\nan agent has to perform discrete actions before obtaining a reward, or when the metric to optimize\nis not differentiable and traditional supervised learning methods cannot be used. This is applicable\nto sequence generation tasks, because many of the metrics used to evaluate these tasks (like BLEU,\nROUGE or METEOR) are not differentiable.\nIn order to optimize that metric directly, Ranzato et al. (2015) have applied the REINFORCE algo-\nrithm (Williams, 1992) to train various RNN-based models for sequence generation tasks, leading\nto signi\ufb01cant improvements compared to previous supervised learning methods. While their method\nrequires an additional neural network, called a critic model, to predict the expected reward and sta-\nbilize the objective function gradients, Rennie et al. (2016) designed a self-critical sequence training\nmethod that does not require this critic model and lead to further improvements on image captioning\ntasks.",
                "subsection": []
            },
            {
                "id": "4.3",
                "section": "Text summarization",
                "text": "Most summarization models studied in the past are extractive in nature (Dorr et al., 2003; Nallapati\net al., 2017; Durrett et al., 2016), which usually work by identifying the most important phrases of an\ninput document and re-arranging them into a new summary sequence. The more recent abstractive\nsummarization models have more degrees of freedom and can create more novel sequences. Many\nabstractive models such as Rush et al. (2015), Chopra et al. (2016) and Nallapati et al. (2016) are all\nbased on the neural encoder-decoder architecture (Section 4.1).\nA well-studied set of summarization tasks is the Document Understanding Conference (DUC)1.\nThese summarization tasks are varied, including short summaries of a single document and long\nsummaries of multiple documents categorized by subject. Most abstractive summarization models\nhave been evaluated on the DUC-2004 dataset, and outperform extractive models on that task (Dorr\net al., 2003). However, models trained on the DUC-2004 task can only generate very short sum-\nmaries up to 75 characters, and are usually used with one or two input sentences. Chen et al. (2016)\napplied different kinds of attention mechanisms for summarization on the CNN dataset, and Nalla-\npati et al. (2016) used different attention and pointer functions on the CNN and Daily Mail datasets\ncombined. In parallel of our work, See et al. (2017) also developed an abstractive summarization\nmodel on this dataset with an extra loss term to increase temporal coverage of the encoder attention\nfunction.",
                "subsection": []
            }
        ]
    },
    {
        "id": "3",
        "section": "Hybrid learning objective",
        "text": "In this section, we explore different ways of training our encoder-decoder model. In particular, we\npropose reinforcement learning-based algorithms and their application to our summarization task.",
        "subsection": [
            {
                "id": "5.1",
                "section": "Cnn",
                "text": "We evaluate our model on a modi\ufb01ed version of the CNN/Daily Mail dataset (Hermann et al., 2015),\nfollowing the same pre-processing steps described in Nallapati et al. (2016). We refer the reader to\nthat paper for a detailed description. Our \ufb01nal dataset contains 287,113 training examples, 13,368\nvalidation examples and 11,490 testing examples. After limiting the input length to 800 tokens and\noutput length to 100 tokens, the average input and output lengths are respectively 632 and 53 tokens.",
                "subsection": []
            },
            {
                "id": "5.2",
                "section": "Newyork times",
                "text": "The New York Times (NYT) dataset (Sandhaus, 2008) is a large collection of articles published\nbetween 1996 and 2007. Even though this dataset has been used to train extractive summarization\nsystems (Durrett et al., 2016; Hong & Nenkova, 2014; Li et al., 2016) or closely-related models\nfor predicting the importance of a phrase in an article (Yang & Nenkova, 2014; Nye & Nenkova,\n2015; Hong et al., 2015), we are the \ufb01rst group to run an end-to-end abstractive summarization\nmodel on the article-abstract pairs of this dataset. While CNN/Daily Mail summaries have a similar\nwording to their corresponding articles, NYT abstracts are more varied, are shorter and can use\na higher level of abstraction and paraphrase. Because of these differences, these two formats are\na good complement to each other for abstractive summarization models. We describe the dataset\npreprocessing and pointer supervision in Section A of the Appendix.\n1Model ROUGE-1 ROUGE-2 ROUGE-L\nLead-3 (Nallapati et al., 2017) 39.2 15.7 35.5\nSummaRuNNer (Nallapati et al., 2017) 39.6 16.2 35.3\nwords-lvt2k-temp-att (Nallapati et al., 2016) 35.46 13.30 32.65\nML, no intra-attention 37.86 14.69 34.99\nML, with intra-attention 38.30 14.81 35.49\nRL, with intra-attention 41.16 15.75 39.08\nML+RL, with intra-attention 39.87 15.82 36.90\nTable 1: Quantitative results for various models on the CNN/Daily Mail test dataset\nModel ROUGE-1 ROUGE-2 ROUGE-L\nML, no intra-attention 44.26 27.43 40.41\nML, with intra-attention 43.86 27.10 40.11\nRL, no intra-attention 47.22 30.51 43.27\nML+RL, no intra-attention 47.03 30.72 43.10\nTable 2: Quantitative results for various models on the New York Times test dataset\nSource document\nJenson Button was denied his 100th race for McLaren after an ERS prevented him from making it to the start-\nline. It capped a miserable weekend for the Briton; his time in Bahrain plagued by reliability issues. Button\nspent much of the race on Twitter delivering his verdict as the action unfolded. \u2019Kimi is the man to watch,\u2019 and\n\u2019loving the sparks\u2019, were among his pearls of wisdom, but the tweet which courted the most attention was a\nrather mischievous one: \u2019Ooh is Lewis backing his team mate into Vettel?\u2019 he quizzed after Rosberg accused\nHamilton of pulling off such a manoeuvre in China. Jenson Button waves to the crowd ahead of the Bahrain\nGrand Prix which he failed to start Perhaps a career in the media beckons... Lewis Hamilton has out-quali\ufb01ed\nand \ufb01nished ahead of Nico Rosberg at every race this season. Indeed Rosberg has now beaten his Mercedes\nteam-mate only once in the 11 races since the pair infamously collided in Belgium last year. Hamilton secured\nthe 36th win of his career in Bahrain and his 21st from pole position. Only Michael Schumacher (40), Ayrton\nSenna (29) and Sebastian Vettel (27) have more. (...)\nGround truth summary\nButton denied 100th race start for McLaren after ERS failure. Button then spent much of the Bahrain Grand\nPrix on Twitter delivering his verdict on the action as it unfolded. Lewis Hamilton has out-quali\ufb01ed and \ufb01nished\nahead of Mercedes team-mate Nico Rosberg at every race this season. Bernie Ecclestone con\ufb01rms F1 will make\nits bow in Azerbaijan next season.\nML, with intra-attention (ROUGE-1 41.58)\nButton was denied his 100th race for McLaren. ERS prevented him from making it to the start-line. The Briton.\nHe quizzed after Nico Rosberg accused Lewis Hamilton of pulling off such a manoeuvre in China. Button has\nbeen in Azerbaijan for the \ufb01rst time since 2013.\nRL, with intra-attention (ROUGE-1 50.00)\nButton was denied his 100th race for McLaren after an ERS prevented him from making it to the start-line.\nIt capped a miserable weekend for the Briton. Button has out-quali\ufb01ed. Finished ahead of Nico Rosberg at\nBahrain. Lewis Hamilton has. In 11 races. . The race. To lead 2,000 laps. . In. . . And. .\nML+RL, with intra-attention (ROUGE-1 44.00)\nButton was denied his 100th race for McLaren. The ERS prevented him from making it to the start-line. Button\nwas his team mate in the 11 races in Bahrain. He quizzed after Nico Rosberg accused Lewis Hamilton of\npulling off such a manoeuvre in China.\nTable 3: Example from the CNN/Daily Mail test dataset showing the outputs of our three best models\nafter de-tokenization, re-capitalization, replacing anonymized entities, and replacing numbers. The\nROUGE score corresponds to the speci\ufb01c example.",
                "subsection": []
            }
        ]
    },
    {
        "id": "4",
        "section": "Related work",
        "text": "",
        "subsection": [
            {
                "id": "6.1",
                "section": "Experiments",
                "text": "Setup : We evaluate the intra-decoder attention mechanism and the mixed-objective learning by\nrunning the following experiments on both datasets. We \ufb01rst run maximum-likelihood (ML) training\nwith and without intra-decoder attention (removing cd\nModel R-1 R-2\nFirst sentences 28.6 17.3\nFirstkwords 35.7 21.6\nFull (Durrett et al., 2016) 42.2 24.9\nML+RL, with intra-attn 42.94 26.02\nTable 4: Comparison of ROUGE recall scores for lead baselines, the extractive model of Durrett\net al. (2016) and our model on their NYT dataset splits.\nattention) and select the best performing architecture. Next, we initialize our model with the best\nML parameters and we compare reinforcement learning (RL) with our mixed-objective learning\n(ML+RL), following our objective functions in Equation 15 and 16. The hyperparameters and other\nimplementation details are described in the Appendix.\nROUGE metrics and options : We report the full-length F-1 score of the ROUGE-1, ROUGE-2\nand ROUGE-L metrics with the Porter stemmer option. For RL and ML+RL training, we use the\nROUGE-L score as a reinforcement reward. We also tried ROUGE-2 but we found that it created\nsummaries that almost always reached the maximum length, often ending sentences abruptly.",
                "subsection": []
            },
            {
                "id": "6.2",
                "section": "Quantitative analysis",
                "text": "0-9 10-19 20-29 30-39 40-49 50-59 60-69 70-79 80-89 90-99 100-109\nGround truth length0.04\n0.02\n0.000.020.040.060.080.100.12Cumulated ROUGE-1 gain on CNN/Daily Mail with intra-attn\nFigure 2: Cumulated ROUGE-1 relative im-\nprovement obtained by adding intra-attention\nto the ML model on the CNN/Daily Mail\ndataset.Our results for the CNN/Daily Mail dataset are\nshown in Table 1, and for the NYT dataset in Table\n2. We observe that the intra-decoder attention func-\ntion helps our model achieve better ROUGE scores\non the CNN/Daily Mail but not on the NYT dataset.\nFurther analysis on the CNN/Daily Mail test set\nshows that intra-attention increases the ROUGE-1\nscore of examples with a long ground truth sum-\nmary, while decreasing the score of shorter sum-\nmaries, as illustrated in Figure 2. This con\ufb01rms\nour assumption that intra-attention improves per-\nformance on longer output sequences, and explains\nwhy intra-attention doesnt improve performance on\nthe NYT dataset, which has shorter summaries on\naverage.\nIn addition, we can see that on all datasets, both the\nRL and ML+RL models obtain much higher scores than the ML model. In particular, these methods\nclearly surpass the state-of-the-art model from Nallapati et al. (2016) on the CNN/Daily Mail dataset,\nas well as the lead-3 extractive baseline (taking the \ufb01rst 3 sentences of the article as the summary)\nand the SummaRuNNer extractive model (Nallapati et al., 2017).\nSee et al. (2017) also reported their results on a closely-related abstractive model the CNN/DailyMail\nbut used a different dataset preprocessing pipeline, which makes direct comparison with our numbers\ndif\ufb01cult. However, their best model has lower ROUGE scores than their lead-3 baseline, while our\nML+RL model beats the lead-3 baseline as shown in Table 1. Thus, we conclude that our mixed-\nobjective model obtains a higher ROUGE performance than theirs.\nWe also compare our model against extractive baselines (either lead sentences or lead words) and\nthe extractive summarization model built by Durrett et al. (2016), which was trained using a smaller\nversion of the NYT dataset that is 6 times smaller than ours but contains longer summaries. We\ntrained our ML+RL model on their dataset and show the results on Table 4. Similarly to Durrett\net al. (2016), we report the limited-length ROUGE recall scores instead of full-length F-scores. For\neach example, we limit the generated summary length or the baseline length to the ground truth\nsummary length. Our results show that our mixed-objective model has higher ROUGE scores than\nModel Readability Relevance\nML 6.76 7.14\nRL 4.18 6.32\nML+RL 7.04 7.45\nTable 5: Comparison of human readability scores on a random subset of the CNN/Daily Mail test\ndataset. All models are with intra-decoder attention.",
                "subsection": []
            },
            {
                "id": "6.3",
                "section": "Qualitative analysis",
                "text": "We perform human evaluation to ensure that our increase in ROUGE scores is also followed by\nan increase in human readability and quality. In particular, we want to know whether the ML+RL\ntraining objective did improve readability compared to RL.\nEvaluation setup : To perform this evaluation, we randomly select 100 test examples from the\nCNN/Daily Mail dataset. For each example, we show the original article, the ground truth summary\nas well as summaries generated by different models side by side to a human evaluator. The human\nevaluator does not know which summaries come from which model or which one is the ground truth.\nTwo scores from 1 to 10 are then assigned to each summary, one for relevance (how well does the\nsummary capture the important parts of the article) and one for readability (how well-written the\nsummary is). Each summary is rated by 5 different human evaluators on Amazon Mechanical Turk\nand the results are averaged across all examples and evaluators.\nResults : Our human evaluation results are shown in Table 5. We can see that even though RL\nhas the highest ROUGE-1 and ROUGE-L scores, it produces the least readable summaries among\nour experiments. The most common readability issue observed in our RL results, as shown in the\nexample of Table 3, is the presence of short and truncated sentences towards the end of sequences.\nThis con\ufb01rms that optimizing for single discrete evaluation metric such as ROUGE with RL can be\ndetrimental to the model quality.\nOn the other hand, our RL+ML summaries obtain the highest readability and relevance scores among\nour models, hence solving the readability issues of the RL model while also having a higher ROUGE\nscore than ML. This demonstrates the usefulness and value of our RL+ML training method for\nabstractive summarization.",
                "subsection": []
            }
        ]
    },
    {
        "id": "5",
        "section": "Datasets",
        "text": "",
        "subsection": []
    },
    {
        "id": "6",
        "section": "Results",
        "text": "",
        "subsection": []
    },
    {
        "id": "7",
        "section": "Conclusion",
        "text": "We presented a new model and training procedure that obtains state-of-the-art results in text summa-\nrization for the CNN/Daily Mail, improves the readability of the generated summaries and is better\nsuited to long output sequences. We also run our abstractive model on the NYT dataset for the \ufb01rst\ntime. We saw that despite their common use for evaluation, ROUGE scores have their shortcom-\nings and should not be the only metric to optimize on summarization model for long sequences.\nOur intra-attention decoder and combined training objective could be applied to other sequence-to-\nsequence tasks with long inputs and outputs, which is an interesting direction for further research.\nREFERENCES\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\nlearning to align and translate. arXiv preprint arXiv:1409.0473 , 2014.\nQian Chen, Xiaodan Zhu, Zhenhua Ling, Si Wei, and Hui Jiang. Distraction-based neural networks\nfor modeling documents. In Proceedings of the Twenty-Fifth International Joint Conference on\nArti\ufb01cial Intelligence (IJCAI-16) , pp. 2754\u20132760, 2016.\nJianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\nreading. arXiv preprint arXiv:1601.06733 , 2016.\nSumit Chopra, Michael Auli, Alexander M Rush, and SEAS Harvard. Abstractive sentence sum-\nmarization with attentive recurrent neural networks. Proceedings of NAACL-HLT16 , pp. 93\u201398,\nBonnie Dorr, David Zajic, and Richard Schwartz. Hedge trimmer: A parse-and-trim approach to\nheadline generation. In Proceedings of the HLT-NAACL 03 on Text summarization workshop-\nVolume 5 , pp. 1\u20138. Association for Computational Linguistics, 2003.\nGreg Durrett, Taylor Berg-Kirkpatrick, and Dan Klein. Learning-based single-document summa-\nrization with compression and anaphoricity constraints. arXiv preprint arXiv:1603.08887 , 2016.\nCaglar Gulcehre, Sungjin Ahn, Ramesh Nallapati, Bowen Zhou, and Yoshua Bengio. Pointing the\nunknown words. arXiv preprint arXiv:1603.08148 , 2016.\nKarl Moritz Hermann, Tomas Kocisky, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa\nSuleyman, and Phil Blunsom. Teaching machines to read and comprehend. In Advances in\nNeural Information Processing Systems , pp. 1693\u20131701, 2015.\nSepp Hochreiter and J \u00a8urgen Schmidhuber. Long short-term memory. Neural computation , 9(8):\n1735\u20131780, 1997.\nKai Hong and Ani Nenkova. Improving the estimation of word importance for news multi-document\nsummarization-extended technical report. 2014.\nKai Hong, Mitchell Marcus, and Ani Nenkova. System combination for multi-document summa-\nrization. In EMNLP , pp. 107\u2013117, 2015.\nHakan Inan, Khashayar Khosravi, and Richard Socher. Tying word vectors and word classi\ufb01ers: A\nloss framework for language modeling. Proceedings of the International Conference on Learning\nRepresentations , 2017.\nDiederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint\narXiv:1412.6980 , 2014.\nJunyi Jessy Li, Kapil Thadani, and Amanda Stent. The role of discourse units in near-extractive\nsummarization. In 17th Annual Meeting of the Special Interest Group on Discourse and Dialogue ,\npp. 137, 2016.\nChin-Yew Lin. Rouge: A package for automatic evaluation of summaries. In Text summarization\nbranches out: Proceedings of the ACL-04 workshop , volume 8. Barcelona, Spain, 2004.\nChia-Wei Liu, Ryan Lowe, Iulian V Serban, Michael Noseworthy, Laurent Charlin, and Joelle\nPineau. How not to evaluate your dialogue system: An empirical study of unsupervised eval-\nuation metrics for dialogue response generation. arXiv preprint arXiv:1603.08023 , 2016.\nChristopher D Manning, Mihai Surdeanu, John Bauer, Jenny Rose Finkel, Steven Bethard, and\nDavid McClosky. The stanford corenlp natural language processing toolkit. In ACL (System\nDemonstrations) , pp. 55\u201360, 2014.\nStephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture\nmodels. Proceedings of the International Conference on Learning Representations , 2017.\nTomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. Distributed represen-\ntations of words and phrases and their compositionality. In Advances in neural information pro-\ncessing systems , pp. 3111\u20133119, 2013.\nRamesh Nallapati, Bowen Zhou, C \u00b8 a \u02d8glar G \u00a8ulc \u00b8ehre, Bing Xiang, et al. Abstractive text summarization\nusing sequence-to-sequence rnns and beyond. arXiv preprint arXiv:1602.06023 , 2016.\nRamesh Nallapati, Feifei Zhai, and Bowen Zhou. Summarunner: A recurrent neural network based\nsequence model for extractive summarization of documents. Proceedings of the 31st AAAI con-\nference , 2017.\nMohammad Norouzi, Samy Bengio, Navdeep Jaitly, Mike Schuster, Yonghui Wu, Dale Schuurmans,\net al. Reward augmented maximum likelihood for neural structured prediction. In Advances In\nNeural Information Processing Systems , pp. 1723\u20131731, 2016.\nBenjamin Nye and Ani Nenkova. Identi\ufb01cation and characterization of newsworthy verbs in world\nnews. InJeffrey Pennington, Richard Socher, and Christopher D Manning. Glove: Global vectors for word\nrepresentation. In EMNLP , volume 14, pp. 1532\u20131543, 2014.\nO\ufb01r Press and Lior Wolf. Using the output embedding to improve language models. arXiv preprint\narXiv:1608.05859 , 2016.\nMarc\u2019Aurelio Ranzato, Sumit Chopra, Michael Auli, and Wojciech Zaremba. Sequence level train-\ning with recurrent neural networks. arXiv preprint arXiv:1511.06732 , 2015.\nSteven J Rennie, Etienne Marcheret, Youssef Mroueh, Jarret Ross, and Vaibhava Goel. Self-critical\nsequence training for image captioning. arXiv preprint arXiv:1612.00563 , 2016.\nAlexander M Rush, Sumit Chopra, and Jason Weston. A neural attention model for abstractive\nsentence summarization. arXiv preprint arXiv:1509.00685 , 2015.\nEvan Sandhaus. The new york times annotated corpus. Linguistic Data Consortium, Philadelphia ,\n6(12):e26752, 2008.\nBaskaran Sankaran, Haitao Mi, Yaser Al-Onaizan, and Abe Ittycheriah. Temporal attention model\nfor neural machine translation. arXiv preprint arXiv:1608.02927 , 2016.\nAbigail See, Peter J. Liu, and Christopher D. Manning. Get to the point: Summarization with\npointer-generator networks. In Proceedings of the 55th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers) , pp. 1073\u20131083, July 2017.\nIlya Sutskever, Oriol Vinyals, and Quoc V Le. Sequence to sequence learning with neural networks.\nInAdvances in neural information processing systems , pp. 3104\u20133112, 2014.\nArun Venkatraman, Martial Hebert, and J Andrew Bagnell. Improving multi-step prediction of\nlearned time series models. In AAAI , pp. 3024\u20133030, 2015.\nOriol Vinyals, Meire Fortunato, and Navdeep Jaitly. Pointer networks. In Advances in Neural\nInformation Processing Systems , pp. 2692\u20132700, 2015.\nRonald J Williams. Simple statistical gradient-following algorithms for connectionist reinforcement\nlearning. Machine learning , 8(3-4):229\u2013256, 1992.\nRonald J Williams and David Zipser. A learning algorithm for continually running fully recurrent\nneural networks. Neural computation , 1(2):270\u2013280, 1989.\nYonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang Macherey,\nMaxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google\u2019s neural machine trans-\nlation system: Bridging the gap between human and machine translation. arXiv preprint\narXiv:1609.08144 , 2016.\nYinfei Yang and Ani Nenkova. Detecting information-dense texts in multiple news domains. In\nAAAI , pp. 1650\u20131656, 2014.\nWenyuan Zeng, Wenjie Luo, Sanja Fidler, and Raquel Urtasun. Ef\ufb01cient summarization with read-\nagain and copy mechanism. arXiv preprint arXiv:1611.03382 , 2016.\nA NYT DATASET\nA.1 P REPROCESSING\nWe remove all documents that do not have a full article text, abstract or headline. We concatenate the\nheadline, byline and full article text, separated by special tokens, to produce a single input sequence\nfor each example. We tokenize the input and abstract pairs with the Stanford tokenizer (Manning\net al., 2014). We convert all tokens to lower-case and replace all numbers with \u201c0\u201d, remove \u201c(s)\u201d and\n\u201c(m)\u201d marks in the abstracts and all occurrences of the following words, singular or plural, if they are\nand \u201cdrawing\u201d. Since the NYT abstracts almost never contain periods, we consider them multi-\nsentence summaries if we split sentences based on semicolons. This allows us to make the summary\nformat and evaluation procedure similar to the CNN/Daily Mail dataset. These pre-processing steps\ngive us an average of 549 input tokens and 40 output tokens per example, after limiting the input\nand output lengths to 800 and 100 tokens.\nA.2 D ATASET SPLITS\nWe created our own training, validation, and testing splits for this dataset. Instead of producing\nrandom splits, we sorted the documents by their publication date in chronological order and used\nthe \ufb01rst 90% (589,284 examples) for training, the next 5% (32,736) for validation, and the remaining\n5% (32,739) for testing. This makes our dataset splits easily reproducible and follows the intuition\nthat if used in a production environment, such a summarization model would be used on recent\narticles rather than random ones.\nA.3 P OINTER SUPERVISION\nWe run each input and abstract sequence through the Stanford named entity recognizer (NER) (Man-\nning et al., 2014). For all named entity tokens in the abstract if the type \u201cPERSON\u201d, \u201cLOCATION\u201d,\n\u201cORGANIZATION\u201d or \u201cMISC\u201d, we \ufb01nd their \ufb01rst occurrence in the input sequence. We use this\ninformation to supervise p(ut)(Equation 11) and \u000be\nti(Equation 4) during training. Note that the\nNER tagger is only used to create the dataset and is no longer needed during testing, thus we\u2019re\nnot adding any dependencies to our model. We also add pointer supervision for out-of-vocabulary\noutput tokens if they are present in the input.\nB H YPERPARAMETERS AND IMPLEMENTATION DETAILS\nFor ML training, we use the teacher forcing algorithm with the only difference that at each decoding\nstep, we choose with a 25% probability the previously generated token instead of the ground-truth\ntoken as the decoder input token yt\u00001, which reduces exposure bias (Venkatraman et al., 2015). We\nuse a\r= 0:9984 for the ML+RL loss function.\nWe use two 200-dimensional LSTMs for the bidirectional encoder and one 400-dimensional LSTM\nfor the decoder. We limit the input vocabulary size to 150,000 tokens, and the output vocabulary to\n50,000 tokens by selecting the most frequent tokens in the training set. Input word embeddings are\n100-dimensional and are initialized with GloVe (Pennington et al., 2014). We train all our models\nwith Adam (Kingma & Ba, 2014) with a batch size of 50 and a learning rate \u000bof 0.001 for ML\ntraining and 0.0001 for RL and ML+RL training. At test time, we use beam search of width 5 on all",
        "subsection": []
    },
    {
        "missing": []
    },
    {
        "references": []
    },
    {
        "title": "A Deep Reinforced Model for Abstractive Summarization",
        "arxiv_id": "1705.04304"
    }
]