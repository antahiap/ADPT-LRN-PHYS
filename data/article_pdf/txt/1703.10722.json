[
    {
        "id": "",
        "section": "Abstract",
        "text": "We present two simple ways of reducing the number of parameters and acceler-\nating the training of large Long Short-Term Memory (LSTM) networks: the \ufb01rst\none is \u201dmatrix factorization by design\u201d of LSTM matrix into the product of two\nsmaller matrices, and the second one is partitioning of LSTM matrix, its inputs\nand states into the independent groups. Both approaches allow us to train large\nLSTM networks signi\ufb01cantly faster to the near state-of the art perplexity while\nusing signi\ufb01cantly less RNN parameters.",
        "subsection": [
            {
                "id": "1.1",
                "section": "Long short -term memory overview",
                "text": "Learning long-range dependencies with Recurrent Neural Networks (RNN) is challenging due to\nthe vanishing and exploding gradient problems (Bengio et al., 1994; Pascanu et al., 2013). To ad-\ndress this issue, the LSTM cell has been introduced by Hochreiter & Schmidhuber (1997), with the\nfollowing recurrent computations:\nLSTM :ht\u00001; ct\u00001; xt!ht; ct: (1)\nwhere xtis input, htis cell\u2019s state, and ctis cell\u2019s memory. We consider LSTM cell with projection\nof size p, LSTMP, where Equation 1 is computed as follows (Sak et al., 2014; Zaremba et al., 2014).\nFirst, cell gates (i; f; o; g )are computed:\n0\nB@i\nf\no\ng1\nCA=0\nB@sigm\nsigm\nsigm\ntanh1\nCAT\u0012\nxt\nht\u00001\u0013\n(2)\nwhere xt2Rp,ht2Rp, andT:R2p!R4nis an af\ufb01ne transform T=W\u0003[xt; ht\u00001] +b.\nNext state ht2Rpand memory ct2Rnare computed using following equations:\nct=f\fct\u00001+i\fg;ht=P(o\ftanh(ct))\nwhere P:Rn!Rpis a linear projection. The major part of LSTMP cell computation is in\ncomputing af\ufb01ne transform Tbecause it involves multiplication with 4n\u00022pmatrix W. Thus we\nfocus on reducing the number of parameters inWorkshop track - ICLR 2017",
                "subsection": []
            },
            {
                "id": "1.2",
                "section": "Related work",
                "text": "The partition of layer into parallel groups have been introduced by Krizhevsky et al. (2012) in\nAlexNet, where some convolutional layers have been divided into two groups to split the model\nbetween two GPUs. Multi-group convnets have been widely used to reduce network weights and\nrequired compute, for example by Esser et al. (2016). This multi-group approach was extended to the\nextreme in Xception architecture by Chollet (2016). The idea of factorization of large convolutinal\nlayer into the stack of layers with smaller \ufb01lters was used, for example, in VGG networks (Simonyan\n& Zisserman, 2014), and in ResNet \u201cbottleneck design\u201d (He et al., 2016). Denil et al. (2013) have\nshown that it is possible to train several different deep architectures by learning only a small number\nof weights and predicting the rest. In case of LSTM networks, ConvLSTM (Shi et al., 2015), has\nbeen introduced to better exploit possible spatiotemporal correlations, which is conceptually similar\nto grouping.",
                "subsection": []
            }
        ]
    },
    {
        "id": "1",
        "section": "Introduction",
        "text": "LSTM networks (Hochreiter & Schmidhuber, 1997) have been successfully used in language model-\ning (Jozefowicz et al., 2016; Shazeer et al., 2017), speech recognition (Xiong et al., 2016), machine\ntranslation (Wu et al., 2016), and many other tasks. However, these networks have millions of\nparameters, and require weeks of training on multi-GPU systems.\nWe introduce two modi\ufb01cations of LSTM cell with projection, LSTMP (Sak et al., 2014), to reduce\nthe number of parameters and speed-up training. The \ufb01rst method, factorized LSTM (F-LSTM)\napproximates big LSTM matrix with a product of two smaller matrices. The second method, group\nLSTM (G-LSTM) partitions LSTM cell into the independent groups. We test F-LSTM and G-LSTM\narchitectures on the task of language modeling using One Billion Word Benchmark (Chelba et al.,\n2013). As a baseline, we used BIGLSTM model without CNN inputs described by Jozefowicz et al.\n(2016). We train all networks for 1 week on a DGX Station system with 4 Tesla V100 GPUs, after\nwhich BIGLSTM\u2019s evaluation perplexity was 35.1. Our G-LSTM based model got 36 and F-LSTM\nbased model got 36.3 while using two to three times less RNN parameters.",
        "subsection": [
            {
                "id": "2.1",
                "section": "Factorized lstm cell",
                "text": "Factorized LSTM (F-LSTM) replaces matrix Wby the product of two smaller matrices that essen-\ntially try to approximate WasW\u0019W2\u0003W1, where W1is of size 2p\u0002r,W2isr\u00024n, and\nr < p < =n(\u201dfactorization by design\u201d). The key assumption here is that Wcan be well approxi-\nmated by the matrix of rank r. Such approximation contains less LSTMP parameters than original\nmodel - (r\u00032p+r\u00034n)versus (2p\u00034n)and, therefore, can be computed faster and synchronized\nfaster in the case of distributed training.\nFigure 1: Language model using: (a) 2 regular LSTM layers, (b) 2 F-LSTM layers, and (c) 2 G-\nLSTM layers with 2 group in each layer. Equations inside cells show what kind of af\ufb01ne transforms\nare computed by those cells at each time step. Here d= (x; h)for models without groups and\nd1 = (x1; h1),d2 = (x2; h2)for model with two groups; and time index dropped for clarity.",
                "subsection": []
            },
            {
                "id": "2.2",
                "section": "Group lstm cell",
                "text": "This approach is inspired by groups in Alexnet (Krizhevsky et al., 2012). We postulate that some\nparts of the input xtand hidden state htcan be thought of as independent feature groups. For\nexample, if we use two groups, then both xtandhtare effectively split into two vectors concatenated\ntogether xt= (x1\nt; x2\nt)andht= (h1\nt; h2\nt), with hi\ntonly dependent on xi\nt,hi\nt\u00001and cell\u2019s memory\nstate. Therefore, for kgroups Equation 2 changes to:\n0\nB@i\nf\no\ng1\nCA=0\nB@0\nB@sigm\nsigm\nsigm\ntanh1\nCAT1\u0012\nx1\nt\nh1\nt\u00001\u0013\n; :::;0\nB@sigm\nsigm\nsigm\ntanh1\nCATk\u0012\nxk\nt\nhk\nt\u00001\u00131\nCA (3)\nwhere, Tjis a group j\u2019s af\ufb01ne transform from R2p=ktoR4n=k. The partitioned Twill now have\nk\u00034n\u00032p\nk\u0003kparameters. This cell architecture is well suited for model parallelism since every group\nWorkshop track - ICLR 2017\nTable 1: One Billion Words benchmark evaluation results after 1 week of training using one DGX\nStation with 4 Tesla V100 GPUs.\nModel Perplexity Step Num of RNN parameters Words/sec\nBIGLSTM baseline 35.1 0.99M 151,060,480 33.8K\nBIG F-LSTM F512 36.3 1.67 M 52,494,336 56.5K\nBIG G-LSTM G-2 36 1.37M 83,951,616 41.7K\nBIG G-LSTM G-4 40.6 1.128M 50,397,184 56K\nBIG G-LSTM G-8 39.4 850.4K 33,619,968 58.5K\nthe Figure 1 (c). While this might look similar to ensemble (Shazeer et al., 2017) or multi-tower\n(Ciregan et al., 2012) models, the key differences are: (1) input to different groups is different\nand assumed independent, and (2) instead of computing ensemble output, it is concatenated into\nindependent pieces.",
                "subsection": []
            }
        ]
    },
    {
        "id": "2",
        "section": "Models",
        "text": "",
        "subsection": [
            {
                "id": "3.1",
                "section": "Future research",
                "text": "While one might go further and try to approximate transform Tusing arbitrary feed forward neural\nnetwork with 2pinputs and 4noutputs, during our initial experiments we did not see immediate\nbene\ufb01ts of doing so. Hence, it remains a topic of future research.\nIt might be possible to reduce the number of RNN parameters even further by stacking G-LSTM\nlayers with increasing group counts on top of each other. In our second, smaller experiment, we\nreplace the second layer of BIG G-LSTM-G4 network by the layer with 8 groups instead of 4, and\ncall it BIG G-LSTM-G4-G8. We let both BIG G-LSTM-G4 and BIG G-LSTM-G4-G8 ran for 1\nweek on 4 GPUs each and achieved very similar perplexities. Hence, the model with \u201chierarchical\u201d\ngroups did not lose much accuracy, ran faster and got better perplexity. Such \u201chierarchical\u201d group\nlayers look intriguing as they might provide a way for learning different levels of abstractions but\nWorkshop track - ICLR 2017\nAcknowledgements We are grateful to Scott Gray and Ciprian Chelba for helping us identify and\ncorrect issues with earlier versions of this work.\nREFERENCES\nYoshua Bengio, Patrice Simard, and Paolo Frasconi. Learning long-term dependencies with gradient\ndescent is dif\ufb01cult. IEEE transactions on neural networks , 5(2):157\u2013166, 1994.\nCiprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge, Thorsten Brants, Phillipp Koehn, and Tony\nRobinson. One billion word benchmark for measuring progress in statistical language modeling.\narXiv preprint arXiv:1312.3005 , 2013.\nFranc \u00b8ois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv preprint\narXiv:1610.02357 , 2016.\nDan Ciregan, Ueli Meier, and J \u00a8urgen Schmidhuber. Multi-column deep neural networks for image\nclassi\ufb01cation. In Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on ,\npp. 3642\u20133649. IEEE, 2012.\nMisha Denil, Babak Shakibi, Laurent Dinh, Nando de Freitas, et al. Predicting parameters in deep\nlearning. In Advances in Neural Information Processing Systems , pp. 2148\u20132156, 2013.\nSteven K Esser, Paul A Merolla, John V Arthur, Andrew S Cassidy, Rathinakumar Appuswamy,\nAlexander Andreopoulos, David J Berg, Jeffrey L McKinstry, Timothy Melano, Davis R Barch,\net al. Convolutional networks for fast, energy-ef\ufb01cient neuromorphic computing. Proceedings of\nthe National Academy of Sciences , pp. 201604850, 2016.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-\nnition. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , June 2016.\nSepp Hochreiter and J \u00a8urgen Schmidhuber. Long short-term memory. Neural computation , 9(8):\n1735\u20131780, 1997.\nRafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring the\nlimits of language modeling. arXiv preprint arXiv:1602.02410 , 2016.\nAlex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classi\ufb01cation with deep convo-\nlutional neural networks. In Advances in neural information processing systems , pp. 1097\u20131105,\n2012.\nRazvan Pascanu, Tomas Mikolov, and Yoshua Bengio. On the dif\ufb01culty of training recurrent neural\nnetworks. ICML (3) , 28:1310\u20131318, 2013.\nHasim Sak, Andrew W Senior, and Franc \u00b8oise Beaufays. Long short-term memory recurrent neural\nnetwork architectures for large scale acoustic modeling. In Interspeech , pp. 338\u2013342, 2014.\nNoam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer.\narXiv preprint arXiv:1701.06538 , 2017.\nXingjian Shi, Zhourong Chen, Hao Wang, Dit-Yan Yeung, Wai-kin Wong, and Wang-chun Woo.\nConvolutional lstm network: A machine learning approach for precipitation nowcasting. In\nProceedings of the 28th International Conference on Neural Information Processing Systems ,\nNIPS\u201915, pp. 802\u2013810, Cambridge, MA, USA, 2015. MIT Press. URL http://dl.acm.\norg/citation.cfm?id=2969239.2969329 .\nKaren Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image\nrecognition. arXiv preprint arXiv:1409.1556 , 2014.\nYonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang Macherey,\nMaxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google\u2019s neural machine trans-\nlation system: Bridging the gap between human and machine translation. arXiv preprint\nWorkshop track - ICLR 2017\nWayne Xiong, Jasha Droppo, Xuedong Huang, Frank Seide, Mike Seltzer, Andreas Stolcke, Dong\nYu, and Geoffrey Zweig. Achieving human parity in conversational speech recognition. arXiv\npreprint arXiv:1610.05256 , 2016.\nWojciech Zaremba, Ilya Sutskever, and Oriol Vinyals. Recurrent neural network regularization.\nWorkshop track - ICLR 2017\nAPPENDIX : TRAINING LOSS FOR 4 LSTM- LIKE MODELS\nFigure 2: Y-axis: same for (A) and (B) - training loss log-scale, X-axis: for (A) - step, or mini-batch\ncount, for (B) - hours (w.g. wall time) of training. BIGLSTM baseline, BIG G-LSTM-G4, BIG\nG-LSTM-G16, and BIG F-LSTM-F512 all trained for exactly one week. It is clearly visible, that at\nthe same step count, the model with more parameters wins. On the other hand, factorized models\ncan do signi\ufb01cantly more iterations in the given amount of time and therefore get to the better results",
                "subsection": []
            }
        ]
    },
    {
        "id": "3",
        "section": "Experiments and results",
        "text": "For testing we used the task of learning the joint probabilities over word sequences of arbitrary\nlengths n:P(w1; :::; w n) =Qn\ni=1P(wijw1; :::; w i\u00001), such that \u201creal\u201d sentences have high prob-\nabilities compared to the random sequences of words. Figure 1 (a) shows the typical LSTM-based\nmodel, where \ufb01rst the words are embedded into the low dimensional dense input for RNN, then the\n\u201ccontext\u201d is learned using RNNs via number of steps and, \ufb01nally, the softmax layer converts RNN\noutput into the probability distribution P(w1; :::; w n). We test the following models:\n\u000fBIGLSTM - model with projections but without CNN inputs from Jozefowicz et al. (2016)\n\u000fBIG F-LSTM F512 - with intermediate rank of 512 for LSTM matrix W,\n\u000fBIG G-LSTM G-4, with 4 groups in both layers\n\u000fBIG G-LSTM G-16, with 16 groups in both layers.\nWe train all models on DGX Station with 4 GV100 GPUs for one ween using Adagrad optimizer,\nprojection size of 1024, cell size of 8192, mini-batch of 256 per GPU, sampled softmax with 8192\nsamples and 0.2 learning rate. Note that the use of projection is crucial as it helps to keep down\nembedding and softmax layer sizes. Table 1 summarizes our experiments.\nJudging from the training loss Plots 2 in Appendix , it is clearly visible that at the same step count,\nmodel with more parameters wins. However, given the same amount of time, factorized models\ntrain faster. While the difference between BIGLSTM and BIG G-LSTM-G2 is clearly visible, BIG\nG-LSTM-G2 contains almost 2 times less RNN parameters than BIGLSTM, trains faster and, as a\nresults, achieves similar evaluation perplexity within the same training time budget (1 week).\nOur code is available at https://github.com/okuchaiev/f-lm",
        "subsection": []
    },
    {
        "missing": []
    },
    {
        "references": []
    },
    {
        "title": "Factorization tricks for LSTM networks",
        "arxiv_id": "1703.10722"
    }
]