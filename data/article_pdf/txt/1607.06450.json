[
    {
        "id": "",
        "section": "Abstract",
        "text": "Training state-of-the-art, deep neural networks is computationally expensive. One\nway to reduce the training time is to normalize the activities of the neurons. A\nrecently introduced technique called batch normalization uses the distribution of\nthe summed input to a neuron over a mini-batch of training cases to compute a\nmean and variance which are then used to normalize the summed input to that\nneuron on each training case. This signi\ufb01cantly reduces the training time in feed-\nforward neural networks. However, the effect of batch normalization is dependent\non the mini-batch size and it is not obvious how to apply it to recurrent neural net-\nworks. In this paper, we transpose batch normalization into layer normalization by\ncomputing the mean and variance used for normalization from all of the summed\ninputs to the neurons in a layer on a single training case. Like batch normalization,\nwe also give each neuron its own adaptive bias and gain which are applied after\nthe normalization but before the non-linearity. Unlike batch normalization, layer\nnormalization performs exactly the same computation at training and test times.\nIt is also straightforward to apply to recurrent neural networks by computing the\nnormalization statistics separately at each time step. Layer normalization is very\neffective at stabilizing the hidden state dynamics in recurrent networks. Empiri-\ncally, we show that layer normalization can substantially reduce the training time\ncompared with previously published techniques.",
        "subsection": []
    },
    {
        "id": "1",
        "section": "Introduction",
        "text": "Deep neural networks trained with some version of Stochastic Gradient Descent have been shown\nto substantially outperform previous approaches on various supervised learning tasks in computer\nvision [Krizhevsky et al., 2012] and speech processing [Hinton et al., 2012]. But state-of-the-art\ndeep neural networks often require many days of training. It is possible to speed-up the learning\nby computing gradients for different subsets of the training cases on different machines or splitting\nthe neural network itself over many machines [Dean et al., 2012], but this can require a lot of com-\nmunication and complex software. It also tends to lead to rapidly diminishing returns as the degree\nof parallelization increases. An orthogonal approach is to modify the computations performed in\nthe forward pass of the neural net to make learning easier. Recently, batch normalization [Ioffe and\nSzegedy, 2015] has been proposed to reduce training time by including additional normalization\nstages in deep neural networks. The normalization standardizes each summed input using its mean\nand its standard deviation across the training data. Feedforward neural networks trained using batch\nnormalization converge faster even with simple SGD. In addition to training time improvement, the\nstochasticity from the batch statistics serves as a regularizer during training.\nDespite its simplicity, batch normalization requires running averages of the summed input statis-\ntics. In feed-forward networks with \ufb01xed depth, it is straightforward to store the statistics separately\nfor each hidden layer. However, the summed inputs to the recurrent neurons in a recurrent neu-\nral network (RNN) often vary with the length of the sequence so applying batch normalization to\ntion cannot be applied to online learning tasks or to extremely large distributed models where the\nminibatches have to be small.\nThis paper introduces layer normalization, a simple normalization method to improve the training\nspeed for various neural network models. Unlike batch normalization, the proposed method directly\nestimates the normalization statistics from the summed inputs to the neurons within a hidden layer\nso the normalization does not introduce any new dependencies between training cases. We show that\nlayer normalization works well for RNNs and improves both the training time and the generalization\nperformance of several existing RNN models.",
        "subsection": []
    },
    {
        "id": "2",
        "section": "Background",
        "text": "A feed-forward neural network is a non-linear mapping from a input pattern xto an output vector\ny. Consider the lthhidden layer in a deep feed-forward, neural network, and let albe the vector\nrepresentation of the summed inputs to the neurons in that layer. The summed inputs are computed\nthrough a linear projection with the weight matrix Wland the bottom-up inputs hlgiven as follows:\nal\ni=wl\ni>hlhl+1\ni=f(al\ni+bl\ni) (1)\nwheref(\u0001)is an element-wise non-linear function and wl\niis the incoming weights to the ithhidden\nunits andbl\niis the scalar bias parameter. The parameters in the neural network are learnt using\ngradient-based optimization algorithms with the gradients being computed by back-propagation.\nOne of the challenges of deep learning is that the gradients with respect to the weights in one layer\nare highly dependent on the outputs of the neurons in the previous layer especially if these outputs\nchange in a highly correlated way. Batch normalization [Ioffe and Szegedy, 2015] was proposed\nto reduce such undesirable \u201ccovariate shift\u201d. The method normalizes the summed inputs to each\nhidden unit over the training cases. Speci\ufb01cally, for the ithsummed input in the lthlayer, the batch\nnormalization method rescales the summed inputs according to their variances under the distribution\nof the data\n\u0016al\ni=gl\ni\n\u001bl\ni\u0000\nal\ni\u0000\u0016l\ni\u0001\n\u0016l\ni=E\nx\u0018P(x)\u0002\nal\ni\u0003\n\u001bl\ni=r\nE\nx\u0018P(x)h\u0000\nal\ni\u0000\u0016l\ni\u00012i\n(2)\nwhere \u0016al\niis normalized summed inputs to the ithhidden unit in the lthlayer andgiis a gain parame-\nter scaling the normalized activation before the non-linear activation function. Note the expectation\nis under the whole training data distribution. It is typically impractical to compute the expectations\nin Eq. (2) exactly, since it would require forward passes through the whole training dataset with the\ncurrent set of weights. Instead, \u0016and\u001bare estimated using the empirical samples from the current\nmini-batch. This puts constraints on the size of a mini-batch and it is hard to apply to recurrent\nneural networks.",
        "subsection": [
            {
                "id": "3.1",
                "section": "Layer normalized recurrent neural networks",
                "text": "The recent sequence to sequence models [Sutskever et al., 2014] utilize compact recurrent neural\nnetworks to solve sequential prediction problems in natural language processing. It is common\namong the NLP tasks to have different sentence lengths for different training cases. This is easy to\ndeal with in an RNN because the same weights are used at every time-step. But when we apply batch\nnormalization to an RNN in the obvious way, we need to to compute and store separate statistics for\neach time step in a sequence. This is problematic if a test sequence is longer than any of the training\nsequences. Layer normalization does not have such problem because its normalization terms depend\nonly on the summed inputs to a layer at the current time-step. It also has only one set of gain and\nbias parameters shared over all time-steps.\nIn a standard RNN, the summed inputs in the recurrent layer are computed from the current input\nxtand previous vector of hidden states ht\u00001which are computed as at=Whhht\u00001+Wxhxt. The\nlayer normalized recurrent layer re-centers and re-scales its activations using the extra normalization\nterms similar to Eq. (3):\nht=fhg\n\u001bt\f\u0000\nat\u0000\u0016t\u0001\n+bi\n\u0016t=1\nHHX\ni=1at\ni\u001bt=vuut1\nHHX\ni=1(at\ni\u0000\u0016t)2(4)\nwhereWhhis the recurrent hidden to hidden weights and Wxhare the bottom up input to hidden\nweights.\fis the element-wise multiplication between two vectors. bandgare de\ufb01ned as the bias\nand gain parameters of the same dimension as ht.\nIn a standard RNN, there is a tendency for the average magnitude of the summed inputs to the recur-\nrent units to either grow or shrink at every time-step, leading to exploding or vanishing gradients. In\na layer normalized RNN, the normalization terms make it invariant to re-scaling all of the summed\ninputs to a layer, which results in much more stable hidden-to-hidden dynamics.",
                "subsection": []
            }
        ]
    },
    {
        "id": "3",
        "section": "Layer normalization",
        "text": "We now consider the layer normalization method which is designed to overcome the drawbacks of\nbatch normalization.\nNotice that changes in the output of one layer will tend to cause highly correlated changes in the\nsummed inputs to the next layer, especially with ReLU units whose outputs can change by a lot.\nThis suggests the \u201ccovariate shift\u201d problem can be reduced by \ufb01xing the mean and the variance of\nthe summed inputs within each layer. We, thus, compute the layer normalization statistics over all\nthe hidden units in the same layer as follows:\n\u0016l=1\nHHX\ni=1al\ni\u001bl=vuut1\nHHX\ni=1\u0000\nal\ni\u0000\u0016l\u00012(3)\nwhereHdenotes the number of hidden units in a layer. The difference between Eq. (2) and Eq. (3)\nis that under layer normalization, all the hidden units in a layer share the same normalization terms\n\u0016and\u001b, but different training cases have different normalization terms. Unlike batch normalization,\nlayer normaliztion does not impose any constraint on the size of a mini-batch and it can be used in\nthe pure online regime with batch size 1.",
        "subsection": []
    },
    {
        "id": "4",
        "section": "Related work",
        "text": "Batch normalization has been previously extended to recurrent neural networks [Laurent et al., 2015,\nAmodei et al., 2015, Cooijmans et al., 2016]. The previous work [Cooijmans et al., 2016] suggests\nthe best performance of recurrent batch normalization is obtained by keeping independent normal-\nization statistics for each time-step. The authors show that initializing the gain parameter in the\nrecurrent batch normalization layer to 0.1 makes signi\ufb01cant difference in the \ufb01nal performance of\nthe model. Our work is also related to weight normalization [Salimans and Kingma, 2016]. In\nweight normalization, instead of the variance, the L2 norm of the incoming weights is used to\nnormalize the summed inputs to a neuron. Applying either weight normalization or batch normal-\nization using expected statistics is equivalent to have a different parameterization of the original\nfeed-forward neural network. Re-parameterization in the ReLU network was studied in the Path-\nnormalized SGD [Neyshabur et al., 2015]. Our proposed layer normalization method, however, is\nnot a re-parameterization of the original neural network. The layer normalized model, thus, has\ndifferent invariance properties than the other methods, that we will study in the following section.",
        "subsection": [
            {
                "id": "5.1",
                "section": "Invariance under weights and data transformations",
                "text": "The proposed layer normalization is related to batch normalization and weight normalization. Al-\nthough, their normalization scalars are computed differently, these methods can be summarized as\nnormalizing the summed inputs aito a neuron through the two scalars \u0016and\u001b. They also learn an\nadaptive bias band gaingfor each neuron after the normalization.\nhi=f(gi\n\u001bi(ai\u0000\u0016i) +bi) (5)\nNote that for layer normalization and batch normalization, \u0016and\u001bis computed according to Eq. 2\nand 3. In weight normalization, \u0016is 0, and\u001b=kwk2.\nWeight matrix Weight matrix Weight vector Dataset Dataset Single training case\nre-scaling re-centering re-scaling re-scaling re-centering re-scaling\nBatch norm Invariant No Invariant Invariant Invariant No\nWeight norm Invariant No Invariant No No No\nLayer norm Invariant Invariant No Invariant No Invariant\nTable 1: Invariance properties under the normalization methods.\nTable 1 highlights the following invariance results for three normalization methods.\nWeight re-scaling and re-centering: First, observe that under batch normalization and weight\nnormalization, any re-scaling to the incoming weights wiof a single neuron has no effect on the\nnormalized summed inputs to a neuron. To be precise, under batch and weight normalization, if\nthe weight vector is scaled by \u000e, the two scalar \u0016and\u001bwill also be scaled by \u000e. The normalized\nsummed inputs stays the same before and after scaling. So the batch and weight normalization are\ninvariant to the re-scaling of the weights. Layer normalization, on the other hand, is not invariant\nto the individual scaling of the single weight vectors. Instead, layer normalization is invariant to\nscaling of the entire weight matrix and invariant to a shift to all of the incoming weights in the\nweight matrix. Let there be two sets of model parameters \u0012,\u00120whose weight matrices WandW0\ndiffer by a scaling factor \u000eand all of the incoming weights in W0are also shifted by a constant\nvector \r, that isW0=\u000eW+1\r>. Under layer normalization, the two models effectively compute\nthe same output:\nh0=f(g\n\u001b0(W0x\u0000\u00160) +b) =f(g\n\u001b0\u0000\n(\u000eW+1\r>)x\u0000\u00160\u0001\n+b)\n=f(g\n\u001b(Wx\u0000\u0016) +b) =h: (6)\nNotice that if normalization is only applied to the input before the weights, the model will not be\ninvariant to re-scaling and re-centering of the weights.\nData re-scaling and re-centering: We can show that all the normalization methods are invariant\nto re-scaling the dataset by verifying that the summed inputs of neurons stays constant under the\nchanges. Furthermore, layer normalization is invariant to re-scaling of individual training cases,\nbecause the normalization scalars \u0016and\u001bin Eq. (3) only depend on the current input data. Let x0\nbe a new data point obtained by re-scaling xby\u000e. Then we have,\nh0\ni=f(gi\n\u001b0\u0000\nw>\nix0\u0000\u00160\u0001\n+bi) =f(gi\n\u000e\u001b\u0000\n\u000ew>\nix\u0000\u000e\u0016\u0001\n+bi) =hi: (7)\nIt is easy to see re-scaling individual data points does not change the model\u2019s prediction under layer\nnormalization. Similar to the re-centering of the weight matrix in layer normalization, we can also\nshow that batch normalization is invariant to re-centering of the dataset.",
                "subsection": []
            },
            {
                "id": "5.2",
                "section": "Geometry of parameter space during learning",
                "text": "We have investigated the invariance of the model\u2019s prediction under re-centering and re-scaling of\nthe parameters. Learning, however, can behave very differently under different parameterizations,\neven though the models express the same underlying function. In this section, we analyze learning\nbehavior through the geometry and the manifold of the parameter space. We show that the normal-\nization scalar \u001bcan implicitly reduce learning rate and makes learning more stable.",
                "subsection": [
                    {
                        "id": "5.2.1",
                        "section": "Riemannian metric",
                        "text": "The learnable parameters in a statistical model form a smooth manifold that consists of all possible\ninput-output relations of the model. For models whose output is a probability distribution, a natural\nway to measure the separation of two points on this manifold is the Kullback-Leibler divergence\nbetween their model output distributions. Under the KL divergence metric, the parameter space is a\nRiemannian manifold.\nThe curvature of a Riemannian manifold is entirely captured by its Riemannian metric, whose\nquadratic form is denoted as ds2. That is the in\ufb01nitesimal distance in the tangent space at a point\nin the parameter space. Intuitively, it measures the changes in the model output from the parameter\nspace along a tangent direction. The Riemannian metric under KL was previously studied [Amari,\n1998] and was shown to be well approximated under second order Taylor expansion using the Fisher\ninformation matrix:\nds2= D KL\u0002\nP(yjx;\u0012)kP(yjx;\u0012+\u000e)\u0003\n\u00191\n2\u000e>F(\u0012)\u000e; (8)\nF(\u0012) = E\nx\u0018P(x);y\u0018P(yjx)\"\n@logP(yjx;\u0012)\n@\u0012@logP(yjx;\u0012)\n@\u0012>#\n; (9)\nwhere,\u000eis a small change to the parameters. The Riemannian metric above presents a geometric\nview of parameter spaces. The following analysis of the Riemannian metric provides some insight\ninto how normalization methods could help in training neural networks.",
                        "subsection": []
                    },
                    {
                        "id": "5.2.2",
                        "section": "The geometry of normalized generalized linear models",
                        "text": "We focus our geometric analysis on the generalized linear model. The results from the following\nanalysis can be easily applied to understand deep neural networks with block-diagonal approxima-\ntion to the Fisher information matrix, where each block corresponds to the parameters for a single\nneuron.\nA generalized linear model (GLM) can be regarded as parameterizing an output distribution from\nthe exponential family using a weight vector wand bias scalar b. To be consistent with the previous\nsections, the log likelihood of the GLM can be written using the summed inputs aas the following:\nlogP(yjx;w;b) =(a+b)y\u0000\u0011(a+b)\n\u001e+c(y;\u001e); (10)\nE[yjx] =f(a+b) =f(w>x+b);Var[yjx] =\u001ef0(a+b); (11)\nwhere,f(\u0001)is the transfer function that is the analog of the non-linearity in neural networks, f0(\u0001)\nis the derivative of the transfer function, \u0011(\u0001)is a real valued function and c(\u0001)is the log parti-\ntion function. \u001eis a constant that scales the output variance. Assume a H-dimensional output\nvector y= [y1;y2;\u0001\u0001\u0001;yH]is modeled using Hindependent GLMs and logP(yjx;W;b) =PH\ni=1logP(yijx;wi;bi). LetWbe the weight matrix whose rows are the weight vectors of the\nindividual GLMs, bdenote the bias vector of length Handvec(\u0001)denote the Kronecker vector op-\nerator. The Fisher information matrix for the multi-dimensional GLM with respect to its parameters\n\u0012= [w>\n1;b1;\u0001\u0001\u0001;w>\nH;bH]>= vec([W;b]>)is simply the expected Kronecker product of the data\nfeatures and the output covariance matrix:\nF(\u0012) = E\nx\u0018P(x)\u0014Cov[yjx]\n\u001e2\n\u0014\nxx>x\nx>1\u0015\u0015\n: (12)\nWe obtain normalized GLMs by applying the normalization methods to the summed inputs ain\nthe original model through \u0016and\u001b. Without loss of generality, we denote \u0016Fas the Fisher infor-\nmation matrix under the normalized multi-dimensional GLM with the additional gain parameters\n\u0012= vec([W;b;g]>):\n\u0016F(\u0012) =2\n664\u0016F11\u0001\u0001\u0001 \u0016F1H\n.........\n\u0016FH1\u0001\u0001\u0001 \u0016FHH3\n775;\u0016Fij=E\nx\u0018P(x)2\n664Cov[yi; yjjx]\n\u001e22\n664gigj\n\u001bi\u001bj\u001fi\u001f>\nj\u001figi\n\u001bi\u001figi(aj\u0000\u0016j)\n\u001bi\u001bj\n\u001f>\njgj\n\u001bj1aj\u0000\u0016j\n\u001bj\n\u001f>\njgj(ai\u0000\u0016i)\n\u001bi\u001bjai\u0000\u0016i\n\u001bi(ai\u0000\u0016i)(aj\u0000\u0016j)\n\u001bi\u001bj3\n7753\n775\n(13)\n\u001fi=x\u0000@\u0016i\n@wi\u0000ai\u0000\u0016i\n\u001bi@\u001bi\n@wi: (14)\nImplicit learning rate reduction through the growth of the weight vector: Notice that, com-\nparing to standard GLM, the block \u0016Fijalong the weight vector widirection is scaled by the gain\nparameters and the normalization scalar \u001bi. If the norm of the weight vector wigrows twice as large,\neven though the model\u2019s output remains the same, the Fisher information matrix will be different.\nThe curvature along the widirection will change by a factor of1\n2because the \u001biwill also be twice\nas large. As a result, for the same parameter update in the normalized model, the norm of the weight\nvector effectively controls the learning rate for the weight vector. During learning, it is harder to\nchange the orientation of the weight vector with large norm. The normalization methods, therefore,\n343536373839404142Image Retrieval (Validation)\nOrder-Embedding + LN\nOrder-Embedding(a) Recall@1\n71727374757677Image Retrieval (Validation)\nOrder-Embedding + LN\nOrder-Embedding050100150200250300\niteration x 3007172737475767778mean Recall@5Image Retrieval (Validation)\nOrder-Embedding + LN\nOrder-Embedding (b) Recall@5\n848586878889Image Retrieval (Validation)\nOrder-Embedding + LN\nOrder-Embedding050100150200250300\niteration x 30084858687888990mean Recall@10Image Retrieval (Validation)\nOrder-Embedding + LN\nOrder-Embedding (c) Recall@10\nFigure 1: Recall@K curves using order-embeddings with and without layer normalization.\nMSCOCO\nCaption Retrieval Image Retrieval\nModel R@1 R@5 R@10 Mean rR@1 R@5 R@10 Mean r\nSym [Vendrov et al., 2016] 45.4 88.7 5.8 36.3 85.8 9.0\nOE [Vendrov et al., 2016] 46.7 88.9 5.7 37.9 85.9 8.1\nOE (ours) 46.6 79.3 89.1 5.2 37.8 73.6 85.7 7.9\nOE + LN 48.5 80.6 89.8 5.1 38.9 74.3 86.3 7.6\nTable 2: Average results across 5 test splits for caption and image retrieval. R@K is Recall@K\n(high is good). Mean ris the mean rank (low is good). Sym corresponds to the symmetric baseline\nwhile OE indicates order-embeddings.\nhave an implicit \u201cearly stopping\u201d effect on the weight vectors and help to stabilize learning towards\nconvergence.\nLearning the magnitude of incoming weights: In normalized models, the magnitude of the incom-\ning weights is explicitly parameterized by the gain parameters. We compare how the model output\nchanges between updating the gain parameters in the normalized GLM and updating the magnitude\nof the equivalent weights under original parameterization during learning. The direction along the\ngain parameters in \u0016Fcaptures the geometry for the magnitude of the incoming weights. We show\nthat Riemannian metric along the magnitude of the incoming weights for the standard GLM is scaled\nby the norm of its input, whereas learning the gain parameters for the batch normalized and layer\nnormalized models depends only on the magnitude of the prediction error. Learning the magnitude\nof incoming weights in the normalized model is therefore, more robust to the scaling of the input\nand its parameters than in the standard model. See Appendix for detailed derivations.",
                        "subsection": []
                    }
                ]
            }
        ]
    },
    {
        "id": "5",
        "section": "Analysis",
        "text": "In this section, we investigate the invariance properties of different normalization schemes.",
        "subsection": [
            {
                "id": "6.1",
                "section": "Order embeddings of images and language",
                "text": "In this experiment, we apply layer normalization to the recently proposed order-embeddings model\nof Vendrov et al. [2016] for learning a joint embedding space of images and sentences. We follow\nthe same experimental protocol as Vendrov et al. [2016] and modify their publicly available code\nto incorporate layer normalization1which utilizes Theano [Team et al., 2016]. Images and sen-\ntences from the Microsoft COCO dataset [Lin et al., 2014] are embedded into a common vector\nspace, where a GRU [Cho et al., 2014] is used to encode sentences and the outputs of a pre-trained\nVGG ConvNet [Simonyan and Zisserman, 2015] (10-crop) are used to encode images. The order-\nembedding model represents images and sentences as a 2-level partial ordering and replaces the\ncosine similarity scoring function used in Kiros et al. [2014] with an asymmetric one.\n1https://github.com/ivendrov/order-embedding\n0.40.50.60.70.80.91.0validation error rateAttentive reader\nLSTM\nBN-LSTM\nBN-everywhere\nLN-LSTMFigure 2: Validation curves for the attentive reader model. BN results are taken from [Cooijmans\net al., 2016].\nWe trained two models: the baseline order-embedding model as well as the same model with layer\nnormalization applied to the GRU. After every 300 iterations, we compute Recall@K (R@K) values\non a held out validation set and save the model whenever R@K improves. The best performing\nmodels are then evaluated on 5 separate test sets, each containing 1000 images and 5000 captions,\nfor which the mean results are reported. Both models use Adam [Kingma and Ba, 2014] with the\nsame initial hyperparameters and both models are trained using the same architectural choices as\nused in Vendrov et al. [2016]. We refer the reader to the appendix for a description of how layer\nnormalization is applied to GRU.\nFigure 1illustrates the validation curves of the models, with and without layer normalization. We\nplot R@1, R@5 and R@10 for the image retrieval task. We observe that layer normalization offers\na per-iteration speedup across all metrics and converges to its best validation model in 60% of the\ntime it takes the baseline model to do so. In Table 2, the test set results are reported from which we\nobserve that layer normalization also results in improved generalization over the original model. The\nresults we report are state-of-the-art for RNN embedding models, with only the structure-preserving\nmodel of Wang et al. [2016] reporting better results on this task. However, they evaluate under\ndifferent conditions (1 test set instead of the mean over 5) and are thus not directly comparable.",
                "subsection": []
            },
            {
                "id": "6.2",
                "section": "Teaching machines to read and comprehend",
                "text": "In order to compare layer normalization to the recently proposed recurrent batch normalization\n[Cooijmans et al., 2016], we train an unidirectional attentive reader model on the CNN corpus both\nintroduced by Hermann et al. [2015]. This is a question-answering task where a query description\nabout a passage must be answered by \ufb01lling in a blank. The data is anonymized such that entities\nare given randomized tokens to prevent degenerate solutions, which are consistently permuted dur-\ning training and evaluation. We follow the same experimental protocol as Cooijmans et al. [2016]\nand modify their public code to incorporate layer normalization2which uses Theano [Team et al.,\n2016]. We obtained the pre-processed dataset used by Cooijmans et al. [2016] which differs from\nthe original experiments of Hermann et al. [2015] in that each passage is limited to 4 sentences.\nIn Cooijmans et al. [2016], two variants of recurrent batch normalization are used: one where BN\nis only applied to the LSTM while the other applies BN everywhere throughout the model. In our\nexperiment, we only apply layer normalization within the LSTM.\nThe results of this experiment are shown in Figure 2. We observe that layer normalization not only\ntrains faster but converges to a better validation result over both the baseline and BN variants. In\nCooijmans et al. [2016], it is argued that the scale parameter in BN must be carefully chosen and is\nset to 0.1 in their experiments. We experimented with layer normalization for both 1.0 and 0.1 scale\ninitialization and found that the former model performed signi\ufb01cantly better. This demonstrates that\nlayer normalization is not sensitive to the initial scale in the same way that recurrent BN is.3",
                "subsection": []
            },
            {
                "id": "6.3",
                "section": "Skip-thought vectors",
                "text": "Skip-thoughts [Kiros et al., 2015] is a generalization of the skip-gram model [Mikolov et al., 2013]\nfor learning unsupervised distributed sentence representations. Given contiguous text, a sentence is\n2https://github.com/cooijmanstim/Attentive_reader/tree/bn\n3We only produce results on the validation set, as in the case of Cooijmans et al. [2016]\n82.583.584.585.5Skip-Thoughts + LN\nSkip-Thoughts\nOriginal(a) SICK(r)\n27313233Skip-Thoughts + LN\nSkip-Thoughts\nOriginal5 10 15 20\niteration x 500002728293031323334MSE x 100\nSkip-Thoughts + LN\nSkip-Thoughts\nOriginal (b) SICK(MSE)\n7282Skip-Thoughts + LN\nSkip-Thoughts\nOriginal5 10 15 20\niteration x 5000070727476788082Accuracy\nSkip-Thoughts + LN\nSkip-Thoughts\nOriginal (c) MR\n82Skip-Thoughts + LN\nSkip-Thoughts\nOriginal5 10 15 20\niteration x 5000074767880828486Accuracy\nSkip-Thoughts + LN\nSkip-Thoughts\nOriginal\n(d) CR\n90.591.592.593.594.5Skip-Thoughts + LN\nSkip-Thoughts\nOriginal5 10 15 20\niteration x 5000090.090.591.091.592.092.593.093.594.094.5Accuracy\nSkip-Thoughts + LN\nSkip-Thoughts\nOriginal (e) SUBJ\n838485868788899091Skip-Thoughts + LN\nSkip-Thoughts\nOriginal5 10 15 20\niteration x 50000838485868788899091Accuracy\nSkip-Thoughts + LN\nSkip-Thoughts\nOriginal (f) MPQA\nFigure 3: Performance of skip-thought vectors with and without layer normalization on downstream\ntasks as a function of training iterations. The original lines are the reported results in [Kiros et al.,\n2015]. Plots with error use 10-fold cross validation. Best seen in color.\nMethod SICK( r) SICK(\u001a) SICK( MSE ) MR CR SUBJ MPQA\nOriginal [Kiros et al., 2015] 0.848 0.778 0.287 75.5 79.3 92.1 86.9\nOurs 0.842 0.767 0.298 77.3 81.8 92.6 87.9\nOurs + LN 0.854 0.785 0.277 79.5 82.6 93.4 89.0\nOurs + LN y 0.858 0.788 0.270 79.4 83.1 93.7 89.3\nTable 3: Skip-thoughts results. The \ufb01rst two evaluation columns indicate Pearson and Spearman cor-\nrelation, the third is mean squared error and the remaining indicate classi\ufb01cation accuracy. Higher is\nbetter for all evaluations except MSE. Our models were trained for 1M iterations with the exception\nof (y) which was trained for 1 month (approximately 1.7M iterations)\nencoded with a encoder RNN and decoder RNNs are used to predict the surrounding sentences.\nKiros et al. [2015] showed that this model could produce generic sentence representations that\nperform well on several tasks without being \ufb01ne-tuned. However, training this model is time-\nconsuming, requiring several days of training in order to produce meaningful results.\nIn this experiment we determine to what effect layer normalization can speed up training. Using the\npublicly available code of Kiros et al. [2015]4, we train two models on the BookCorpus dataset [Zhu\net al., 2015]: one with and one without layer normalization. These experiments are performed with\nTheano [Team et al., 2016]. We adhere to the experimental setup used in Kiros et al. [2015], training\na 2400-dimensional sentence encoder with the same hyperparameters. Given the size of the states\nused, it is conceivable layer normalization would produce slower per-iteration updates than without.\nHowever, we found that provided CNMeM5is used, there was no signi\ufb01cant difference between the\ntwo models. We checkpoint both models after every 50,000 iterations and evaluate their performance\non \ufb01ve tasks: semantic-relatedness (SICK) [Marelli et al., 2014], movie review sentiment (MR)\n[Pang and Lee, 2005], customer product reviews (CR) [Hu and Liu, 2004], subjectivity/objectivity\nclassi\ufb01cation (SUBJ) [Pang and Lee, 2004] and opinion polarity (MPQA) [Wiebe et al., 2005]. We\nplot the performance of both models for each checkpoint on all tasks to determine whether the\nperformance rate can be improved with LN.\nThe experimental results are illustrated in Figure 3. We observe that applying layer normalization\nresults both in speedup over the baseline as well as better \ufb01nal results after 1M iterations are per-\nformed as shown in Table 3. We also let the model with layer normalization train for a total of a\nmonth, resulting in further performance gains across all but one task. We note that the performance\n4https://github.com/ryankiros/skip-thoughts\n5https://github.com/NVIDIA/cnmem\nBaseline test\nBaseline train\nLN test\nLN trainFigure 5: Handwriting sequence generation model negative log likelihood with and without layer\nnormalization. The models are trained with mini-batch size of 8 and sequence length of 500,\ndifferences between the original reported results and ours are likely due to the fact that the publicly\navailable code does not condition at each timestep of the decoder, where the original model does.",
                "subsection": []
            },
            {
                "id": "6.4",
                "section": "Modeling binarized mnist using draw",
                "text": "80859095Baseline\nWN\nLN0 20 40 60 80 100\nEpoch80859095100Test Variational BoundBaseline\nWN\nLN\nFigure 4: DRAW model test nega-\ntive log likelihood with and without\nlayer normalization.We also experimented with the generative modeling on the\nMNIST dataset. Deep Recurrent Attention Writer (DRAW)\n[Gregor et al., 2015] has previously achieved the state-of-the-\nart performance on modeling the distribution of MNIST dig-\nits. The model uses a differential attention mechanism and\na recurrent neural network to sequentially generate pieces of\nan image. We evaluate the effect of layer normalization on\na DRAW model using 64 glimpses and 256 LSTM hidden\nunits. The model is trained with the default setting of Adam\n[Kingma and Ba, 2014] optimizer and the minibatch size of\n128. Previous publications on binarized MNIST have used\nvarious training protocols to generate their datasets. In this\nexperiment, we used the \ufb01xed binarization from Larochelle\nand Murray [2011]. The dataset has been split into 50,000\ntraining, 10,000 validation and 10,000 test images.\nFigure 4 shows the test variational bound for the \ufb01rst 100\nepoch. It highlights the speedup bene\ufb01t of applying layer nor-\nmalization that the layer normalized DRAW converges almost twice as fast than the baseline model.\nAfter 200 epoches, the baseline model converges to a variational log likelihood of 82.36 nats on the\ntest data and the layer normalization model obtains 82.09 nats.",
                "subsection": []
            },
            {
                "id": "6.5",
                "section": "Handwriting sequence generation",
                "text": "The previous experiments mostly examine RNNs on NLP tasks whose lengths are in the range of 10\nto 40. To show the effectiveness of layer normalization on longer sequences, we performed hand-\nwriting generation tasks using the IAM Online Handwriting Database [Liwicki and Bunke, 2005].\nIAM-OnDB consists of handwritten lines collected from 221 different writers. When given the input\ncharacter string, the goal is to predict a sequence of x and y pen co-ordinates of the corresponding\nhandwriting line on the whiteboard. There are, in total, 12179 handwriting line sequences. The input\nstring is typically more than 25 characters and the average handwriting line has a length around 700.\nWe used the same model architecture as in Section (5.2) of Graves [2013]. The model architecture\nconsists of three hidden layers of 400 LSTM cells, which produce 20 bivariate Gaussian mixture\ncomponents at the output layer, and a size 3 input layer. The character sequence was encoded with\none-hot vectors, and hence the window vectors were size 57. A mixture of 10 Gaussian functions\nwas used for the window parameters, requiring a size 30 parameter vector. The total number of\nweights was increased to approximately 3.7M. The model is trained using mini-batches of size 8\nand the Adam [Kingma and Ba, 2014] optimizer.\nThe combination of small mini-batch size and very long sequences makes it important to have very\nstable hidden dynamics. Figure 5 shows that layer normalization converges to a comparable log\nlikelihood as the baseline model but is much faster.\n0 10 20 30 40 50 60\nEpochTrain NLL\nBatchNorm bz128\nBaseline bz128\nLayerNorm bz128\n0 10 20 30 40 50 600.0050.0100.0150.0200.025Test Err.\nBatchNorm bz128\nBaseline bz128\nLayerNorm bz128\n0 10 20 30 40 50 60\nEpochTrain NLL\nLayerNorm bz4\nBaseline bz4\nBatchNorm bz4\n0 10 20 30 40 50 600.0050.0100.0150.0200.025Test Err.\nLayerNorm bz4\nBaseline bz4\nBatchNorm bz4Figure 6: Permutation invariant MNIST 784-1000-1000-10 model negative log likelihood and test\nerror with layer normalization and batch normalization. (Left) The models are trained with batch-\nsize of 128. (Right) The models are trained with batch-size of 4.",
                "subsection": []
            },
            {
                "id": "6.6",
                "section": "Permutation invariant mnist",
                "text": "In addition to RNNs, we investigated layer normalization in feed-forward networks. We show how\nlayer normalization compares with batch normalization on the well-studied permutation invariant\nMNIST classi\ufb01cation problem. From the previous analysis, layer normalization is invariant to input\nre-scaling which is desirable for the internal hidden layers. But this is unnecessary for the logit\noutputs where the prediction con\ufb01dence is determined by the scale of the logits. We only apply\nlayer normalization to the fully-connected hidden layers that excludes the last softmax layer.\nAll the models were trained using 55000 training data points and the Adam [Kingma and Ba, 2014]\noptimizer. For the smaller batch-size, the variance term for batch normalization is computed using\nthe unbiased estimator. The experimental results from Figure 6 highlight that layer normalization is\nrobust to the batch-sizes and exhibits a faster training convergence comparing to batch normalization\nthat is applied to all layers.",
                "subsection": []
            },
            {
                "id": "6.7",
                "section": "Convolutional networks",
                "text": "We have also experimented with convolutional neural networks. In our preliminary experiments, we\nobserved that layer normalization offers a speedup over the baseline model without normalization,\nbut batch normalization outperforms the other methods. With fully connected layers, all the hidden\nunits in a layer tend to make similar contributions to the \ufb01nal prediction and re-centering and re-\nscaling the summed inputs to a layer works well. However, the assumption of similar contributions\nis no longer true for convolutional neural networks. The large number of the hidden units whose\nreceptive \ufb01elds lie near the boundary of the image are rarely turned on and thus have very different\nstatistics from the rest of the hidden units within the same layer. We think further research is needed\nto make layer normalization work well in ConvNets.",
                "subsection": []
            }
        ]
    },
    {
        "id": "6",
        "section": "Experimental results",
        "text": "We perform experiments with layer normalization on 6 tasks, with a focus on recurrent neural net-\nworks: image-sentence ranking, question-answering, contextual language modelling, generative\nmodelling, handwriting sequence generation and MNIST classi\ufb01cation. Unless otherwise noted,\nthe default initialization of layer normalization is to set the adaptive gains to 1and the biases to 0in\nthe experiments.",
        "subsection": []
    },
    {
        "id": "7",
        "section": "Conclusion",
        "text": "In this paper, we introduced layer normalization to speed-up the training of neural networks. We\nprovided a theoretical analysis that compared the invariance properties of layer normalization with\nbatch normalization and weight normalization. We showed that layer normalization is invariant to\nper training-case feature shifting and scaling.\nEmpirically, we showed that recurrent neural networks bene\ufb01t the most from the proposed method\nespecially for long sequences and small mini-batches.\nAcknowledgments\nThis research was funded by grants from NSERC, CFI, and Google.",
        "subsection": []
    },
    {
        "missing": []
    },
    {
        "references": [
            "",
            " for learning a joint embedding space of images and sentences. We follow the same experimental protocol as Vendrov et al. [2016] and modify their publicly available code to incorporate layer normalization1which utilizes Theano [Team et al., 2016]. Images and sen- tences from the Microsoft COCO dataset [Lin et al., 2014] are embedded into a common vector space, where a GRU [Cho et al., 2014] is used to encode sentences and the outputs of a pre-trained VGG ConvNet [Simonyan and Zisserman, 2015] (10-crop) are used to encode images. The order- embedding model represents images and sentences as a 2-level partial ordering and replaces the cosine similarity scoring function used in Kiros et al. [2014] with an asymmetric one.",
            ". This is a question-answering task where a query description about a passage must be answered by \ufb01lling in a blank. The data is anonymized such that entities are given randomized tokens to prevent degenerate solutions, which are consistently permuted dur- ing training and evaluation. We follow the same experimental protocol as Cooijmans et al. [2016] and modify their public code to incorporate layer normalization2which uses Theano [Team et al., 2016]. We obtained the pre-processed dataset used by Cooijmans et al. [2016] which differs from the original experiments of Hermann et al. [2015] in that each passage is limited to 4 sentences.",
            ". The dataset has been split into 50,000 training, 10,000 validation and 10,000 test images.",
            ". The model architecture consists of three hidden layers of 400 LSTM cells, which produce 20 bivariate Gaussian mixture components at the output layer, and a size 3 input layer. The character sequence was encoded with one-hot vectors, and hence the window vectors were size 57. A mixture of 10 Gaussian functions was used for the window parameters, requiring a size 30 parameter vector. The total number of weights was increased to approximately 3.7M. The model is trained using mini-batches of size 8 and the Adam [Kingma and Ba, 2014] optimizer."
        ]
    },
    {
        "title": "Layer Normalization",
        "arxiv_id": "1607.06450"
    }
]