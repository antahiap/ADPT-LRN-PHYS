[
    {
        "id": "",
        "section": "Abstract",
        "text": "Neural machine translation (NMT) mod-\nels typically operate with a \ufb01xed vocabu-\nlary, but translation is an open-vocabulary\nproblem. Previous work addresses the\ntranslation of out-of-vocabulary words by\nbacking off to a dictionary. In this pa-\nper, we introduce a simpler and more ef-\nfective approach, making the NMT model\ncapable of open-vocabulary translation by\nencoding rare and unknown words as se-\nquencesofsubwordunits. Thisisbasedon\nthe intuition that various word classes are\ntranslatable via smaller units than words,\nfor instance names (via character copying\nor transliteration), compounds (via com-\npositional translation), and cognates and\nloanwords (via phonological and morpho-\nlogical transformations). We discuss the\nsuitability of different word segmentation\ntechniques, including simple character n-\ngram models andasegmentation based on\nthebyte pair encoding compression algo-\nrithm, and empirically show that subword\nmodelsimproveoveraback-off dictionary\nbaseline for the WMT15 translation tasks\nEnglish\u2192German and English \u2192Russian\nbyup to1.1 and 1.3 B LEU, respectively.",
        "subsection": []
    },
    {
        "id": "1",
        "section": "Introduction",
        "text": "Neural machine translation has recently shown\nimpressive results (Kalchbrenner and Blunsom,\n2013; Sutskever et al., 2014; Bahdanau et al.,\n2015). However, the translation of rare words\nis an open problem. The vocabulary of neu-\nral models is typically limited to 30000\u201350000\nwords,but translation isanopen-vocabulary prob-\nlem, and especially for languages with produc-\ntive word formation processes such as aggluti-\nnation and compounding, translation models re-\nquire mechanisms that go below the word level.\nAs an example, consider compounds such as the\nGermanAbwasser |behandlungs |anlange\u2018sewage\nwater treatment plant\u2019, for which a segmented,\nvariable-length representation is intuitively more\nappealingthanencodingthewordasa\ufb01xed-length\nvector.\nFor word-level NMT models, the translation\nof out-of-vocabulary words has been addressed\nthrough aback-off toadictionary look-up (Jeanet\nal., 2015; Luong et al., 2015b). We note that such\ntechniques make assumptions that often do not\nhold true in practice. For instance, there is not al-\nwaysa1-to-1correspondence between sourceand\ntarget words because of variance in the degree of\nmorphological synthesis between languages, like\nin our introductory compounding example. Also,\nword-level models are unable to translate or gen-\nerateunseen words. Copying unknownwordsinto\nthetargettext,asdoneby(Jeanetal.,2015;Luong\net al., 2015b), is a reasonable strategy for names,\nbut morphological changes and transliteration is\noften required, especially if alphabets differ.\nWeinvestigate NMTmodelsthat operate onthe\nlevel of subword units. Our main goal is to model\nopen-vocabulary translation in the NMT network\nitself, without requiring a back-off model for rare\nwords. In addition to making the translation pro-\ncesssimpler,wealso\ufb01ndthatthesubwordmodels\nachieve better accuracy for the translation of rare\nwords than large-vocabulary models and back-off\ndictionaries, and are able to productively generate\nnewwordsthatwerenotseenattrainingtime. Our\nanalysisshowsthattheneural networksareableto\nlearn compounding and transliteration from sub-\nwordrepresentations.\nchine translation is possible by encoding\n(rare) words via subword units. We \ufb01nd our\narchitecture simpler and more effective than\nusing large vocabularies and back-off dictio-\nnaries(Jeanetal.,2015;Luongetal.,2015b).\n\u2022 We adapt byte pair encoding (BPE) (Gage,\n1994), a compression algorithm, to the task\nof word segmentation. BPE allows for the\nrepresentation ofanopenvocabulary through\na \ufb01xed-size vocabulary of variable-length\ncharacter sequences, making it a very suit-\nable word segmentation strategy for neural\nnetwork models.",
        "subsection": []
    },
    {
        "id": "2",
        "section": "Neural machinetranslation",
        "text": "We follow the neural machine translation archi-\ntecture by Bahdanau et al. (2015), which we will\nbrie\ufb02ysummarizehere. However,wenotethatour\napproach isnot speci\ufb01c tothis architecture.\nTheneuralmachinetranslationsystemisimple-\nmentedasanencoder-decoder networkwithrecur-\nrent neural networks.\nThe encoder is a bidirectional neural network\nwith gated recurrent units (Cho et al., 2014)\nthat reads an input sequence x= (x1,...,xm)\nand calculates a forward sequence of hidden\nstates(\u2212 \u2192h1,...,\u2212 \u2192hm), and a backward sequence\n(\u2190 \u2212h1,...,\u2190 \u2212hm). The hidden states\u2212 \u2192hjand\u2190 \u2212hjare\nconcatenated to obtain the annotation vector hj.\nThe decoder is a recurrent neural network that\npredicts a target sequence y= (y1,...,yn). Each\nwordyiis predicted based on a recurrent hidden\nstatesi, the previously predicted word yi\u22121, and\na context vector ci.ciis computed as a weighted\nsum of the annotations hj. The weight of each\nannotation hjis computed through an alignment\nmodel\u03b1ij, which models the probability that yiis\naligned to xj. The alignment model is a single-\nlayer feedforward neural network that is learned\njointly with the rest of the network through back-\npropagation.\nA detailed description can be found in (Bah-\ndanau et al., 2015). Training is performed on a\nparallel corpus with stochastic gradient descent.\nFor translation, a beam search with small beam\nsize isemployed.",
        "subsection": [
            {
                "id": "3.1",
                "section": "Related work",
                "text": "For Statistical Machine Translation (SMT), the\ntranslation ofunknownwordshasbeenthesubject\nof intensive research.\nA large proportion of unknown words are\nnames, which can just be copied into the tar-\nget text if both languages share an alphabet. If\nalphabets differ, transliteration is required (Dur-\nrani et al., 2014). Character-based translation has\nalso been investigated with phrase-based models,\nwhich proved especially successful for closely re-\nlated languages (Vilar et al., 2007; Tiedemann,\n2009; Neubig et al.,2012).\nThe segmentation of morphologically complex\nwordssuchascompoundsiswidelyusedforSMT,\nand various algorithms for morpheme segmen-\ntation have been investigated (Nie\u00dfen and Ney,\n2000; Koehn and Knight, 2003; Virpioja et al.,\n2007; Stallard et al., 2012). Segmentation al-\ngorithms commonly used for phrase-based SMT\ntend tobeconservative intheir splitting decisions,\nwhereas we aim for an aggressive segmentation\nthat allows for open-vocabulary translation with a\ncompact network vocabulary, and without having\ntoresort to back-off dictionaries.\nThe best choice of subword units may be task-\nspeci\ufb01c. For speech recognition, phone-level lan-\nguage models have been used (Bazzi and Glass,\n2000). Mikolov et al. (2012) investigate subword\nlanguage models, and propose to use syllables.\nFor multilingual segmentation tasks, multilingual\nalgorithmshavebeenproposed(SnyderandBarzi-\nlay,2008). We\ufb01ndtheseintriguing, butinapplica-\nble at test time.\nVarious techniques have been proposed to pro-\nduce \ufb01xed-length continuous word vectors based\non characters or morphemes (Luong et al., 2013;\nBothaandBlunsom,2014;Lingetal.,2015a;Kim\net al., 2015). An effort to apply such techniques\nto NMT, parallel to ours, has found no signi\ufb01cant\nimprovement over word-based approaches (Ling\net al., 2015b). One technical difference from our\nwork is that the attention mechanism still oper-\nates on the level of words in the model by Ling\net al. (2015b), and that the representation of each\nword is \ufb01xed-length. We expect that the attention\nresentation: the network can learn to place atten-tion on different subword units at each step. Re-\ncall our introductory example Abwasserbehand-\nlungsanlange , for which a subword segmentation\navoidstheinformationbottleneckofa\ufb01xed-length\nrepresentation.\nNeural machine translation differs from phrase-\nbasedmethodsinthattherearestrongincentivesto\nminimize the vocabulary size of neural models to\nincreasetimeandspaceef\ufb01ciency,andtoallowfor\ntranslation without back-off models. At the same\ntime,wealsowantacompactrepresentation ofthe\ntext itself, since an increase in text length reduces\nef\ufb01ciency and increases the distances over which\nneural models need to pass information.\nAsimplemethodtomanipulatethetrade-offbe-\ntweenvocabulary sizeandtextsizeistouseshort-\nlists of unsegmented words, using subword units\nonly for rare words. As an alternative, we pro-\npose a segmentation algorithm based on byte pair\nencoding (BPE), which lets us learn a vocabulary\nthat provides agood compression rate of thetext.",
                "subsection": []
            },
            {
                "id": "3.2",
                "section": "Byte pairencoding",
                "text": "Byte Pair Encoding (BPE) (Gage, 1994) is a sim-\nple data compression technique that iteratively re-\nplaces the most frequent pair of bytes in a se-\nquence with a single, unused byte. We adapt this\nalgorithmforwordsegmentation. Insteadofmerg-\ningfrequent pairsofbytes,wemergecharactersor\ncharacter sequences.\nFirstly,weinitializethesymbolvocabularywith\nthe character vocabulary, and represent each word\nas a sequence of characters, plus a special end-of-\nword symbol \u2018\u00b7\u2019, which allows us to restore the\noriginal tokenization after translation. We itera-\ntively count all symbol pairs and replace each oc-\ncurrence of the most frequent pair (\u2018A\u2019, \u2018B\u2019) with\na new symbol \u2018AB\u2019. Each merge operation pro-\nduces a new symbol which represents a charac-\ntern-gram. Frequent character n-grams(or whole\nwords) are eventually merged into a single sym-\nbol,thusBPErequires noshortlist. The\ufb01nalsym-\nbolvocabularysizeisequaltothesizeoftheinitial\nvocabulary, plus the number of merge operations\n\u2013thelatter istheonly hyperparameter of thealgo-\nrithm.\nFor ef\ufb01ciency, we do not consider pairs that\ncross word boundaries. The algorithm can thus be\nrun on the dictionary extracted from a text, with\nAlgorithm 1 Learn BPEoperations\nimportre, collections\ndefget_stats(vocab):\npairs = collections.defaultdict(int)\nforword, freq invocab.items():\nsymbols = word.split()\nforiinrange(len(symbols)-1):\npairs[symbols[i],symbols[i+1]] += freq\nreturnpairs\ndefmerge_vocab(pair, v_in):\nv_out = {}\nbigram = re.escape(' '.join(pair))\np = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)')\nforwordinv_in:\nw_out = p.sub(''.join(pair), word)\nv_out[w_out] = v_in[word]\nreturnv_out\nvocab = {'l o w </w>' : 5, 'l o w e r </w>' : 2,\n'n e w e s t </w>':6, 'w i d e s t </w>':3}\nnum_merges = 10\nforiinrange(num_merges):\npairs = get_stats(vocab)\nbest = max(pairs, key=pairs.get)\nvocab = merge_vocab(best, vocab)\nprint(best)\nr\u00b7 \u2192 r\u00b7\nlo\u2192lo\nlow\u2192low\ne r\u00b7 \u2192 er\u00b7\nFigure1: BPEmergeoperations learned fromdic-\ntionary {\u2018low\u2019, \u2018lowest\u2019, \u2018newer\u2019, \u2018wider\u2019}.\ngorithm 1. In practice, we increase ef\ufb01ciency by\nindexing all pairs, and updating data structures in-\ncrementally.\nThe main difference to other compression al-\ngorithms, such as Huffman encoding, which have\nbeen proposed to produce a variable-length en-\ncoding of words for NMT (Chitnis and DeNero,\n2015), is that our symbol sequences are still in-\nterpretable as subword units, and that the network\ncangeneralize totranslate andproduce newwords\n(unseen attraining time) onthebasisofthesesub-\nword units.\nFigure 1 shows a toy example of learned BPE\noperations. At test time, we \ufb01rst split words into\nsequencesofcharacters,thenapplythelearnedop-\nerationstomergethecharacters intolarger, known\nsymbols. This is applicable to any word, and\nallows for open-vocabulary networks with \ufb01xed\nsymbol vocabularies.3In our example, the OOV\n\u2018lower\u2019 would be segmented into \u2018low er \u00b7\u2019.\n3The only symbols that will be unknown at test time are\nunknown characters, or symbols of which all occurrences\nin the training text have been merged into larger symbols,\nlike \u2018safeguar\u2019, which has alloccurrences in our training t ext\nWe evaluate two methods of applying BPE:\nlearning two independent encodings, one for the\nsource, one for the target vocabulary, or learning\nthe encoding on the union of the two vocabular-\nies(whichwecall jointBPE ).4Theformerhasthe\nadvantage of being more compact in terms of text\nand vocabulary size, and having stronger guaran-\ntees that each subword unit has been seen in the\ntraining text of the respective language, whereas\nthelatterimprovesconsistencybetweenthesource\nand the target segmentation. If we apply BPE in-\ndependently, the same name may be segmented\ndifferently in the two languages, which makes it\nharder for the neural models to learn a mapping\nbetween the subword units. To increase the con-\nsistency between English and Russian segmenta-\ntion despite the differing alphabets, we transliter-\nate the Russian vocabulary into Latin characters\nwith ISO-9 to learn the joint BPE encoding, then\ntransliterate the BPE merge operations back into\nCyrillictoapplythemtotheRussiantrainingtext.5",
                "subsection": []
            }
        ]
    },
    {
        "id": "3",
        "section": "Subword translation",
        "text": "the translation of some words is transparent inthat they are translatable by a competent transla-\ntor even if they are novel to him or her, based\non a translation of known subword units such as\nmorphemes or phonemes. Word categories whose\ntranslation is potentially transparent include:\n\u2022 namedentities. Betweenlanguagesthatshare\nan alphabet, names can often be copied from\nsourcetotargettext. Transcriptionortranslit-\neration may be required, especially if the al-\nphabets or syllabaries differ. Example:\nBarack Obama(English; German)\n\u0411\u0430\u0440\u0430\u043a \u041e\u0431\u0430\u043c\u0430 (Russian)\n\u30d0\u30e9\u30af\u30fb\u30aa\u30d0\u30de(ba-ra-kuo-ba-ma)(Japanese)\n\u2022 cognates and loanwords. Cognates and loan-\nwords with a common origin can differ in\nregular ways between languages, so that\ncharacter-level translation rules are suf\ufb01cient\n(Tiedemann, 2012). Example:\nclaustrophobia (English)\nKlaustrophobie (German)\n\u041a\u043b\u0430\u0443\u0441\u0442\u0440\u043e\u0444\u043e\u0431\u0438\u044f (Klaustrofobi\u00e2) (Russian)\n\u2022 morphologically complexwords. Wordscon-\ntaining multiple morphemes, for instance\nformed via compounding, af\ufb01xation, or in-\n\ufb02ection, may be translatable by translating\nthe morphemes separately. Example:\nsolar system (English)\nSonnensystem (Sonne + System) (German)\nNaprendszer (Nap+ Rendszer) (Hungarian)\nIn an analysis of 100 rare tokens (not among\nthe 50000 most frequent types) in our German\ntraining data1, the majority of tokens are poten-\ntially translatable from English through smaller\nunits. We \ufb01nd 56 compounds, 21 names,\n6 loanwords with a common origin ( emanci-\npate\u2192emanzipieren ), 5 cases of transparent af\ufb01x-\nation (sweetish\u2018sweet\u2019 + \u2018-ish\u2019\u2192s\u00fc\u00dflich\u2018s\u00fc\u00df\u2019 +\n\u2018-lich\u2019), 1 number and 1 computer language iden-\nti\ufb01er.\nOur hypothesis is that a segmentation of rare\nwords into appropriate subword units is suf\ufb01-\ncient to allow for the neural translation network\nto learn transparent translations, and to general-\nizethisknowledgetotranslateandproduceunseen\nwords.2Weprovide empirical support for this hy-\n1Primarilyparliamentaryproceedingsandwebcrawldata.\n2Not every segmentation we produce is transparent.\nWhile we expect no performance bene\ufb01t from opaque seg-\npothesis in Sections 4and 5. First, wediscuss dif-\nferent subword representations.",
        "subsection": [
            {
                "id": "4.1",
                "section": "Subwordstatistics",
                "text": "Apart from translation quality, which we will ver-\nify empirically, our main objective is to represent\nan open vocabulary through a compact \ufb01xed-size\nsubword vocabulary, and allow for ef\ufb01cient train-\ning and decoding.8\nStatisticsfordifferentsegmentationsoftheGer-\n6Clipped unigram precision is essentially 1-gram BLEU\nwithout brevity penalty.\n7github.com/sebastien-j/LV_groundhog\n8man side of the parallel data are shown in Table\n1. Asimple baseline is the segmentation of words\nintocharacter n-grams.9Character n-gramsallow\nfor different trade-offs between sequence length\n(# tokens) and vocabulary size (# types), depend-\ning on the choice of n. The increase in sequence\nlength is substantial; one way to reduce sequence\nlength istoleave ashortlist of the kmost frequent\nwordtypesunsegmented. Onlytheunigramrepre-\nsentation is truly open-vocabulary. However, the\nunigram representation performed poorly in pre-\nliminaryexperiments,andwereporttranslationre-\nsultswithabigramrepresentation, whichisempir-\nicallybetter, but unabletoproduce sometokens in\nthetest set with thetraining set vocabulary.\nWe report statistics for several word segmenta-\ntiontechniquesthathaveprovenusefulinprevious\nSMT research, including frequency-based com-\npound splitting (Koehn and Knight, 2003), rule-\nbased hyphenation (Liang, 1983), and Morfessor\n(Creutz and Lagus, 2002). We \ufb01nd that they only\nmoderately reduce vocabulary size, and do not\nsolvetheunknownwordproblem,andwethus\ufb01nd\nthem unsuitable for our goal of open-vocabulary\ntranslation without back-off dictionary.\nBPE meets our goal of being open-vocabulary,\nand the learned merge operations can be applied\nto the test set to obtain a segmentation with no\nunknown symbols.10Its main difference from\nthe character-level model is that the more com-\npact representation of BPE allows for shorter se-\nquences, and that the attention model operates\non variable-length units.11Table 1 shows BPE\nwith59500 merge operations, and joint BPEwith\n89500 operations.\nIn practice, we did not include infrequent sub-\nword units in the NMT network vocabulary, since\nthere is noise in the subword symbol sets, e.g.\nbecause of characters from foreign alphabets.\nHence, our network vocabularies in Table 2 are\ntypically slightly smallerthanthenumber oftypes\ninTable 1.\n9Ourcharactern-gramsdonotcrosswordboundaries. We\nmark whether a subword is word-\ufb01nal or not with a special\ncharacter,whichallowsustorestoretheoriginaltokeniza tion.\n10Joint BPE can produce segments that are unknown be-\ncause they only occur in the English training text, but these\nare rare(0.05% of testtokens).\nvocabulary BLEU CHRF3unigram F 1(%)\nname segmentation shortlist source target single ens-8 single ens-8 all rare OOV\nsyntax-based (Sennrichand Haddow, 2015) 24.4 - 55.3 - 59.1 46.0 37.7\nWUnk - - 300000 500000 20.6 22.8 47.2 48.9 56.7 20.4 0.0\nWDict - - 300000 500000 22.0 24.2 50.5 52.4 58.1 36.8 36.8\nC2-50k char-bigram 50000 60000 60000 22.8 25.3 51.9 53.5 58.4 40.5 30.9\nBPE-60k BPE - 60000 60000 21.5 24.5 52.053.958.4 40.9 29.3\nBPE-J90k BPE(joint) - 90000 90000 22.824.751.754.158.5 41.8 33.6\nTable 2: English\u2192German translation performance (B LEU,CHRF3 and unigram F 1) on newstest2015.\nEns-8: ensemble of 8 models. Best NMTsystem in bold. Unigram F1(with ensembles) iscomputed for\nall words ( n= 44085), rare words (not among top 50000 in training set; n= 2900), and OOVs (not in\ntraining set; n= 1168).\nsegmentation #tokens #types #UNK\nnone 100m 1750000 1079\ncharacters 550m 3000 0\ncharacter bigrams 306m 20000 34\ncharacter trigrams 214m 120000 59\ncompound splitting\u25b3102m 1100000 643\nmorfessor* 109m 544000 237\nhyphenation\u22c4186m 404000 230\nBPE 112m 63000 0\nBPE(joint) 111m 82000 32\ncharacter bigrams129m 69000 34(shortlist: 50000)\nTable 1: Corpus statistics for German training\ncorpus with different word segmentation tech-\nniques. #UNK: number of unknown tokens in\nnewstest2013.\u25b3: (Koehn and Knight, 2003); *:\n(Creutz and Lagus, 2002); \u22c4: (Liang, 1983).",
                "subsection": []
            },
            {
                "id": "4.2",
                "section": "Translation experiments",
                "text": "English\u2192German translation results are shown in\nTable2; English\u2192Russian results in Table3.\nOur baseline WDictisaword-level model with\naback-offdictionary. Itdiffersfrom WUnkinthat\nthelatter usesnoback-off dictionary, andjust rep-\nresents out-of-vocabulary words as UNK12. The\nback-off dictionary improves unigram F 1for rare\nand unseen words, although the improvement is\nsmaller for English \u2192Russian, since the back-off\ndictionary is incapable of transliterating names.\nAllsubwordsystemsoperatewithoutaback-off\ndictionary. We \ufb01rst focus on unigram F 1, where\nall systems improve over the baseline, especially\nfor rare words (36.8% \u219241.8% for EN\u2192DE;\n26.5%\u219229.7% for EN\u2192RU). For OOVs, the\nbaseline strategy of copying unknown words\nworkswellforEnglish \u2192German. However,when\nalphabets differ, like in English \u2192Russian, the\nsubword models do much better.\n12We useUNKUnigram F 1scores indicate that learning the\nBPE symbols on the vocabulary union ( BPE-\nJ90k) is more effective than learning them sep-\narately (BPE-60k ), and more effective than using\ncharacter bigramswithashortlist of50000unseg-\nmented words ( C2-50k), but all reported subword\nsegmentations are viable choices and outperform\ntheback-off dictionary baseline.\nOur subword representations cause big im-\nprovements in the translation of rare and unseen\nwords, but these only constitute 9-11% of the test\nsets. Since rare words tend to carry central in-\nformation in a sentence, we suspect that B LEU\nandCHRF3 underestimate their effect on transla-\ntion quality. Still, we also see improvements over\nthe baseline in total unigram F 1, as well as B LEU\nandCHRF3, and the subword ensembles outper-\nform the WDict baseline by 0.3\u20131.3 B LEUand\n0.6\u20132CHRF3. There is some inconsistency be-\ntweenB LEUandCHRF3,whichweattributetothe\nfact that B LEUhas a precision bias, and CHRF3 a\nrecall bias.\nFor English\u2192German, we observe the best\nBLEUscore of 25.3 with C2-50k, but the best\nCHRF3 score of 54.1 with BPE-J90k. For com-\nparison to the (to our knowledge) best non-neural\nMT system on this data set, we report syntax-\nbased SMT results (Sennrich and Haddow, 2015).\nWe observe that our best systems outperform the\nsyntax-based system in terms of B LEU, but not\nin terms of CHRF3. Regarding other neural sys-\ntems, Luong et al. (2015a) report a B LEUscore of\n25.9onnewstest2015, butwenotethattheyusean\nensemble of 8 independently trained models, and\nalso report strong improvements from applying\ndropout, which we did not use. We are con\ufb01dent\nthat our improvements to the translation of rare\ntecture, training algorithm, or better ensembles.\nFor English\u2192Russian, the state of the art is\nthe phrase-based system by Haddow et al. (2015).\nIt outperforms our WDict baseline by 1.5 B LEU.\nThe subword models are a step towards closing\nthis gap, and BPE-J90k yields an improvement of\n1.3 BLEU, and 2.0 CHRF3, over WDict.\nAsafurther commentonourtranslation results,\nwe want to emphasize that performance variabil-\nity is still an open problem with NMT.Onour de-\nvelopment set, we observe differences of up to 1\nBLEUbetween different models. For single sys-\ntems, we report the results of the model that per-\nforms best on dev (out of 8), which has a stabi-\nlizing effect, but how to control for randomness\ndeserves further attention in future research.",
                "subsection": []
            }
        ]
    },
    {
        "id": "4",
        "section": "Evaluation",
        "text": "We aim to answer the following empirical ques-\ntions:\n\u2022 Can we improve the translation of rare and\nunseen words in neural machine translation\nby representing them via subword units?\n\u2022 Which segmentation into subword units per-\nforms best in terms of vocabulary size, text\nsize, and translation quality?\nWe perform experiments on data from the\nshared translation task of WMT 2015. For\nEnglish\u2192German, our training set consists of 4.2\nmillion sentence pairs, or approximately 100 mil-\nliontokens. ForEnglish \u2192Russian, thetrainingset\nconsists of 2.6 million sentence pairs, or approx-\nimately 50 million tokens. We tokenize and true-\ncase the data with the scripts provided in Moses\n(Koehn et al., 2007). We use newstest2013 as de-\nvelopment set, and report results on newstest2014\nand newstest2015.\nWe report results with B LEU(mteval-v13a.pl ),\nandCHRF3 (Popovi \u00b4c, 2015), a character n-gram\nF3score which was found to correlate well with\n4In practice, we simply concatenate the source and target\nside of the trainingsettolearnjoint BPE.\nhuman judgments, especially for translations out\nof English (Stanojevi \u00b4c et al., 2015). Since our\nmain claim is concerned with the translation of\nrare and unseen words, we report separate statis-\ntics for these. We measure these through unigram\nF1, which we calculate as the harmonic mean of\nclipped unigram precision and recall.6\nWe perform all experiments with Groundhog7\n(Bahdanau et al., 2015). We generally follow set-\ntings by previous work (Bahdanau et al., 2015;\nJean et al., 2015). All networks have a hidden\nlayer size of 1000, and an embedding layer size\nof620. FollowingJeanetal.(2015), weonlykeep\nashortlist of \u03c4= 30000 words inmemory.\nDuringtraining,weuseAdadelta(Zeiler,2012),\na minibatch size of 80, and reshuf\ufb02e the train-\ning set between epochs. We train a network for\napproximately 7 days, then take the last 4 saved\nmodels (models being saved every 12 hours), and\ncontinue training each with a \ufb01xed embedding\nlayer (as suggested by (Jean et al., 2015)) for 12\nhours. We perform two independent training runs\nfor each models, once with cut-off for gradient\nclipping (Pascanu et al., 2013) of 5.0, once with\na cut-off of 1.0 \u2013 the latter produced better single\nmodels for most settings. We report results of the\nsystemthatperformedbestonourdevelopmentset\n(newstest2013), and of an ensemble of all 8 mod-\nels.\nWe use a beam size of 12 for beam search,\nwith probabilities normalized by sentence length.\nWe use a bilingual dictionary based on fast-align\n(Dyer et al., 2013). For our baseline, this serves\nas back-off dictionary for rare words. Wealso use\nthe dictionary to speed up translation for all ex-\nperiments, only performing thesoftmax over a\ufb01l-\ntered list of candidate translations (like Jean et al.\n(2015), weuse K= 30000;K\u2032= 10).",
        "subsection": [
            {
                "id": "5.1",
                "section": "Unigram accuracy",
                "text": "Ourmainclaimsarethatthetranslationofrareand\nunknown words is poor in word-level NMT mod-\nels, and that subword models improve the trans-\nlation of these word types. To further illustrate\nthe effect of different subword segmentations on\nthe translation of rare and unseen words, we plot\ntarget-side words sorted by their frequency in the\ntraining set.13To analyze the effect of vocabulary\nsize,wealsoincludethesystem C2-3/500k ,which\nis a system with the same vocabulary size as the\nWDict baseline, and character bigrams to repre-\nsent unseen words.\nFigure 2 shows results for the English\u2013German\nensemble systems on newstest2015. Unigram\nF1of all systems tends to decrease for lower-\nfrequency words. Thebaseline system has a spike\nin F1for OOVs, i.e. words that do not occur in\nthe training text. This is because a high propor-\ntion of OOVs are names, for which a copy from\nthe source to the target text is a good strategy for\nEnglish\u2192German.\nThesystemswithatargetvocabularyof500000\nwords mostly differ in how well they translate\nwords with rank > 500000. A back-off dictionary\nis an obvious improvement over producing UNK,\nbutthesubwordsystemC2-3/500k achievesbetter\nperformance. Note that all OOVs that the back-\noff dictionary produces are words that are copied\nfromthesource,usuallynames,whilethesubword\n13systems can productively form new words such as\ncompounds.\nFor the 50000 most frequent words, the repre-\nsentation is the same for all neural networks, and\nall neural networks achieve comparable unigram\nF1for this category. For the interval between fre-\nquency rank 50000 and 500000, the comparison\nbetween C2-3/500k and C2-50k unveils an inter-\nesting difference. The two systems only differ in\nthesizeoftheshortlist, withC2-3/500k represent-\ning words in this interval as single units, and C2-\n50k via subword units. We \ufb01nd that the perfor-\nmance of C2-3/500k degrades heavily up to fre-\nquency rank 500000, at which point the model\nswitches to a subword representation and perfor-\nmance recovers. The performance of C2-50k re-\nmains more stable. We attribute this to the fact\nthat subword units are less sparse than words. In\nour training set, the frequency rank 50000 corre-\nsponds to a frequency of 60 in the training data;\nthe frequency rank 500000 to a frequency of 2.\nBecause subword representations are less sparse,\nreducing the size of the network vocabulary, and\nrepresenting more words via subword units, can\nlead tobetter performance.\nThe F1numbers hide some qualitative differ-\nences between systems. For English \u2192German,\nWDict produces few OOVs (26.5% recall), but\nwithhighprecision(60.6%),whereasthesubword\nsystemsachievehigher recall, butlowerprecision.\nWe note that the character bigram model C2-50k\nproduces the most OOV words, and achieves rel-\natively low precision of 29.1% for this category.\nHowever, it outperforms the back-off dictionary\nin recall (33.0%). BPE-60k, which suffers from\ntransliteration (or copy) errors due to segmenta-\ntion inconsistencies, obtains a slightly better pre-\ncision(32.4%),butaworserecall(26.6%). Incon-\ntrast to BPE-60k, the joint BPEencoding of BPE-\nJ90k improves both precision (38.6%) and recall\n(29.8%).\nFor English\u2192Russian, unknown names can\nonly rarely be copied, and usually require translit-\neration. Consequently, the WDict baseline per-\nforms more poorly for OOVs (9.2% precision;\n5.2% recall), and the subword models improve\nboth precision and recall (21.9% precision and\n15.6% recall for BPE-J90k). The full unigram Fvocabulary BLEU CHRF3unigram F 1(%)\nname segmentation shortlist source target single ens-8 single ens-8 all rare OOV\nphrase-based (Haddow et al.,2015) 24.3 - 53.8 - 56.0 31.3 16.5\nWUnk - - 300000 500000 18.8 22.4 46.5 49.9 54.2 25.2 0.0\nWDict - - 300000 500000 19.1 22.8 47.5 51.0 54.8 26.5 6.6\nC2-50k char-bigram 50000 60000 60000 20.9 24.1 49.0 51.6 55.2 27.8 17.4\nBPE-60k BPE - 60000 60000 20.5 23.6 49.852.755.3 29.7 15.6\nBPE-J90k BPE(joint) - 90000 100000 20.424.149.753.055.8 29.7 18.3\nTable 3: English\u2192Russian translation performance (B LEU,CHRF3 and unigram F 1) on newstest2015.\nEns-8: ensemble of 8 models. Best NMTsystem in bold. Unigram F1(with ensembles) iscomputed for\nall words ( n= 55654), rare words (not among top 50000 in training set; n= 5442), and OOVs (not in\ntraining set; n= 851).\n10010110210310410510600.20.40.60.81\n50000 500000\ntraining set frequency rankunigram F 1\nBPE-J90k\nC2-50k\nC2-300/500k\nWDict\nWUnk\nFigure 2: English\u2192German unigram F 1on new-\nstest2015 plotted by training set frequency rank\nfor different NMTsystems.\n10010110210310410510600.20.40.60.81\n50000 500000\ntraining set frequency rankunigram F 1\nBPE-J90k\nC2-50k\nWDict\nWUnk\nFigure 3: English\u2192Russian unigram F 1on new-\nstest2015 plotted by training set frequency rank\nfor different NMTsystems.5.2 ManualAnalysis\nTable 4 shows two translation examples for\nthe translation direction English \u2192German, Ta-\nble 5 for English\u2192Russian. The baseline sys-\ntem fails for all of the examples, either by delet-\ning content ( health), or by copying source words\nthat should be translated or transliterated. The\nsubword translations of health research insti-\ntutesshow that the subword systems are capa-\nbleoflearning translations whenoversplitting ( re-\nsearch\u2192Fo|rs|ch|un|g), or whenthesegmentation\ndoes not match morpheme boundaries: the seg-\nmentation Forschungs |instituten wouldbelinguis-\ntically more plausible, and simpler to align to the\nEnglishresearch institutes , than the segmentation\nForsch|ungsinstitu |tenintheBPE-60ksystem, but\nstill, a correct translation is produced. If the sys-\ntems have failed to learn a translation due to data\nsparseness,likefor asinine,whichshouldbetrans-\nlated asdumm,weseetranslations that are wrong,\nbutcouldbeplausible for(partial) loanwords ( asi-\nnine Situation\u2192Asinin-Situation ).\nThe English\u2192Russian examples show that\nthe subword systems are capable of translitera-\ntion. However, transliteration errors do occur,\neither due to ambiguous transliterations, or be-\ncause of non-consistent segmentations between\nsource and target text which make it hard for\nthe system to learn a transliteration mapping.\nNote that the BPE-60k system encodes Mirza-\nyevainconsistently for the two language pairs\n(Mirz|ayeva\u2192\u041c\u0438\u0440|\u0437\u0430|\u0435\u0432\u0430Mir|za|eva). This ex-\nample is still translated correctly, but we observe\nspurious insertions and deletions of characters in\nthe BPE-60k system. An example is the translit-\neration of rak\ufb01sk, where a \u043fis inserted and a \u043a\nis deleted. We trace this error back to transla-\nsystem sentence\nsource healthresearchinstitutes\nreference Gesundheitsforschungsinstitute\nWDict Forschungsinstitute\nC2-50k Fo|rs|ch|un|gs|in|st|it|ut|io|ne|n\nBPE-60k Gesundheits |forsch|ungsinstitu |ten\nBPE-J90k Gesundheits |forsch|ungsin|stitute\nsource asinine situation\nreference dumme Situation\nWDict asinine situation \u2192UNK\u2192asinine\nC2-50k as|in|in|e situation \u2192As|in|en|si|tu|at|io|n\nBPE-60k as|in|ine situation \u2192A|in|line-|Situation\nBPE-J90K as|in|ine situation \u2192As|in|in-|Situation\nTable 4: English\u2192German translation example.\n\u201c|\u201d marks subword boundaries.\nsystem sentence\nsource Mirzayeva\nreference \u041c\u0438\u0440\u0437\u0430\u0435\u0432\u0430 (Mirzaeva)\nWDict Mirzayeva \u2192UNK\u2192Mirzayeva\nC2-50k Mi|rz|ay|ev|a\u2192\u041c\u0438|\u0440\u0437|\u0430\u0435|\u0432\u0430(Mi|rz|ae|va)\nBPE-60k Mirz|ayeva\u2192\u041c\u0438\u0440|\u0437\u0430|\u0435\u0432\u0430(Mir|za|eva)\nBPE-J90k Mir|za|yeva\u2192\u041c\u0438\u0440|\u0437\u0430|\u0435\u0432\u0430(Mir|za|eva)\nsource rak\ufb01sk\nreference \u0440\u0430\u043a\u0444\u0438\u0441\u043a\u0430 (rak\ufb01ska)\nWDict rak\ufb01sk\u2192UNK\u2192rak\ufb01sk\nC2-50k ra|kf|is|k\u2192\u0440\u0430|\u043a\u0444|\u0438\u0441|\u043a(ra|kf|is|k)\nBPE-60k rak|f|isk\u2192\u043f\u0440\u0430|\u0444|\u0438\u0441\u043a(pra|f|isk)\nBPE-J90k rak|f|isk\u2192\u0440\u0430\u043a|\u0444|\u0438\u0441\u043a\u0430(rak|f|iska)\nTable 5: English\u2192Russian translation examples.\n\u201c|\u201d marks subword boundaries.\n(pra|krit|i)),fromwhichthetranslation( rak\u2192\u043f\u0440\u0430)\nis erroneously learned. The segmentation of the\njoint BPE system (BPE-J90k) is more consistent\n(pra|krit|i\u2192\u043f\u0440\u0430|\u043a\u0440\u0438\u0442|\u0438(pra|krit|i)).",
                "subsection": []
            }
        ]
    },
    {
        "id": "5",
        "section": "Analysis",
        "text": "",
        "subsection": []
    },
    {
        "id": "6",
        "section": "Conclusion",
        "text": "The main contribution of this paper is that we\nshow that neural machine translation systems are\ncapable of open-vocabulary translation by repre-\nsenting rare and unseen words as a sequence of\nsubword units.14This is both simpler and more\neffective than using a back-off translation model.\nWe introduce a variant of byte pair encoding for\nword segmentation, which is capable of encod-\ning open vocabularies with a compact symbol vo-\ncabulary of variable-length subword units. We\nshow performance gains over the baseline with\nbothBPEsegmentation, andasimplecharacter bi-\ngram segmentation.\nOur analysis shows that not only out-of-\nvocabulary words, but also rare in-vocabulary\nwords are translated poorly by our baseline NMT\n14system, and that reducing the vocabulary size\nof subword models can actually improve perfor-\nmance. Inthiswork,ourchoiceofvocabulary size\nis somewhat arbitrary, and mainly motivated by\ncomparison to prior work. One avenue of future\nresearchistolearntheoptimalvocabulary sizefor\na translation task, which we expect to depend on\nthe language pair and amount of training data, au-\ntomatically. We also believe there is further po-\ntential in bilingually informed segmentation algo-\nrithms to create more alignable subword units, al-\nthough the segmentation algorithm cannot rely on\nthetarget text at runtime.\nWhile the relative effectiveness will depend on\nlanguage-speci\ufb01c factors such as vocabulary size,\nwe believe that subword segmentations are suit-\nable for most language pairs, eliminating the need\nfor large NMTvocabularies or back-off models.\nAcknowledgments\nWe thank Maja Popovi \u00b4c for her implementa-\ntion of CHRF, with which we veri\ufb01ed our re-\nimplementation. The research presented in this\npublication was conducted in cooperation with\nSamsung Electronics Polska sp. z o.o. - Sam-\nsung R&D Institute Poland. This project received\nfunding from theEuropean Union\u2019s Horizon 2020\nresearch and innovation programme under grant\nagreement 645452 (QT21).",
        "subsection": []
    },
    {
        "missing": []
    },
    {
        "references": []
    },
    {
        "title": "Neural Machine Translation of Rare Words with Subword Units",
        "arxiv_id": "1508.07909"
    }
]