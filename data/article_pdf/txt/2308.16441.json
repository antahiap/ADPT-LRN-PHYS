[
    {
        "id": "",
        "section": "ABSTRACT",
        "text": "As the basic element of graph-structured data, node has been recog-\nnized as the main object of study in graph representation learning.\nA single node intuitively has multiple node-centered subgraphs\nfrom the whole graph (e.g., one person in a social network has mul-\ntiple social circles based on his different relationships). We study\nthis intuition under the framework of graph contrastive learning,\nand propose a multiple node-centered subgraphs contrastive repre-\nsentation learning method to learn node representation on graphs\nin a self-supervised way. Specifically, we carefully design a series\nof node-centered regional subgraphs of the central node. Then,\nthe mutual information between different subgraphs of the same\nnode is maximized by contrastive loss. Experiments on various\nreal-world datasets and different downstream tasks demonstrate\nthat our model has achieved state-of-the-art results.\nCCS CONCEPTS\n\u2022Computing methodologies \u2192Unsupervised learning ;Neural\nnetworks ;Learning latent representations .\nKEYWORDS\nContrastive Learning, Node-centered Subgraph, Graph Representa-\ntion Learning\nACM Reference Format:\nDong Li, Wenjun Wang, Minglai Shao, and Chen Zhao. 2023. Contrastive\nRepresentation Learning Based on Multiple Node-centered Subgraphs. In\nProceedings of the 32nd ACM International Conference on Information and\nKnowledge Management (CIKM \u201923), October 21\u201325, 2023, Birmingham, United\nKingdom. ACM, New York, NY, USA, 10 pages. https://doi.org/10.1145/\n3583780.3614825",
        "subsection": []
    },
    {
        "id": "1",
        "section": "INTRODUCTION",
        "text": "Graph representation learning has received increasing attention\nrecently [ 5], which aims to transform high-dimensional graph-\nstructured data into low-dimensional dense vectorized representa-\ntions. As the basic elements of graph-structured data, node repre-\nsentation has been the main object of graph representation learning.\n\u2217Corresponding author\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than the\nauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or\nrepublish, to post on servers or to redistribute to lists, requires prior specific permission\nand/or a fee. Request permissions from permissions@acm.org.\nCIKM \u201923, October 21\u201325, 2023, Birmingham, United Kingdom\n\u00a92023 Copyright held by the owner/author(s). Publication rights licensed to ACM.\nACM ISBN 979-8-4007-0124-5/23/10. . . $15.00\nhttps://doi.org/10.1145/3583780.3614825\nFigure 1: A descriptive illustration of different proxy tasks\namong DGI, GMI, GIC and our proposed MNCSCL. The two-\nway arrows represent MI maximization, and the different\ncolors represent different models. The subgraph generator\nand multiple subgraphs of \u210e\ud835\udc56are described in details in Sec-\ntion 3.1 and Section 3.2.\nA comprehensive node representation can be well used for a vari-\nety of downstream tasks, such as node classification [ 14] and link\nprediction [3].\nA widespread graph representation learning method is the appli-\ncation of graph neural networks (GNN) [ 4,14,29,32,40]. But most\nof such methods focus on supervised learning, relying on super-\nvised signals in the graph. For real-world networks, the acquisition\nof these supervised signals is often cumbersome and expensive.\nSelf-supervised learning [ 11] is a popular research area in recent\nyears, which designs proxy tasks for unlabeled data to mine the\nrepresentational properties of the data itself as supervised informa-\ntion.\nAs one of the representative methods of self-supervised learning,\nthe proxy task of contrastive learning is to maximize the Mutual In-\nformation (MI) [ 25] between the input and related content [ 34]. For\nexample, Deep Graph Infomax (DGI) [ 30] maximizes MI between\nCIKM \u201923, October 21\u201325, 2023, Birmingham, United Kingdom Li et al.\nFigure 2: The pipelines of MNCSCL. For a specific node \ud835\udc63\ud835\udc56with attribute x\ud835\udc56, we first get its negative example \u02dcx\ud835\udc56through\nperturbing the structure and attributes of the input graph Gwith a corruption function C. Then we use a subgraph generator\nTto get a series of node-centered subgraphs G\ud835\udc56and their corresponding negative set \u02dcG\ud835\udc56from h\ud835\udc56and \u02dch\ud835\udc56(obtained by a shared\nencoderF). Finally, the mutual information between G\ud835\udc56and \u02dcGis maximized in the latent space V\ud835\udc56andU\ud835\udc56(obtained by a readout\nfunctionR) by contrastive loss. For more details, refer to the \u201coverall framework\u201d subsection in Section 3.\ncorrupted graph. Graphical Mutual Information (GMI) [ 23] doesn\u2019t\nuse corruption function, instead, it maximizes the MI between the\nhidden representation of nodes and their original local structure.\nGraph InfoClust (GIC) [ 19], on the other hand, maximizes the MI\nbetween the node representation and its corresponding cluster\nrepresentation on the basis of DGI. Although these methods have\nachieved many advances, they all focus on the MI between node\nembeddings and only one related graph structure, as shown in\nFigure 1.\nIn reality, we can look at a specific thing from multiple perspec-\ntives. For graph data, we can observe individual nodes in a graph\nfrom multiple perspectives, yet little literature has focused on this.\nIntuitively, for an individual in a social network, there may be a\nsocial circle of relatives based on blood relations, a social circle\nof colleagues based on work relations, and other social circles of\nfriends with many different interests. If we analyze this individ-\nual from these different social circles, it is actually equivalent to\nlearning from multiple perspectives on the nodes in this network.\nBased on this intuition, we propose Multiple Node-centered\nSubgraphs Contrastive Representation Learning (MNCSCL). MNC-\nSCL takes each node in the network as the center and samples its\nnode-centered regional subgraphs under different semantics, thus\nforming several different perspectives of the corresponding node,\nas shown in Figure 1. More specifically, we first generate a negative\nexample through the corrupt function, then generate a series of\nnode-centered subgraphs of the original graph by the view gen-\nerator, and sample the corresponding subgraphs on the negative\nexample. Then, these subgraphs are fed into graph neural networkencoders to obtain the representations of central nodes and its\nsubgraphs after pooling. Finally, the mutual information between\ndifferent subgraphs of the same node is maximized in the latent\nspace by contrastive learning objective function. Experimental re-\nsults on a variety of datasets demonstrate the superb performance\nof our design. The major contributions of this paper are as follows:\n\u2022We propose a novel framework to learn node representation\nthrough multiple node-centered subgraphs of nodes, which\nis a novel idea in current work to observe a single node from\nmultiple perspectives.\n\u2022We carefully design five node-centered subgraphs and ana-\nlyze the influence of different subgraphs on the learning qual-\nity of node representation through extensive experiments,\nwhich is of reference significance.\n\u2022We evaluated MNCSCL on five standard datasets and two\ndownstream tasks to validate the effectiveness of the pro-\nposed method. Our experiments show that the contrastive\nlearning of multiple subgraphs outperforms the above men-\ntioned single-subgraph contrastive learning in terms of re-\nsults.",
        "subsection": [
            {
                "id": "2.1",
                "section": "Graph Representation Learning Based on",
                "text": "Contrastive Learning\nInspired by the advances of contrastive learning in fields such as\nCV and NLP, some work has started to apply contrastive learning\nContrastive Representation Learning Based on Multiple Node-centered Subgraphs CIKM \u201923, October 21\u201325, 2023, Birmingham, United Kingdom\ngraph contrastive learning is to maximize the MI between similar\ninstances in a graph [ 33], and its model design focuses on three mod-\nules: data augmentation, proxy tasks, and contrastive objectives.\nAmong them, the most important modules is the proxy task, which\ndescribes the definition of similar instances (i.e. positive example)\nand dissimilar instances (i.e. negative example). DGI [ 30] extends\nthe idea of DIM [ 8] to graph data to learn node representations by\nmaximizing the MI between local node representation and global\ngraph representation. GMI [ 23] takes nodes and their neighbors\nas objects of study and maximizes the MI between hidden repre-\nsentation of each node and the original features of its neighboring\nnodes. GIC [ 19] clusters the nodes in the graph by a differentiable\nversion of K-means clustering, and then maximizes the MI between\nthe node representation and its corresponding cluster summaries.\nSUBG-CON [ 10] obtains the context subgraph of each node by\nsubgraph sampling based data augmentation, and then maximizes\nthe consistency between them. Despite the good results achieved,\nthese works perform graph contrastive learning only on a single\nperspective for nodes.",
                "subsection": []
            },
            {
                "id": "2.2",
                "section": "Multi-view Contrastive Learning",
                "text": "Recently, multi-view representation learning has become a rapidly\ngrowing direction in machine learning and data mining areas [ 17,\n22,37\u201340]. It has had a lot of success in areas such as computer\nvision. For instance, Contrastive Multiview Coding (CMC) [ 28] uses\ncontrastive learning to maximize the mutual information between\nmultiple views of a dataset to perform representation learning of\nimages. MVGRL [ 6] obtains multiple views of graph through data\naugmentation, and they find out that unlike visual representation\nlearning, increasing the number of views of the entire graph to\nmore than two by data augmentation does not improve perfor-\nmance. Unlike the multi-view graph contrastive learning summa-\nrized in MVGRL which focuses on node attributes at graph-level,\nour multiple node-centered subgraphs in this paper focus more on\nthe differences in structure at node-level.",
                "subsection": []
            }
        ]
    },
    {
        "id": "2",
        "section": "RELATED WORK",
        "text": "",
        "subsection": [
            {
                "id": "3.1",
                "section": "Subgraph Generator",
                "text": "We can deal with a whole graph from different views [ 6], and the\nsame is true for any node in the graph. For a single node, there are\nmany subgraphs centered on it (e.g., ego network [ 42]). If these\nnode-centered regional subgraphs have certain semantic informa-\ntion (e.g., ego network represents a specific individual and other\npersons who have a social relationship with him), then we can treat\nthem as different perspectives of the central node.\nThe main role of the subgraph generator Tis to sample these\nnode-centered regional subgraphs from Gand generate the corre-\nsponding negative samples from \u02dcG. Specifically, for a specific node\n\ud835\udc63\ud835\udc56and a prepared node-centered subgraph type \ud835\udc58, the subgraph gen-\neratorTfirst gets\ud835\udc56\ud835\udc51\ud835\udc65, a set which represents the index of chosen\nnodes for subgraph G\ud835\udc58\n\ud835\udc56to be obtained. Then, the node representa-\ntion matrix H\ud835\udc58\n\ud835\udc56\u2208R\ud835\udc41\u2032\u00d7\ud835\udc39\u2032and adjacency matrix A\ud835\udc58\n\ud835\udc56\u2208R\ud835\udc41\u2032\u00d7\ud835\udc41\u2032of\nG\ud835\udc58\n\ud835\udc56are denoted respectively as\nH\ud835\udc58\n\ud835\udc56=H\ud835\udc56\ud835\udc51\ud835\udc65,:,A\ud835\udc58\n\ud835\udc56=A\ud835\udc56\ud835\udc51\ud835\udc65,\ud835\udc56\ud835\udc51\ud835\udc65, (1)\nwhere\u00b7\ud835\udc56\ud835\udc51\ud835\udc65is an indexing operation and \ud835\udc41\u2032is the length of \ud835\udc56\ud835\udc51\ud835\udc65.\nIn this way, we can obtain the \ud835\udc58-th node-centered subgraph\nG\ud835\udc58\n\ud835\udc56=(H\ud835\udc58\n\ud835\udc56,A\ud835\udc58\n\ud835\udc56) \u223c T( H,A)for any specific node \ud835\udc63\ud835\udc56. Likewise,\nthe corresponding negative example can be obtained by \u02dcG\ud835\udc58\n\ud835\udc56=\n(\u02dcH\ud835\udc58\n\ud835\udc56,\u02dcA\ud835\udc58\n\ud835\udc56)=(\u02dcH\ud835\udc56\ud835\udc51\ud835\udc65,:,\u02dcA\ud835\udc56\ud835\udc51\ud835\udc65,\ud835\udc56\ud835\udc51\ud835\udc65).3.2 Node-centered Subgraphs Design\nTo learn a more comprehensive representation, we carefully design\nfive different node-centered subgraphs, as illustrated in Figure 3.\nThe details of them are as follows:\nSubgraph 1: Basic subgraph. The basic subgraph only contains\nthe central node itself (i.e., \ud835\udc41\u2032=1), that means for each node \ud835\udc63\ud835\udc56:\n\ud835\udc56\ud835\udc51\ud835\udc65={\ud835\udc56}. (2)\nFurther, we can get the basic subgraph representation and its cor-\nresponding negative example by v1\n\ud835\udc56=h\ud835\udc56andu1\n\ud835\udc56=\u02dch\ud835\udc56. For any\nspecific node, basic subgraph is the purest \u201csubgraph\u201d as well as\nthe main subgraph, which contains the most concentrated features\nof the node iteself.\nSubgraph 2: Neighboring subgraph. The neighbors of the\ncentral node are often closely related to the node in structure, and\nstudy with them can better capture the structural features of node\n[4]. The neighboring subgraph contains all nodes with a distance\nless than or equal to \ud835\udc51from the central node \ud835\udc63\ud835\udc56, denoted as\n\ud835\udc56\ud835\udc51\ud835\udc65={\ud835\udc57|\ud835\udc51\ud835\udc56\ud835\udc60(\ud835\udc63\ud835\udc56,\ud835\udc63\ud835\udc57)\u2264\ud835\udc51}, (3)\nwhere\ud835\udc51\ud835\udc56\ud835\udc60(\u00b7,\u00b7)is a function used to get the distance between two\nnodes. After obtaining the neighboring subgraph G2\n\ud835\udc56by Eq. (1), a\nreadout functionR:R\ud835\udc41\u00d7\ud835\udc39\u2032\u2192R\ud835\udc39\u2032is used to obtain the neigh-\nboring subgraph representation and its corresponding negative\nexample:\nv2\n\ud835\udc56=R(H2\n\ud835\udc56),u2\n\ud835\udc56=R(\u02dcH2\n\ud835\udc56). (4)\nIt is beneficial to learn a more comprehensive representation\nwhen we take a larger range of neighbors. But at the same time, the\nfeatures of the central node will be weakened, which will cause the\nmodel to be more inclined to learn the representation of a region\nor even the whole graph. It follows that it\u2019s important to choose\nthe value of \ud835\udc51. As shown in the Figure 5, the model reaches the best\nperformance when \ud835\udc51=1for the neighboring subgraph.\nSubgraph 3: Intimate subgraph. The intimate subgraph takes\ninto account the similarity that actually exists between two nodes\nin the input graph G. It contains the first \ud835\udc59nodes that are most\nsimilar to the central node. This is equivalent to identifying the\nnodes that are structurally close to the central node from another\nperspective, and thus learning the structural features of the nodes\nmore comprehensively.\nThe similarity between nodes is usually measured by a similarity\nscores matrix S\u2208R\ud835\udc41\u00d7\ud835\udc41, where S(\ud835\udc56,\ud835\udc57)measures the similarity\nbetween nodes \ud835\udc63\ud835\udc56and\ud835\udc63\ud835\udc57. Here we follow the personalized pagerank\n(PPR) algorithm [ 9] as introduced in [ 36]. The similarity scores\nmatrix Sbased on the PPR algorithm can be denoted as\nS=\ud835\udefc\u00b7(I\u2212(1\u2212\ud835\udefc)\u00b7\u00afA)\u22121, (5)\nwhere Iis the identity matrix and \u00afA=AD\u22121denotes the colum-\nnormalized adjacency matrix. Dis the diagonal matrix correspond-\ning to Awith D(\ud835\udc56,\ud835\udc56)=\u00cd\n\ud835\udc57A(\ud835\udc56,\ud835\udc57)on its diagonal. \ud835\udefc\u2208[0,1]is a\nparameter which is set as 0.15 in [ 10]. For a specific node \ud835\udc63\ud835\udc56, the\nsubgraph generator Tchooses its top- \ud835\udc59similar nodes (i.e., \ud835\udc41\u2032=\ud835\udc59)\nto generate intimate subgraph with S(\ud835\udc56,:), which can be denoted as\n\ud835\udc56\ud835\udc51\ud835\udc65=\ud835\udc61\ud835\udc5c\ud835\udc5d_\ud835\udc5f\ud835\udc4e\ud835\udc5b\ud835\udc58(S(\ud835\udc56,:),\ud835\udc59), (6)\nwhere\ud835\udc61\ud835\udc5c\ud835\udc5d_\ud835\udc5f\ud835\udc4e\ud835\udc5b\ud835\udc58(\u00b7)is a function that selects the top- \ud835\udc59values from\na vector and return the corresponding indices. Same asContrastive Representation Learning Based on Multiple Node-centered Subgraphs CIKM \u201923, October 21\u201325, 2023, Birmingham, United Kingdom\n2subsection, we can obtain the intimate subgraph representation\nand its corresponding negative example with v3\n\ud835\udc56=R(H3\n\ud835\udc56)and\nu3\n\ud835\udc56=R(\u02dcH3\n\ud835\udc56).\nSubgraph 4: Communal subgraph. In graph clustering, nodes\nin a uniform cluster tend to have similarity in attributes. Therefore,\nwe can select all nodes in the cluster to which the central node\nbelongs to get the communal subgraph. These nodes with similar\nattributes to the central node can help the model better learn the\nattribute features of the central node.\nIt is particularly noteworthy that the other subgraphs are ob-\ntained independently of the node attributes (i.e., H), but the com-\nmunal subgraph is attribute-related. Since Hwill change during\ntraining, a fixed \ud835\udc56\ud835\udc51\ud835\udc65before the model training as other subgraphs\nmay lead to undesirable results.\nThere are already many methods to perform graph clustering\n[26]. We tried three different clustering strategies based on \ud835\udc3e-means\nclustering [18] due to the stable and excellent performance of it.\n\u2022Strategy 1: Precomputed K-means. Same as other sub-\ngraphs, the node-centered subgraphs are sampled before\nmodel training starts. Specifically, the traditional \ud835\udc3e-means\nclustering algorithm is used to cluster the nodes in the input\ngraphG. For any specific node \ud835\udc63\ud835\udc56, take all the indices of\nnodes in its cluster as \ud835\udc56\ud835\udc51\ud835\udc65and further compute v4\n\ud835\udc56andu4\n\ud835\udc56.\n\u2022Strategy 2: A differentiable version of K-means. To up-\ndate the communal subgraph during training, we need an\nend-to-end clustering algorithm. Here we follow a differen-\ntiable version of K-means as introduced in [ 31]. For each\nnode\ud835\udc63\ud835\udc56, let\ud835\udf07\ud835\udc50denote the center of cluster \ud835\udc50and \u02c6\ud835\udefe\ud835\udc56\ud835\udc50(s.t.,\u00cd\n\ud835\udc50\u02c6\ud835\udefe\ud835\udc56\ud835\udc50=1,\u2200\ud835\udc56) denotes the degree to which node \ud835\udc63\ud835\udc56is as-\nsigned to cluster \ud835\udc50. Suppose that the number of clusters to be\nobtained is\ud835\udc36, ClusterNet updates \ud835\udf07\ud835\udc50via an iterative process\nby alternately setting.\n\ud835\udf07\ud835\udc50=\u00cd\n\ud835\udc56\u02c6\ud835\udefe\ud835\udc56\ud835\udc50h\ud835\udc56\u00cd\n\ud835\udc56\u02c6\ud835\udefe\ud835\udc56\ud835\udc50\ud835\udc50=1,...,\ud835\udc36, (7)\nand\n\u02c6\ud835\udefe\ud835\udc56\ud835\udc50=\ud835\udc52\ud835\udc65\ud835\udc5d(\u2212\ud835\udefd\u00b7\ud835\udc60\ud835\udc56\ud835\udc5a(h\ud835\udc56,\ud835\udf07\ud835\udc50))\u00cd\n\ud835\udc50\ud835\udc52\ud835\udc65\ud835\udc5d(\u2212\ud835\udefd\u00b7\ud835\udc60\ud835\udc56\ud835\udc5a(h\ud835\udc56,\ud835\udf07\ud835\udc50))\ud835\udc50=1,...,\ud835\udc36, (8)\nwhere\ud835\udefdis an inverse-temperature hyperparameter, the stan-\ndard K-means assignment is recovered when \ud835\udefd\u2192\u221e .\ud835\udc60\ud835\udc56\ud835\udc5a(\u00b7,\u00b7)\ndenotes a similarity function between two instances. Even-\ntually, we can get the communal subgraph representation\nby\nv4\n\ud835\udc56=\ud835\udf0e \ud835\udc36\u2211\ufe01\n\ud835\udc50=1\u02c6\ud835\udefe\ud835\udc56\ud835\udc50\ud835\udf07\ud835\udc50!\n, (9)\nwhere\ud835\udf0e(\u00b7)is the logistic sigmoid nonlinearity. Since this\nmethod does not get \ud835\udc56\ud835\udc51\ud835\udc65, we simply get negative exam-\nple{u4\n1,u4\n2,...,u4\n\ud835\udc41}of all nodes by row-wise shuffling of\n{v4\n1,v4\n2,...,v4\n\ud835\udc41}.\n\u2022Strategy 3: An end-to-end version of K-means with an\nestimation network. In this way, we replace the iterative\nprocess in Strategy 2 with an estimation network, which\nutilizes a multi-layer neural network to directly predict the\ndegree to which each node belongs to each cluster \u02c6\ud835\udefe\u2208R\ud835\udc41\u00d7\ud835\udc36,\nFigure 4: Take the five node-centered subgraphs of node \ud835\udc63\ud835\udc56as\nan example. (a) The \u201ccore view\u201d (left) and \u201cfull graph\u201d (right)\nparadigms. The numbers within the regions represent the\nnumber of MI in this region. For example, if we select all 5\nnode-centered subgraphs under the full graph case, MI will\nbe calculated once between every two subgraphs, and hence\nismarked with the number 10. (b) Contrastive lossess under\ncore view and full graph cases. Refer to Section 3.3 for more\ndetails.\ndenoted as\n\u02c6\ud835\udefe=\ud835\udc60\ud835\udc5c\ud835\udc53\ud835\udc61\ud835\udc5a\ud835\udc4e\ud835\udc65(\ud835\udc40\ud835\udc3f\ud835\udc43(H;\ud835\udf03)), (10)\nwhere\ud835\udc60\ud835\udc5c\ud835\udc53\ud835\udc61\ud835\udc5a\ud835\udc4e\ud835\udc65(\u00b7)is a softmax nonlinearity and \ud835\udc40\ud835\udc3f\ud835\udc43(\u00b7)is\na multi-layer neural network with trainable parameters \ud835\udf03.\nThen we get the communal subgraph representation by Eq.\n(8) and Eq. (9) as same as Strategy 2 .\nThe comparison of the three strategies is shown in Figure 5.\nAfter weighing both accuracy and efficiency, we chose Strategy 2\nto generate the communal subgraph.\nSubgraph 5: Full subgraph. To learn a comprehensive rep-\nresentation of a node, it is essential to observe it from a global\nperspective. The full subgraph contains all the nodes (i.e. \ud835\udc41\u2032=\ud835\udc41)\nin the input graph G, e.g., for any specific node \ud835\udc63\ud835\udc56,\n\ud835\udc56\ud835\udc51\ud835\udc65={\ud835\udc57|\ud835\udc57=1,2,...,\ud835\udc41}.CIKM \u201923, October 21\u201325, 2023, Birmingham, United Kingdom Li et al.\nTable 2: The statistics of all the five datasets. *Note that the node classification on PPI dataset is a multilabel classification\nproblem.\nTask Dataset Type #Nodes #Edges #Features #Classes Train / Val / Test\nNode classification\n& Link prediction\n(Transductive)Cora Citation network 2,708 5,429 1,433 7 0.05 / 0.18 / 0.37\nCiteseer Citation network 3,327 4,732 3,703 6 0.04 / 0.15 / 0.30\nPubmed Citation network 19,717 44,338 500 3 0.003 / 0.03 / 0.05\nNode classification\n(Inductive)Reddit Social network 232,965 11,606,919 602 41 0.66 / 0.10 / 0.24\nPPI Protein network 56,944 818,716 50 121* 0.79 / 0.11 / 0.10\nCompared with the previous subgraphs, the full subgraph con-\ntains far more nodes than they do, which extremely weakens the\nspecificity of the central node. At the same time, it is not conducive\nto learning a specialized node representation as all nodes have the\nsame full subgraph. Based on these ideas, we propose full subgraph\nfor specific node \ud835\udc63\ud835\udc56with self-weighted, denoted as\nv5\n\ud835\udc56=(1\u2212\ud835\udf02)R(H5\n\ud835\udc56)+\ud835\udf02h\ud835\udc56, (12)\nwhere\ud835\udf02\u2208[0,1]is a self-weighted factor.\nSo far, we have introduced five carefully designed node-centered\nsubgraphs. It is noted that subgraphs other than Subgraph 4 can\nbe precomputed before model training starts, which allows us to\nquickly obtain these subgraphs during training by performing only\none calculation before training.",
                "subsection": []
            },
            {
                "id": "3.3",
                "section": "Contrastive Loss",
                "text": "The key idea of self-supervised comparative learning is to define a\nproxy task to generate positive and negative samples. The encoder\nFthat generates the node representation is trained by contrast\nbetween positive and negative samples.\nTo handle multiple subgraphs we obtained before, we take the\n\u201ccore view\u201d (CV) and \u201cfull graph\u201d (FG) paradigms as introduced in\nCMC [ 28], as shown in Figure 4.(a). Further, the contrastive lossess\nunder core view and full graph cases are illustrated in Figure 4.(b).\nSpecifically, in the core view case, we regard Subgraph 1 as the most\ncritical node-centered subgraph. It and its corresponding negative\nsample constitute positive and negative pairs with Subgraph 2 \u223c5,\nrespectively. As for the full graph case, any two between Subgraph\n1\u223c5constitute positive pairs, and Subgraph 1 \u223c5respectively form\nnegative pairs with their corresponding negative examples.\nAfter defining the proxy task, we follow the intuitions from DGI\n[30] and use a noise-contrastive type objective with a standard\nbinary cross-entropy (BCE) loss between positive examples and\nnegative examples. MNCSCL\u2019s objective under core view case is\nL\ud835\udc36\ud835\udc49=\ud835\udc41\u2211\ufe01\n\ud835\udc56=1\ud835\udc3e\u2211\ufe01\n\ud835\udc57=2E(X,A)h\n\ud835\udc59\ud835\udc5c\ud835\udc54D(v1\n\ud835\udc56,v\ud835\udc57\n\ud835\udc56)i\n+\ud835\udc41\u2211\ufe01\n\ud835\udc56=1\ud835\udc3e\u2211\ufe01\n\ud835\udc57=2E(\u02dcX,\u02dcA)h\n\ud835\udc59\ud835\udc5c\ud835\udc54(1\u2212D( u1\n\ud835\udc56,v\ud835\udc57\n\ud835\udc56))i\n,(13)\nwhere\ud835\udc3eis the number of selected node-centered subgraphs and D:\nR\ud835\udc39\u2032\u00d7R\ud835\udc39\u2032\u2192Ris a discriminator which is used for estimating the\nMI by assigning higher scores to positive examples than negatives.When using the full graph case, the objective becomes\nL\ud835\udc39\ud835\udc3a=\ud835\udc41\u2211\ufe01\n\ud835\udc56=1\ud835\udc3e\u22121\u2211\ufe01\n\ud835\udc57=1\ud835\udc3e\u2211\ufe01\n\ud835\udc58=\ud835\udc57+1E(X,A)h\n\ud835\udc59\ud835\udc5c\ud835\udc54D(v\ud835\udc57\n\ud835\udc56,v\ud835\udc58\n\ud835\udc56)i\n+\ud835\udc41\u2211\ufe01\n\ud835\udc56=1\ud835\udc3e\u2211\ufe01\n\ud835\udc57=1E(\u02dcX,\u02dcA)h\n\ud835\udc59\ud835\udc5c\ud835\udc54(1\u2212D( v\ud835\udc57\n\ud835\udc56,u\ud835\udc57\n\ud835\udc56))i\n.(14)\nThe Eq. (13) and Eq. (14) are used as the contrastive loss in the\nexperiments respectively.",
                "subsection": []
            }
        ]
    },
    {
        "id": "3",
        "section": "METHODOLOGY",
        "text": "Problem definition. Given a graphG=(V,E)with\ud835\udc41nodes,\nwhereV={\ud835\udc631,\ud835\udc632,...,\ud835\udc63\ud835\udc41}andErepresent the node set and the\nedge set respectively. X={x1,x2,...,x\ud835\udc41} \u2208R\ud835\udc41\u00d7\ud835\udc39is the node\nfeatures matrix, where x\ud835\udc56\u2208R\ud835\udc39denotes the features of dimension\n\ud835\udc39for node\ud835\udc63\ud835\udc56. We use the adjacency matrix A\u2208R\ud835\udc41\u00d7\ud835\udc41to represent\nthe connectivity of the graph, where A(\ud835\udc56,\ud835\udc57)=1if nodes\ud835\udc63\ud835\udc56and\ud835\udc63\ud835\udc57\nare linked, and A(\ud835\udc56,\ud835\udc57)=0otherwise. In this way, a graph can also\nbe represented asG=(X,A). IfV\u2032\u2282V is a subset of vertices of\nGandE\u2032consists of all of the edges in Ethat have both endpoints\ninV, then the subgraph S=(V\u2032,E\u2032)of graphGis an induced\nsubgraph. The subgraphs mentioned in this paper are all induced\nsubgraphs.\nThe goal of self-supervised graph representation learning is\nto learn a encoder F:R\ud835\udc41\u00d7\ud835\udc39\u00d7R\ud835\udc41\u00d7\ud835\udc41\u2192R\ud835\udc41\u00d7\ud835\udc39\u2032, which takes\nthe features matrix Xand the adjacency matrix Aas input to get\nthe node representation H={h1,h2,...,h\ud835\udc41}\u2208R\ud835\udc41\u00d7\ud835\udc39\u2032without\nlabel information, formulated as H=F(X,A). The learned node\nrepresentation Hcan be used directly for downstream tasks suchTable 1: Summary of notations. The first five notations are\ndescribed in detail in the following subsections.\nNotation Meaning\nF Shared encoder\nC Corruption function\nT Subgraph generator\nR Readout function\nD Discriminator\n\ud835\udc41 Number of nodes in the input graph\n\ud835\udc39 Feature dimension of each node\n\ud835\udc39\u2032Feature dimension of each node representation\n\ud835\udc3e Number of node-centered subgraphs sampled by T\n\ud835\udc41\u2032 Number of nodes in the corresponding\nnode-centered subgraph\n\ud835\udc51 Range of neighbors in the neighboring subgraph\n\ud835\udc59Number of nodes that are most similar to the\ncentral node for intimate subgraph\n\ud835\udc36 number of clusters for communal subgraph\n\ud835\udf02 self-weighted factor for full subgraph\nG,\u02dcG Input graph and its negative example obtained by C\nG\ud835\udc58\n\ud835\udc56,\u02dcG\ud835\udc58\n\ud835\udc56The\ud835\udc58-th node-centered subgraph of node \ud835\udc63\ud835\udc56and\nits negative example\nH\ud835\udc58\n\ud835\udc56,\u02dcH\ud835\udc58\n\ud835\udc56Node representation matrix of the \ud835\udc58-th node-centered\nsubgraph of node \ud835\udc63\ud835\udc56and its negative example\nA\ud835\udc58\n\ud835\udc56,\u02dcA\ud835\udc58\n\ud835\udc56Adjacency matrix of the \ud835\udc58-th node-centered subgraph\nof node\ud835\udc63\ud835\udc56and its negative example\nv\ud835\udc58\n\ud835\udc56,u\ud835\udc58\n\ud835\udc56Representation of the \ud835\udc58-th node-centered subgraph of\nnode\ud835\udc63\ud835\udc56and its negative example, obtained by R\nas node classification and link prediction. For the sake of clarity,\nwe list all important notations in Table 1.\nOverall framework. Inspired by recent graph representation\nlearning work based on contrastive learning, we propose MNCSCL\nalgorithm for graph representation learning by maximizing MI of\nmultiple node-centered subgraphs of nodes. As illustrated in Figure\n2, if there is only a single graph provided as input, the summarized\nsteps of MNCSCL are as follows:\n\u2022Utilize a corruption function Cto perturb the structure and\nattributes of the input graph Gto obtain a negative example\n\u02dcG=(\u02dcX,\u02dcA)\u223cC( X,A).\n\u2022Pass input graphGand negative example \u02dcGinto a shared\nencoderFto get node representation Hand \u02dcH.\n\u2022Use a subgraph generator Tto sample a series of node-\ncentered subgraphs G\ud835\udc56={G1\n\ud835\udc56,G2\n\ud835\udc56,...,G\ud835\udc3e\n\ud835\udc56}for node\ud835\udc63\ud835\udc56from\nthe input graphG, where\ud835\udc3eis the number of different sub-\ngraphs andG\ud835\udc58\n\ud835\udc56=(H\ud835\udc58\n\ud835\udc56,A\ud835\udc58\n\ud835\udc56),\ud835\udc58=1,2,...,\ud835\udc3e . The correspond-\ning node-centered subgraphs set \u02dcG\ud835\udc56={\u02dcG1\n\ud835\udc56,\u02dcG2\n\ud835\udc56,..., \u02dcG\ud835\udc3e\n\ud835\udc56}from\nthe negative example \u02dcGare further obtained according to\nG\ud835\udc56.\n\u2022Summarize all subgraphs in G\ud835\udc56and \u02dcG\ud835\udc56through a readout\nfunctionRto get their representations V\ud835\udc56={v1\n\ud835\udc56,v2\n\ud835\udc56,...,v\ud835\udc3e\n\ud835\udc56CIKM \u201923, October 21\u201325, 2023, Birmingham, United Kingdom Li et al.\nFigure 3: Five types of node-centered subgraphs for node\n\ud835\udc634from original graph. Note that nodes are divided into 3\nclusters in (4), which are {\ud835\udc631,\ud835\udc632},{\ud835\udc633,\ud835\udc634}and{\ud835\udc635,\ud835\udc636}.\nof node\ud835\udc63\ud835\udc56and the corresponding negative samples U\ud835\udc56=\n{u1\n\ud835\udc56,u2\n\ud835\udc56,...,u\ud835\udc3e\n\ud835\udc56}. As an example, v\ud835\udc58\n\ud835\udc56=R(H\ud835\udc58\n\ud835\udc56).\n\u2022Update parameters of F,RandD(mentioned later) by\napplying gradient descent to maximize Eq. (13) or Eq. (14).\nIn the following sections, we will elaborate on the crucial com-\nponents mentioned above.",
        "subsection": [
            {
                "id": "4.1",
                "section": "Experimental Settings",
                "text": "Datasets. We use 5 commonly used benchmark datasets in the\nprevious work [ 4,19] for node classification and link prediction\ndownstream tasks, including 3 transductive citation networks (i.e.,\nCora, Citeseer, and Pubmed), a inductive large social network (i.e.,\nReddit) and a inductive protein-protein interaction dataset that\ncontains multiple graphs with multiple labels (i.e., PPI).\n\u2022Cora, Citeseer, and PubMed are all citation networks, with\nCora and Citeseer focusing on papers in computer science\nand information science, and Pubmed containing a large\namount of literature information in medical and life science\nfields. They represent citation relationships between papers\nthrough graph data structure, where each node represents\na paper and the edges represent the citation relationships\nbetween papers.\n\u2022Reddit is a collection of information from Reddit, the world\u2019s\nlargest social news aggregation, discussion and community\nsite. the Reddit dataset provides a large amount of user-\ngenerated content, including posts, comments, polls and\nmore. In Reddit, posts are represented as nodes, and the\nconnections between them correspond to user comments.\n\u2022PPI is a protein-protein interaction dataset that contains\nmultiple graphs with multiple labels. PPI contains the in-\nteraction relationships between proteins that can form a\nnetwork or graph structure. Each node represents a protein,\nwhile edges indicate interactions between proteins.\nWe use all five datasets in the node classification task and follow the\nsettings in the division of the training set and the test set as same as\n[30]. In the link prediction task, we use Cora, Citeseer, and Pubmed\ndatasets and follow the setup described in [ 15]. The statistics of all\nContrastive Representation Learning Based on Multiple Node-centered Subgraphs CIKM \u201923, October 21\u201325, 2023, Birmingham, United Kingdom\nTable 3: The classification accuracy (in %) on the transductive datasets and the micro-averaged F1 ( \u00d7100) on the inductive\ndatasets of the node classification task. Some results are directly taken from their original papers (DGI, GMI#inductive, GIC\nand GRACE#inductive), and other compared results are taken from [ 10,20]. The second column is the data used in the training\nprocess (X: features matrix, A: adjacency matrix, Y: labels). The best result for each dataset is indicated by bolded.\nMethodInput Transductive Inductive\nX A Y Cora Citeseer Pubmed Reddit PPI\nRaw features \u2713 56.6\u00b10.4 57.8 \u00b10.2 69.1 \u00b10.2 58.5 \u00b10.1 42.5 \u00b10.3\nDeep Walk \u2713 67.2 43.2 65.3 32.4 52.9\nGCN \u2713 \u2713 \u2713 81.4\u00b10.6 70.3 \u00b10.7 76.8 \u00b10.6 93.3 \u00b10.1 51.5 \u00b10.6\nFastGCN \u2713 \u2713 \u2713 78.0\u00b12.1 63.5 \u00b11.8 74.4 \u00b10.8 89.5 \u00b11.2 63.7 \u00b10.6\nDGI \u2713 \u2713 82.3\u00b10.6 71.8 \u00b10.7 76.8 \u00b10.6 94.0 \u00b10.1 63.8 \u00b10.2\nGMI \u2713 \u2713 83.0\u00b10.2 72.4 \u00b10.2 79.9 \u00b10.4 95.0 \u00b10.02 65.0 \u00b10.02\nGIC \u2713 \u2713 81.7\u00b10.8 71.9 \u00b10.9 77.4 \u00b10.5 - -\nGRACE \u2713 \u2713 83.1\u00b10.2 72.1 \u00b10.1 79.6 \u00b10.5 94.2 \u00b10.0 66.2 \u00b10.1\nMVGRL \u2713 \u2713 82.9\u00b10.3 72.6 \u00b10.4 80.1 \u00b10.7 - -\nMNCSCL-FG \u2713 \u2713 84.3\u00b10.5 73.2 \u00b10.6 80.0 \u00b10.4 95.2 \u00b10.1 67.3 \u00b10.2\nMNCSCL-CV \u2713 \u2713 84.7 \u00b10.3 73.8 \u00b10.5 81.5 \u00b10.4 95.8 \u00b10.1 67.1\u00b10.2\nBaseline methods. In node classification task, the compared\nmethods include direct use of row features, 1 traditional unsuper-\nvised algorithm (i.e., Deep Walk [ 24]), two supervised graph neural\nnetworks (i.e., GCN [ 14] and FastGCN [ 1]) and 5 state-of-the-art\nself-supervised methods(i.e., DGI, GMI, GIC, GRACE [ 41] and MV-\nGRL [6]).\n\u2022DGI is one of the classical methods of graph representation\nlearning based on contrastive learning. It aims to maximize\nthe MI between the local perspective and the global per-\nspective of the input graph, as well as the corresponding\ncorrupted graph.\n\u2022GMI draws on the ideas of DGI, but rather than employing\na corruption function, this approach focuses on maximizing\nthe MI between the hidden representations of nodes and\ntheir original local structure.\n\u2022GIC is also inspired by DGI, its objective is to maximize the\nMI between the node\u2019s representation and the representation\nof the cluster to which it is assigned.\n\u2022GRACE propose a novel framework for unsupervised graph\nrepresentation learning by leveraging a contrastive objective\nat the node level. In order to enhance the contrast effect, they\ncreated two sets of negative pairs, one within the same view\nand the other across different views.\n\u2022MVGRL performs self-supervised learning by contrasting\nstructural views of graphs, where they contrast first-order\nneighbors of nodes as well as a graph diffusion, with good\nresults.\nIn link prediction, we directly follow the effective link prediction\nmethods used in GIC (i.e., Deep Walk, Spectral Clustering (SC) [ 27],\nVGAE [15], ARGVA [21], DGI and GIC).\n\u2022Spectral Clustering is initially introduced to solve the node\npartitioning problem in graph analysis. It has demonstrated\nsatisfactory performance across diverse domains, such as\ngraphs, text, images, and microarray data. Its effectiveness\nhas been widely acknowledged in these areas.\u2022VGAE is an unsupervised learning framework designed for\ngraph-structured data, utilizing the variational auto-encoder\n(VAE) methodology. In VGAE, a GNN-based encoder is em-\nployed to generate node embeddings, while a straightfor-\nward decoder is used to reconstruct the adjacency matrix.\n\u2022ARGVA is a graph embedding framework specifically de-\nsigned for graph data, incorporating adversarial learning\ntechniques. Similar to VGAE, ARGVA adopts a similar struc-\nture, but it learns the underlying data distribution through\nan adversarial approach.\nEvaluation metrics. For the node classification task, we classify\nthe test set by a logistic regression classifier, and then evaluate the\nperformance using classification accuracy for transductive datasets\n(i.e., Cora, Citeseer and Pubmed) and micro-averaged F1 score for\ninductive datasets (i.e., Reddit and PPI). Suppose that TP, FN, FP\nand TN represent the number of true positives, false negatives,\nfalse positives, and true negatives, respectively. Then classification\naccuracy can be calculated by \ud835\udc4e\ud835\udc50\ud835\udc50\ud835\udc62\ud835\udc5f\ud835\udc4e\ud835\udc50\ud835\udc66 =(\ud835\udc47\ud835\udc43+\ud835\udc47\ud835\udc41)/(\ud835\udc47\ud835\udc43+\ud835\udc39\ud835\udc43+\n\ud835\udc47\ud835\udc41+\ud835\udc39\ud835\udc41). Also micro-averaged F1 score can be calculated by\n\ud835\udc391\u2212\ud835\udc46\ud835\udc50\ud835\udc5c\ud835\udc5f\ud835\udc52 =2\u2217\ud835\udc5d\ud835\udc5f\ud835\udc52\ud835\udc50\ud835\udc56\ud835\udc60\ud835\udc56\ud835\udc5c\ud835\udc5b\u2217\ud835\udc5f\ud835\udc52\ud835\udc50\ud835\udc4e\ud835\udc59\ud835\udc59/(\ud835\udc5d\ud835\udc5f\ud835\udc52\ud835\udc50\ud835\udc56\ud835\udc60\ud835\udc56\ud835\udc5c\ud835\udc5b+\ud835\udc5f\ud835\udc52\ud835\udc50\ud835\udc4e\ud835\udc59\ud835\udc59), where\n\ud835\udc5d\ud835\udc5f\ud835\udc52\ud835\udc50\ud835\udc56\ud835\udc60\ud835\udc56\ud835\udc5c\ud835\udc5b =\ud835\udc47\ud835\udc43/(\ud835\udc47\ud835\udc43+\ud835\udc39\ud835\udc43)and\ud835\udc5f\ud835\udc52\ud835\udc50\ud835\udc4e\ud835\udc59\ud835\udc59 =\ud835\udc47\ud835\udc43/(\ud835\udc47\ud835\udc43+\ud835\udc39\ud835\udc41). For the\nlink prediction task, we use the AUC score (the area under ROC\ncurve) and the AP score (the area under Precision-Recall curve) for\nevaluation. The closer the AUC score and the AP score approaches\n1, the better the performance of the algorithm is.\nTraining strategy. We implement MNCSCL using PyTorch [ 12]\non 4 NVIDIA GeForce RTX 3090 GPUs and use Adam optimizer [ 13]\nwith an initial learning rate of 0.001 (specially, 0.0001 for Reddit)\nduring training. We follow settings in DGI that using an early\nstopping strategy with a patience of 20 epochs for transductive\ndatasets and a fixed number of epochs (150 on Reddit, 20 on PPI)\nfor inductive datasets. For large graphs, we adopt the sampling\nCIKM \u201923, October 21\u201325, 2023, Birmingham, United Kingdom Li et al.\nTable 4: The AUC scores (in %) of the link prediction task. The results of the compared methods are replicated from [ 19]. The\nbest result for each dataset is indicated by bolded.\nMethodCora Citeseer Pubmed\nAUC AP AUC AP AUC AP\nDeepWalk 83.1 \u00b10.01 85.0 \u00b10.00 80.5 \u00b10.02 83.6 \u00b10.01 84.4 \u00b10.00 84.1 \u00b10.00\nSpectral Clustering 84.6 \u00b10.01 88.5 \u00b10.00 80.5 \u00b10.01 85.0 \u00b10.01 84.2 \u00b10.02 87.8 \u00b10.01\nVGAE 91.4 \u00b10.01 92.6 \u00b10.01 90.8 \u00b10.02 92.0 \u00b10.02 96.4 \u00b10.00 96.5 \u00b10.00\nARGVA 92.4 \u00b10.004 93.2 \u00b10.003 92.4 \u00b10.003 93.0 \u00b10.003 96.8 \u00b10.001 97.1 \u00b10.001\nDGI 89.8 \u00b10.8 89.7 \u00b11.0 95.5 \u00b11.0 95.7 \u00b11.0 91.2 \u00b10.6 92.2 \u00b10.5\nGIC 93.5 \u00b10.6 93.3 \u00b10.7 97.0 \u00b10.5 96.8 \u00b10.5 93.7 \u00b10.3 93.5 \u00b10.3\nMNCSCL-CV 94.8 \u00b10.4 94.2 \u00b10.6 97.7 \u00b10.4 97.2 \u00b10.5 94.8\u00b10.2 95.4 \u00b10.4",
                "subsection": []
            },
            {
                "id": "4.2",
                "section": "Implementation Details",
                "text": "Encoder design. For transductive datasets, we adopt a one-layer\nGraph Convolutional Network (GCN) as our encoder, with the\nfollowing propagation rule:\nF(X,A)=\ud835\udf0e(\u02c6D\u22121\n2\u02c6A\u02c6D\u22121\n2XW), (15)\nwhere \u02c6A=A+I\ud835\udc41is the adjacency matrix with self-loops and\n\u02c6D(\ud835\udc56,\ud835\udc56)=\u00cd\n\ud835\udc57\u02c6A(\ud835\udc56,\ud835\udc57)is its corresponding degree matrix. \ud835\udf0e(\u00b7)is the\nPReLU nonlinearity [ 7] and Wis a learnable parameter matrix\nwith\ud835\udc39\u2032=512(specially,\ud835\udc39\u2032=256on Pubmed). As for inductive\ndatasets, we adopt a one-layer GCN with skip connections [ 35] as\nour encoder, with the following propagation rule:\nF(X,A)=\ud835\udf0e(\u02c6D\u22121\n2\u02c6A\u02c6D\u22121\n2XW+\u02c6AW\ud835\udc60\ud835\udc58\ud835\udc56\ud835\udc5d), (16)\nwhere W\ud835\udc60\ud835\udc58\ud835\udc56\ud835\udc5dis a learnable parameter matrix with \ud835\udc39\u2032=512for skip\nconnections.\nCorruption function. For transductive datasets,we transform\nadjacency matrix Ato a diffusion matrix U. Specifically, we compute\ndiffusion using fast approximation and sparsification methods [ 2]\nwith heat kernel [16]:\nU=exp(\ud835\udc61AD\u22121\u2212\ud835\udc61), (17)\nwhere Dis a diagonal degree matrix as in Section 3.1 and \ud835\udc61is\ndiffusion time [ 2]. For Reddit dataset, we implement the corruption\nfunctionCby keeping the adjacency matrix Aunchanged (i.e.\n\u02dcA=A) and perturbing the feature matrix Xby row-wise shuffling.\nAnd for PPI dataset, we simply samples a different graph from the\ntraining set due to it\u2019s a multiple-graph dataset.\nReadout function. We use identical readout function Rfor all\ndatasets, which performs a simple average of all node features for\na given subgraph with \ud835\udc41\u2032nodes:\nR(H)=\ud835\udf0e \n1\n\ud835\udc41\u2032\ud835\udc41\u2032\u2211\ufe01\n\ud835\udc56=1h\ud835\udc56!\n, (18)\nwhere\ud835\udf0e(\u00b7)is the logistic sigmoid nonlinearity.\nDiscriminator. We use a simple bilinear scoring function as\ndiscriminatorD:\nD(h\ud835\udc56,h\ud835\udc57)=\ud835\udf0e(h\ud835\udc47\n\ud835\udc56W\ud835\udc51h\ud835\udc57), (19)\nwhere W\ud835\udc51is a learnable scoring matrix and \ud835\udf0e(\u00b7)is the logistic\nsigmoid nonlinearity.Table 5: The classification accuracy (in %) of different node-\ncentered subgraphs combinations ( Subgraph 1 : Basic sub-\ngraph, Subgraph 2 : Neighboring subgraph, Subgraph 3 : Inti-\nmate subgraph, Subgraph 4 : Communal subgraph, Subgraph\n5: Full subgraph) on Cora, Citeseer and Pubmed datasets with\nsame hyperparameter setting. Note that due to the use of the\n\u201ccore view\u201d paradigm, there must be at least the basic sub-\ngraph and another subgraph. The best result for each dataset\nis indicated by bolded.\nSubgraphs Dataset\n1 2 3 4 5 Cora Citeseer Pubmed\n\u2713 \u2713 82.5\u00b10.7 72.2 \u00b10.7 77.5 \u00b10.1\n\u2713 \u2713 82.8\u00b10.3 72.5 \u00b10.7 78.6 \u00b10.9\n\u2713 \u2713 82.7\u00b10.6 72.1 \u00b10.6 78.2 \u00b10.4\n\u2713 \u2713 82.7\u00b10.5 71.8 \u00b10.6 78.1 \u00b10.6\nAVG of 2 Subgraphs 82.7 \u00b10.6 72.2 \u00b10.7 78.2 \u00b10.6\n\u2713 \u2713 \u2713 83.1\u00b10.5 72.7 \u00b10.9 78.3 \u00b10.8\n\u2713 \u2713 \u2713 82.9\u00b10.9 72.3 \u00b10.5 78.0 \u00b10.9\n\u2713 \u2713 \u2713 83.2\u00b10.4 72.1 \u00b10.4 77.6 \u00b10.8\n\u2713 \u2713 \u2713 82.9\u00b10.5 72.0 \u00b10.7 78.6 \u00b10.4\n\u2713 \u2713 \u2713 83.0\u00b10.5 72.9 \u00b10.5 78.7 \u00b10.8\n\u2713 \u2713 \u2713 83.1\u00b10.8 72.9 \u00b10.6 78.5 \u00b10.6\nAVG of 3 Subgraphs 83.0 \u00b10.6 72.5 \u00b10.7 78.3 \u00b10.8\n\u2713 \u2713 \u2713 \u2713 83.5\u00b10.4 72.6 \u00b10.7 79.2 \u00b10.4\n\u2713 \u2713 \u2713 \u2713 83.5\u00b10.4 73.0 \u00b10.6 78.8 \u00b10.9\n\u2713 \u2713 \u2713 \u2713 83.6\u00b10.6 72.4 \u00b10.6 78.5 \u00b10.8\n\u2713 \u2713 \u2713 \u2713 83.3\u00b10.7 73.1 \u00b11.0 78.6 \u00b10.6\nAVG of 4 Subgraphs 83.5 \u00b10.5 72.7 \u00b10.8 78.7 \u00b10.7\n\u2713 \u2713 \u2713 \u2713 \u2713 84.7 \u00b10.3 73.8 \u00b10.5 81.5 \u00b10.4\nDetails of node-centered subgraphs. We conduct experiments\nwith all five node-centered subgraphs on two different contrastive\nlosses (named MNCSCL-FG under full graph case and MNCSCL-CV\nunder core view case, respectively). More specifically, we choose\n\ud835\udc51=1for the neighboring subgraph and Strategy 2 for the communal\nsubgraph. For hyperparameters, we set the size of intimate subgraph\n\ud835\udc59as 20 (specially, 10 on citeseer). The number of clusters \ud835\udc36Contrastive Representation Learning Based on Multiple Node-centered Subgraphs CIKM \u201923, October 21\u201325, 2023, Birmingham, United Kingdom\nFigure 5: Heat map of classification accuracy (in %) on three\ntransductive datasets when choosing different range of neigh-\nbors in the neighboring subgraph ( \ud835\udc51=1,\ud835\udc51=2and\ud835\udc51=3) and\ndifferent clustering strategies in the communal subgraph\n(S-1, S-2 and S-3, where S-1 denotes Strategy 1 , S-2 denotes\nStrategy 2 and S-3 denotes Strategy 3 ). The bolded value in\neach sub-plot represents the maximum classification accu-\nracy for the current dataset.\ninverse-temperature hyperparameter \ud835\udefdfor communal subgraph is\nset to 128 and 10 respectively. To build a proper full subgraph, we\nalso set the self-weighted factor as 0.01.",
                "subsection": []
            },
            {
                "id": "4.3",
                "section": "Experimental Results and Analysis",
                "text": "Node classification. Experiments show that MNCSCL achieves\nthe best performance on all five datasets compared to other com-\npeting self-supervised methods, as shown in Table 3. We believe\nthis robust performance is due to our comparison of multiple node-\ncentered subgraphs, resulting in learning a more comprehensive\nnode representation. Although MNCSCL-FG and MNCSCL-CV both\nhave shown excellent performance, MNCSCL-FG performs better\non PPI dataset and MNCSCL-CV performs better on other datasets.\nWe think this is due to the very sparse available features on the\nPPI (over 40% of the nodes have all-zero features). It is more ef-\nfective to use more perspectives for comparison on such sparse\ngraph datasets. For other datasets, the contrastive loss under core\nview case is enough for MNCSCL to learn comprehensive infor-\nmation about the nodes, too much comparison will instead lead to\noverfitting and waste of resources. Compared to supervised meth-\nods, MNCSCL also outperforms the two compared methods on\nall datasets. This shows that our method is also very competitive\ncompared to traditional supervision methods.\nLink prediction. To test the generalization capability of MNC-\nSCL, we intend to further investigate the performance of MNCSCL\nin link prediction task, as shown in Table 4. We find that MNCSCL\noutperforms all competing methods on both the Cora and Cite-\nseer datasets, suggesting that a multiple node-centered subgraphs\nbased comparison can help the model learn node representations\nwith good generalizability. The excellent performance on different\ndownstream tasks further proves the feasibility of our method.",
                "subsection": []
            },
            {
                "id": "4.4",
                "section": "Ablation Study",
                "text": "Node-centered subgraph combination. To investigate how the\nnumber of node-centered subgraphs affects the performance of\nMNCSCL, we permuted and combined five previously mentionednode-centered subgraphs under the core view case and observed\nthe classification accuracy of different subgraph combinations on\nCora, Citeseer and Pubmed datasets with same hyperparameter\nsetting, as shown in Table 5. Obviously, as the number of node-\ncentered subgraphs increases, the classification accuracy continues\nto increase, and the best results are achieved when all five subgraphs\nare used. This indicates that multiple node-centered subgraphs\ncontrastive learning can indeed learn better node representation.\nWe also notice that the classification accuracy improvement is\nmore obvious with the increase in the number of node-centered\nsubgraphs.\nRange of neighbors and clustering strategy. We investigate\nthe value of \ud835\udc51in the neighboring subgraph and the selection of\ndifferent clustering strategies in the communal subgraph, and the\nresults are shown in Fig 5. Here we use all five node-centered\nsubgraphs with the contrastive loss under core view case. It can be\nseen that the optimal choice in all three datasets is \ud835\udc51=1neighbors\nandStrategy 2 . Our analyses are as follows.\n\u2022For the neighboring subgraph, the classification accuracy\ntends to decrease as the vale of \ud835\udc51increases. We believe that\ntoo many neighboring nodes will lead to cause overfitting\nof the model and performance degradation. This viewpoint\ncan also be verified from the pubmed dataset, where it is\nnot obvious whether the classification accuracy is better or\nworse when setting \ud835\udc51=1and\ud835\udc51=2. Because the attributes\nof pubmed are relatively sparse, sometimes its neighboring\nsubgraph needs to contain more neighbors to get better\nresults.\n\u2022In the choice of clustering strategy, Strategy 1 is significantly\nless effective than the other two end-to-end strategies. Strat-\negy 2 and Strategy 3 show comparable performance, but\nconsidering that Strategy 3 involves an estimation network,\nwhich means more resource consumption, we finally choose\nStrategy 2 as the clustering strategy for the communal sub-\ngraph.",
                "subsection": []
            }
        ]
    },
    {
        "id": "4",
        "section": "EXPERIMENTS",
        "text": "",
        "subsection": []
    },
    {
        "id": "5",
        "section": "CONCLUSION",
        "text": "We propose Multiple Node-centered Subgraphs Contrastive Repre-\nsentation Learning (MNCSCL), a novel approach to self-supervised\ngraph representation learning. MNCSCL obtains five different node-\ncentered subgraphs carefully designed by us through a subgraph\ngenerator on each node, and maximizes the mutual information\nbetween them through two types of contrastive loss, thus allowing\nus to obtain comprehensive node representation that combines\ninformation from multiple node-centered subgraphs of nodes. Ex-\nperiments show that MNCSCL reach the advanced level of self-\nsupervised learning in both transductive and inductive node classi-\nfication tasks as well as in link prediction task.\nACKNOWLEDGMENTS\nThis work is supported by the NSFC program (No. 62272338) and\nthe Natural Science Foundation of Inner Mongolia Autonomous\nCIKM \u201923, October 21\u201325, 2023, Birmingham, United Kingdom Li et al.\nREFERENCES\n[1]Jie Chen, Tengfei Ma, and Cao Xiao. 2018. Fastgcn: fast learning with graph\nconvolutional networks via importance sampling. arXiv preprint arXiv:1801.10247\n(2018).\n[2]Johannes Gasteiger, Stefan Wei\u00dfenberger, and Stephan G\u00fcnnemann. 2019. Diffu-\nsion improves graph learning. Advances in neural information processing systems\n32 (2019).\n[3] Aditya Grover and Jure Leskovec. 2016. node2vec: Scalable feature learning for\nnetworks. In Proceedings of the 22nd ACM SIGKDD international conference on\nKnowledge discovery and data mining . 855\u2013864.\n[4]Will Hamilton, Zhitao Ying, and Jure Leskovec. 2017. Inductive representation\nlearning on large graphs. Advances in neural information processing systems 30\n(2017).\n[5]William L Hamilton, Rex Ying, and Jure Leskovec. 2017. Representation learning\non graphs: Methods and applications. arXiv preprint arXiv:1709.05584 (2017).\n[6]Kaveh Hassani and Amir Hosein Khasahmadi. 2020. Contrastive multi-view rep-\nresentation learning on graphs. In International Conference on Machine Learning .\nPMLR, 4116\u20134126.\n[7]Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2015. Delving deep\ninto rectifiers: Surpassing human-level performance on imagenet classification.\nInProceedings of the IEEE international conference on computer vision . 1026\u20131034.\n[8]R Devon Hjelm, Alex Fedorov, Samuel Lavoie-Marchildon, Karan Grewal, Phil\nBachman, Adam Trischler, and Yoshua Bengio. 2018. Learning deep represen-\ntations by mutual information estimation and maximization. arXiv preprint\narXiv:1808.06670 (2018).\n[9]Glen Jeh and Jennifer Widom. 2003. Scaling personalized web search. In Proceed-\nings of the 12th international conference on World Wide Web . 271\u2013279.\n[10] Yizhu Jiao, Yun Xiong, Jiawei Zhang, Yao Zhang, Tianqi Zhang, and Yangyong\nZhu. 2020. Sub-graph contrast for scalable self-supervised graph representation\nlearning. In 2020 IEEE international conference on data mining (ICDM) . IEEE,\n222\u2013231.\n[11] Longlong Jing and Yingli Tian. 2020. Self-supervised visual feature learning\nwith deep neural networks: A survey. IEEE transactions on pattern analysis and\nmachine intelligence 43, 11 (2020), 4037\u20134058.\n[12] Nikhil Ketkar and Jojo Moolayil. 2021. Introduction to pytorch. In Deep learning\nwith python . Springer, 27\u201391.\n[13] Diederik P Kingma and Jimmy Ba. 2014. Adam: A method for stochastic opti-\nmization. arXiv preprint arXiv:1412.6980 (2014).\n[14] Thomas N Kipf and Max Welling. 2016. Semi-supervised classification with graph\nconvolutional networks. arXiv preprint arXiv:1609.02907 (2016).\n[15] Thomas N Kipf and Max Welling. 2016. Variational graph auto-encoders. arXiv\npreprint arXiv:1611.07308 (2016).\n[16] Risi Imre Kondor and John Lafferty. 2002. Diffusion kernels on graphs and other\ndiscrete structures. In Proceedings of the 19th international conference on machine\nlearning , Vol. 2002. 315\u2013322.\n[17] Yingming Li, Ming Yang, and Zhongfei Zhang. 2018. A survey of multi-view\nrepresentation learning. IEEE transactions on knowledge and data engineering 31,\n10 (2018), 1863\u20131883.\n[18] J MacQueen. 1967. Classification and analysis of multivariate observations. In\n5th Berkeley Symp. Math. Statist. Probability . 281\u2013297.\n[19] Costas Mavromatis and George Karypis. 2020. Graph infoclust: Leveraging\ncluster-level node information for unsupervised graph representation learning.\narXiv preprint arXiv:2009.06946 (2020).\n[20] Yujie Mo, Liang Peng, Jie Xu, Xiaoshuang Shi, and Xiaofeng Zhu. 2022. Simple\nunsupervised graph representation learning. AAAI.\n[21] Shirui Pan, Ruiqi Hu, Sai-fu Fung, Guodong Long, Jing Jiang, and Chengqi Zhang.\n2019. Learning graph embedding with adversarial training methods. IEEE\ntransactions on cybernetics 50, 6 (2019), 2475\u20132487.\n[22] Qiyao Peng, Hongtao Liu, Yinghui Wang, Hongyan Xu, Pengfei Jiao, Minglai\nShao, and Wenjun Wang. 2022. Towards a multi-view attentive matching for\npersonalized expert finding. In Proceedings of the ACM Web Conference 2022 .\n2131\u20132140.\n[23] Zhen Peng, Wenbing Huang, Minnan Luo, Qinghua Zheng, Yu Rong, Tingyang\nXu, and Junzhou Huang. 2020. Graph representation learning via graphical\nmutual information maximization. In Proceedings of The Web Conference 2020 .\n259\u2013270.\n[24] Bryan Perozzi, Rami Al-Rfou, and Steven Skiena. 2014. Deepwalk: Online learning\nof social representations. In Proceedings of the 20th ACM SIGKDD international\nconference on Knowledge discovery and data mining . 701\u2013710.\n[25] Jiezhong Qiu, Qibin Chen, Yuxiao Dong, Jing Zhang, Hongxia Yang, Ming Ding,\nKuansan Wang, and Jie Tang. 2020. Gcc: Graph contrastive coding for graph\nneural network pre-training. In Proceedings of the 26th ACM SIGKDD International\nConference on Knowledge Discovery & Data Mining . 1150\u20131160.\n[26] Satu Elisa Schaeffer. 2007. Graph clustering. Computer science review 1, 1 (2007),\n27\u201364.\n[27] Lei Tang and Huan Liu. 2011. Leveraging social media networks for classification.\nData Mining and Knowledge Discovery 23, 3 (2011), 447\u2013478.[28] Yonglong Tian, Dilip Krishnan, and Phillip Isola. 2020. Contrastive multiview\ncoding. In European conference on computer vision . Springer, 776\u2013794.\n[29] Petar Veli\u010dkovi\u0107, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro\nLio, and Yoshua Bengio. 2017. Graph attention networks. arXiv preprint\narXiv:1710.10903 (2017).\n[30] Petar Velickovic, William Fedus, William L Hamilton, Pietro Li\u00f2, Yoshua Bengio,\nand R Devon Hjelm. 2019. Deep Graph Infomax. ICLR (Poster) 2, 3 (2019), 4.\n[31] Bryan Wilder, Eric Ewing, Bistra Dilkina, and Milind Tambe. 2019. End to end\nlearning and optimization on graphs. Advances in Neural Information Processing\nSystems 32 (2019).\n[32] Felix Wu, Amauri Souza, Tianyi Zhang, Christopher Fifty, Tao Yu, and Kilian\nWeinberger. 2019. Simplifying graph convolutional networks. In International\nconference on machine learning . PMLR, 6861\u20136871.\n[33] Lirong Wu, Haitao Lin, Cheng Tan, Zhangyang Gao, and Stan Z Li. 2021. Self-\nsupervised learning on graphs: Contrastive, generative, or predictive. IEEE\nTransactions on Knowledge and Data Engineering (2021).\n[34] Minghao Xu, Hang Wang, Bingbing Ni, Hongyu Guo, and Jian Tang. 2021. Self-\nsupervised graph-level representation learning with local and global structure.\nInInternational Conference on Machine Learning . PMLR, 11548\u201311558.\n[35] Jiawei Zhang and Lin Meng. 2019. Gresnet: Graph residual network for reviving\ndeep gnns from suspended animation. arXiv preprint arXiv:1909.05729 (2019).\n[36] Jiawei Zhang, Haopeng Zhang, Congying Xia, and Li Sun. 2020. Graph-bert:\nOnly attention is needed for learning graph representations. arXiv preprint\narXiv:2001.05140 (2020).\n[37] Chen Zhao. 2021. Fairness-Aware Multi-Task and Meta Learning . Ph. D. Disserta-\ntion.\n[38] Chen Zhao and Feng Chen. 2019. Rank-based multi-task learning for fair re-\ngression. In 2019 IEEE International Conference on Data Mining (ICDM) . IEEE,\n916\u2013925.\n[39] Chen Zhao, Feng Chen, and Bhavani Thuraisingham. 2021. Fairness-aware online\nmeta-learning. In Proceedings of the 27th ACM SIGKDD Conference on Knowledge\nDiscovery & Data Mining . 2294\u20132304.\n[40] Jun Zhao, Xudong Liu, Qiben Yan, Bo Li, Minglai Shao, and Hao Peng. 2020.\nMulti-attributed heterogeneous graph convolutional network for bot detection.\nInformation Sciences 537 (2020), 380\u2013393.\n[41] Yanqiao Zhu, Yichen Xu, Feng Yu, Qiang Liu, Shu Wu, and Liang Wang. 2020.\nDeep graph contrastive representation learning. arXiv preprint arXiv:2006.04131\n(2020).\n[42] Thomas Zimmermann and Nachiappan Nagappan. 2008. Predicting defects using\nnetwork analysis on dependency graphs. In Proceedings of the 30th international\nconference on Software engineering",
        "subsection": []
    },
    {
        "missing": []
    }
]