[
    {
        "id": "",
        "section": "Abstract",
        "text": "The capacity of a neural network to absorb information is limited by its number of\nparameters. Conditional computation, where parts of the network are active on a\nper-example basis, has been proposed in theory as a way of dramatically increas-\ning model capacity without a proportional increase in computation. In practice,\nhowever, there are signi",
        "subsection": [
            {
                "id": "1.1",
                "section": "Conditional computation",
                "text": "Exploiting scale in both training data and model size has been central to the success of deep learn-\ning. When datasets are suf\ufb01ciently large, increasing the capacity (number of parameters) of neural\nnetworks can give much better prediction accuracy. This has been shown in domains such as text\n(Sutskever et al., 2014; Bahdanau et al., 2014; Jozefowicz et al., 2016; Wu et al., 2016), images\n(Krizhevsky et al., 2012; Le et al., 2012), and audio (Hinton et al., 2012; Amodei et al., 2015). For\ntypical deep learning models, where the entire model is activated for every example, this leads to\na roughly quadratic blow-up in training costs, as both the model size and the number of training\nexamples increase. Unfortunately, the advances in computing power and distributed computation\nfall short of meeting such demand.\nVarious forms of conditional computation have been proposed as a way to increase model capacity\nwithout a proportional increase in computational costs (Davis & Arel, 2013; Bengio et al., 2013;\nEigen et al., 2013; Ludovic Denoyer, 2014; Cho & Bengio, 2014; Bengio et al., 2015; Almahairi\net al., 2015). In these schemes, large parts of a network are active or inactive on a per-example\nbasis. The gating decisions may be binary or sparse and continuous, stochastic or deterministic.\nVarious forms of reinforcement learning and back-propagation are proposed for trarining the gating\ndecisions.\n\u0003Equally major contributors\nyUnder review as a conference paper at ICLR 2017\nFigure 1: A Mixture of Experts (MoE) layer embedded within a recurrent language model. In this\ncase, the sparse gating function selects two experts to perform computations. Their outputs are\nmodulated by the outputs of the gating network.\nWhile these ideas are promising in theory, no work to date has yet demonstrated massive improve-\nments in model capacity, training time, or model quality. We blame this on a combination of the\nfollowing challenges:\n\u000fModern computing devices, especially GPUs, are much faster at arithmetic than at branch-\ning. Most of the works above recognize this and propose turning on/off large chunks of the\nnetwork with each gating decision.\n\u000fLarge batch sizes are critical for performance, as they amortize the costs of parameter trans-\nfers and updates. Conditional computation reduces the batch sizes for the conditionally\nactive chunks of the network.\n\u000fNetwork bandwidth can be a bottleneck. A cluster of GPUs may have computational power\nthousands of times greater than the aggregate inter-device network bandwidth. To be com-\nputationally ef\ufb01cient, the relative computational versus network demands of an algorithm\nmust exceed this ratio. Embedding layers, which can be seen as a form of conditional com-\nputation, are handicapped by this very problem. Since the embeddings generally need to\nbe sent across the network, the number of (example, parameter) interactions is limited by\nnetwork bandwidth instead of computational capacity.\n\u000fDepending on the scheme, loss terms may be necessary to achieve the desired level of\nsparsity per-chunk and/or per example. Bengio et al. (2015) use three such terms. These\nissues can affect both model quality and load-balancing.\n\u000fModel capacity is most critical for very large data sets. The existing literature on condi-\ntional computation deals with relatively small image recognition data sets consisting of up\nto 600,000 images. It is hard to imagine that the labels of these images provide a suf\ufb01cient\nsignal to adequately train a model with millions, let alone billions of parameters.\nIn this work, we for the \ufb01rst time address all of the above challenges and \ufb01nally realize the promise\nof conditional computation. We obtain greater than 1000x improvements in model capacity with\nonly minor losses in computational ef\ufb01ciency and signi\ufb01cantly advance the state-of-the-art results\non public language modeling and translation data sets.",
                "subsection": []
            },
            {
                "id": "1.2",
                "section": "Ourapproach ",
                "text": "Our approach to conditional computation is to introduce a new type of general purpose neural net-\nwork component: a Sparsely-Gated Mixture-of-Experts Layer (MoE). The MoE consists of a num-\nber of experts, each a simple feed-forward neural network, and a trainable gating network which\nselects a sparse combination of the experts to process each input (see Figure 1). All parts of the\nUnder review as a conference paper at ICLR 2017\nWhile the introduced technique is generic, in this paper we focus on language modeling and machine\ntranslation tasks, which are known to bene\ufb01t from very large models. In particular, we apply a MoE\nconvolutionally between stacked LSTM layers (Hochreiter & Schmidhuber, 1997), as in Figure 1.\nThe MoE is called once for each position in the text, selecting a potentially different combination\nof experts at each position. The different experts tend to become highly specialized based on syntax\nand semantics (see Appendix E Table 9). On both language modeling and machine translation\nbenchmarks, we improve on best published results at a fraction of the computational cost.",
                "subsection": []
            },
            {
                "id": "1.3",
                "section": "Related work on mixtures of experts",
                "text": "Since its introduction more than two decades ago (Jacobs et al., 1991; Jordan & Jacobs, 1994),\nthe mixture-of-experts approach has been the subject of much research. Different types of expert\narchitectures hae been proposed such as SVMs (Collobert et al., 2002), Gaussian Processes (Tresp,\n2001; Theis & Bethge, 2015; Deisenroth & Ng, 2015), Dirichlet Processes (Shahbaba & Neal, 2009),\nand deep networks. Other work has focused on different expert con\ufb01gurations such as a hierarchical\nstructure (Yao et al., 2009), in\ufb01nite numbers of experts (Rasmussen & Ghahramani, 2002), and\nadding experts sequentially (Aljundi et al., 2016). Garmash & Monz (2016) suggest an ensemble\nmodel in the format of mixture of experts for machine translation. The gating network is trained on\na pre-trained ensemble NMT model.\nThe works above concern top-level mixtures of experts. The mixture of experts is the whole model.\nEigen et al. (2013) introduce the idea of using multiple MoEs with their own gating networks as\nparts of a deep model. It is intuitive that the latter approach is more powerful, since complex prob-\nlems may contain many sub-problems each requiring different experts. They also allude in their\nconclusion to the potential to introduce sparsity, turning MoEs into a vehicle for computational\ncomputation.\nOur work builds on this use of MoEs as a general purpose neural network component. While Eigen\net al. (2013) uses two stacked MoEs allowing for two sets of gating decisions, our convolutional\napplication of the MoE allows for different gating decisions at each position in the text. We also\nrealize sparse gating and demonstrate its use as a practical way to massively increase model capacity.",
                "subsection": []
            }
        ]
    },
    {
        "id": "1",
        "section": "Introduction and related work",
        "text": "",
        "subsection": [
            {
                "id": "2.1",
                "section": "Gating network",
                "text": "Softmax Gating: A simple choice of non-sparse gating function (Jordan & Jacobs, 1994) is to\nmultiply the input by a trainable weight matrix Wgand then apply the Softmax function.\nG\u001b(x) =Softmax (x\u0001Wg) (2)\nNoisy Top-K Gating: We add two components to the Softmax gating network: sparsity and noise.\nBefore taking the softmax function, we add tunable Gaussian noise, then keep only the top k values,\nsetting the rest to\u00001 (which causes the corresponding gate values to equal 0). The sparsity serves\nto save computation, as described above. While this form of sparsity creates some theoretically\nscary discontinuities in the output of gating function, we have not yet observed this to be a problem\nin practice. The noise term helps with load balancing, as will be discussed in Appendix A. The\namount of noise per component is controlled by a second trainable weight matrix Wnoise .\nG(x) =Softmax (KeepTopK (H(x);k)) (3)\nH(x)i= (x\u0001Wg)i+StandardNormal ()\u0001Softplus ((x\u0001Wnoise)i) (4)\nKeepTopK (v;k)i=\u001avi ifviis in the top kelements of v.\n\u00001 otherwise.(5)\nTraining the Gating Network We train the gating network by simple back-propagation, along\nwith the rest of the model. If we choose k >1, the gate values for the top k experts have nonzero\nderivatives with respect to the weights of the gating network. This type of occasionally-sensitive\nbehavior is described in (Bengio et al., 2013) with respect to noisy recti\ufb01ers. Gradients also back-\npropagate through the gating network to its inputs. Our method differs here from (Bengio et al.,\n2015) who use boolean gates and a REINFORCE-style approach to train the gating network.",
                "subsection": []
            }
        ]
    },
    {
        "id": "2",
        "section": "Thestructure of the mixture -of-experts layer",
        "text": "The Mixture-of-Experts (MoE) layer consists of a set of n\u201cexpert networks\" E1;\u0001\u0001\u0001;En, and a\n\u201cgating network\" Gwhose output is a sparse n-dimensional vector. Figure 1 shows an overview\nof the MoE module. The experts are themselves neural networks, each with their own parameters.\nAlthough in principle we only require that the experts accept the same sized inputs and produce the\nsame-sized outputs, in our initial investigations in this paper, we restrict ourselves to the case where\nthe models are feed-forward networks with identical architectures, but with separate parameters.\nLet us denote by G(x)andEi(x)the output of the gating network and the output of the i-th expert\nnetwork for a given input x. The output yof the MoE module can be written as follows:\ny=nX\ni=1G(x)iEi(x) (1)\nWe save computation based on the sparsity of the output of G(x). WhereverG(x)i= 0, we need not\ncomputeEi(x). In our experiments, we have up to thousands of experts, but only need to evaluate\na handful of them for every example. If the number of experts is very large, we can reduce the\nbranching factor by using a two-level hierarchical MoE. In a hierarchical MoE, a primary gating\nnetwork chooses a sparse weighted combination of \u201cexperts\", each of which is itself a secondary\nmixture-of-experts with its own gating network. In the following we focus on ordinary MoEs. We\nprovide more details on hierarchical MoEs in Appendix B.\nOur implementation is related to other models of conditional computation. A MoE whose experts are\nsimple weight matrices is similar to the parameterized weight matrix proposed in (Cho & Bengio,\n2014). A MoE whose experts have one hidden layer is similar to the block-wise dropout described\nUnder review as a conference paper at ICLR 2017",
        "subsection": [
            {
                "id": "3.1",
                "section": "Theshrinking batch problem",
                "text": "On modern CPUs and GPUs, large batch sizes are necessary for computational ef\ufb01ciency, so as\nto amortize the overhead of parameter loads and updates. If the gating network chooses kout of\nnexperts for each example, then for a batch of bexamples, each expert receives a much smaller\nbatch of approximatelykb\nn\u001cbexamples. This causes a naive MoE implementation to become\nvery inef\ufb01cient as the number of experts increases. The solution to this shrinking batch problem is\nto make the original batch size as large as possible. However, batch size tends to be limited by the\nmemory necessary to store activations between the forwards and backwards passes. We propose the\nfollowing techniques for increasing the batch size:\nMixing Data Parallelism and Model Parallelism: In a conventional distributed training setting,\nmultiple copies of the model on different devices asynchronously process distinct batches of data,\nand parameters are synchronized through a set of parameter servers. In our technique, these different\nbatches run synchronously so that they can be combined for the MoE layer. We distribute the\nstandard layers of the model and the gating network according to conventional data-parallel schemes,\nbut keep only one shared copy of each expert. Each expert in the MoE layer receives a combined\nbatch consisting of the relevant examples from all of the data-parallel input batches. The same set\nof devices function as data-parallel replicas (for the standard layers and the gating networks) and\nas model-parallel shards (each hosting a subset of the experts). If the model is distributed over d\ndevices, and each device processes a batch of size b, each expert receives a batch of approximately\nkbd\nnexamples. Thus, we achieve a factor of dimprovement in expert batch size.\nIn the case of a hierarchical MoE (Section B), the primary gating network employs data parallelism,\nUnder review as a conference paper at ICLR 2017\nThis technique allows us to increase the number of experts (and hence the number of parameters) by\nproportionally increasing the number of devices in the training cluster. The total batch size increases,\nkeeping the batch size per expert constant. The memory and bandwidth requirements per device also\nremain constant, as do the step times, as does the amount of time necessary to process a number of\ntraining examples equal to the number of parameters in the model. It is our goal to train a trillion-\nparameter model on a trillion-word corpus. We have not scaled our systems this far as of the writing\nof this paper, but it should be possible by adding more hardware.\nTaking Advantage of Convolutionality: In our language models, we apply the same MoE to each\ntime step of the previous layer. If we wait for the previous layer to \ufb01nish, we can apply the MoE\nto all the time steps together as one big batch. Doing so increases the size of the input batch to the\nMoE layer by a factor of the number of unrolled time steps.\nIncreasing Batch Size for a Recurrent MoE: We suspect that even more powerful models may\ninvolve applying a MoE recurrently. For example, the weight matrices of a LSTM or other RNN\ncould be replaced by a MoE. Sadly, such models break the convolutional trick from the last para-\ngraph, since the input to the MoE at one timestep depends on the output of the MoE at the previous\ntimestep. Gruslys et al. (2016) describe a technique for drastically reducing the number of stored\nactivations in an unrolled RNN, at the cost of recomputing forward activations. This would allow\nfor a large increase in batch size.",
                "subsection": []
            },
            {
                "id": "3.2",
                "section": "Network bandwidth",
                "text": "Another major performance concern in distributed computing is network bandwidth. Since the ex-\nperts are stationary (see above) and the number of gating parameters is small, most of the communi-\ncation involves sending the inputs and outputs of the experts across the network. To maintain com-\nputational ef\ufb01ciency, the ratio of an expert\u2019s computation to the size of its input and output must ex-\nceed the ratio of computational to network capacity of the computing device. For GPUs, this may be\nthousands to one. In our experiments, we use experts with one hidden layer containing thousands of\nRELU-activated units. Since the weight matrices in the expert have sizes input _size\u0002hidden _size\nandhidden _size\u0002output _size, the ratio of computation to input and output is equal to the size of\nthe hidden layer. Conveniently, we can increase computational ef\ufb01ciency simply by using a larger\nhidden layer, or more hidden layers.",
                "subsection": []
            }
        ]
    },
    {
        "id": "3",
        "section": "Addressing performance challenges",
        "text": "",
        "subsection": []
    },
    {
        "id": "4",
        "section": "Balancing expert utilization",
        "text": "We have observed that the gating network tends to converge to a state where it always produces\nlarge weights for the same few experts. This imbalance is self-reinforcing, as the favored experts\nare trained more rapidly and thus are selected even more by the gating network. Eigen et al. (2013)\ndescribe the same phenomenon, and use a hard constraint at the beginning of training to avoid this\nlocal minimum. Bengio et al. (2015) include a soft constraint on the batch-wise average of each\ngate.1\nWe take a soft constraint approach. We de\ufb01ne the importance of an expert relative to a batch of\ntraining examples to be the batchwise sum of the gate values for that expert. We de\ufb01ne an additional\nlossLimportance , which is added to the overall loss function for the model. This loss is equal to\nthe square of the coef\ufb01cient of variation of the set of importance values, multiplied by a hand-tuned\nscaling factor wimportance . This additional loss encourages all experts to have equal importance.\nImportance (X) =X\nx2XG(x) (6)\nLimportance (X) =wimportance\u0001CV(Importance (X))2(7)\n1Bengio et al. (2015) also include two additional losses. One controls per-example sparsity, which we do\nnot need since it is enforced by the \ufb01xed value of k. A third loss encourages diversity of gate values. In our\nexperiments, we \ufb01nd that the gate values naturally diversify as the experts specialize (in a virtuous cycle), and\nUnder review as a conference paper at ICLR 2017\nWhile this loss function can ensure equal importance, experts may still receive very different num-\nbers of examples. For example, one expert may receive a few examples with large weights, and\nanother may receive many examples with small weights. This can cause memory and performance\nproblems on distributed hardware. To solve this problem, we introduce a second loss function,\nLload , which ensures balanced loads. Appendix A contains the de\ufb01nition of this function, along\nwith experimental results.",
        "subsection": [
            {
                "id": "5.3",
                "section": "Machine translation ",
                "text": "Model Architecture: Our model was a modi\ufb01ed version of the GNMT model described in (Wu\net al., 2016). To reduce computation, we decreased the number of LSTM layers in the encoder\nand decoder from 9 and 8 to 3 and 2 respectively. We inserted MoE layers in both the encoder\n(between layers 2 and 3) and the decoder (between layers 1 and 2). Each MoE layer contained up\nto 2048 experts each with about two million parameters, adding a total of about 8 billion parameters\nto the models. Further details on model architecture, testing procedure and results can be found in\nAppendix E.\nDatasets: We benchmarked our method on the WMT\u201914 En !Fr and En!De corpora, whose\ntraining sets have 36M sentence pairs and 5M sentence pairs, respectively. The experimental proto-\ncols were also similar to those in (Wu et al., 2016): newstest2014 was used as the test set to compare\nagainst previous work (Luong et al., 2015a; Zhou et al., 2016; Wu et al., 2016), while the combina-\ntion of newstest2012 and newstest2013 was used as the development set. We also tested the same\nmodel on a Google\u2019s Production English to French data.\nTable 2: Results on WMT\u201914 En !Fr newstest2014 (bold values represent best results).\nModel Test Test ops/timenstep Total Training\nPerplexity BLEU #Parameters Time\nMoE with 2048 Experts 2.69 40.35 85M 8.7B 3 days/64 k40s\nMoE with 2048 Experts (longer training) 2.63 40.56 85M 8.7B 6 days/64 k40s\nGNMT (Wu et al., 2016) 2.79 39.22 214M 278M 6 days/96 k80s\nGNMT+RL (Wu et al., 2016) 2.96 39.92 214M 278M 6 days/96 k80s\nPBMT (Durrani et al., 2014) 37.0\nLSTM (6-layer) (Luong et al., 2015b) 31.5\nLSTM (6-layer+PosUnk) (Luong et al., 2015b) 33.1\nDeepAtt (Zhou et al., 2016) 37.7\nDeepAtt+PosUnk (Zhou et al., 2016) 39.2\nTable 3: Results on WMT\u201914 En !De newstest2014 (bold values represent best results).\nModel Test Test ops/timestep Total Training\nPerplexity BLEU #Parameters Time\nMoE with 2048 Experts 4.64 26.03 85M 8.7B 1 day/64 k40s\nGNMT (Wu et al., 2016) 5.25 24.91 214M 278M 1 day/96 k80s\nGNMT +RL (Wu et al., 2016) 8.08 24.66 214M 278M 1 day/96 k80s\nPBMT (Durrani et al., 2014) 20.7\nDeepAtt (Zhou et al., 2016) 20.6\nTable 4: Results on the Google Production En !Fr dataset (bold values represent best results).\nModel Eval Eval Test Test ops/timestep Total Training\nPerplexity BLEU Perplexity BLEU #Parameters Time\nMoE with 2048 Experts 2.60 37.27 2.69 36.57 85M 8.7B 1 day/64 k40s\nGNMT (Wu et al., 2016) 2.78 35.80 2.87 35.56 214M 278M 6 days/96 k80sUnder review as a conference paper at ICLR 2017\nResults: Tables 2, 3, and 4 show the results of our largest models, compared with published\nresults. Our approach achieved BLEU scores of 40.56 and 26.03 on the WMT\u201914 En !Fr and\nEn!De benchmarks. As our models did not use RL re\ufb01nement, these results constitute signi\ufb01cant\ngains of 1.34 and 1.12 BLEU score on top of the strong baselines in (Wu et al., 2016). The perplexity\nscores are also better.2On the Google Production dataset, our model achieved 1.01 higher test BLEU\nscore even after training for only one sixth of the time.",
                "subsection": []
            },
            {
                "id": "5.4",
                "section": "Multilingual machine translation",
                "text": "Dataset: (Johnson et al., 2016) train a single GNMT (Wu et al., 2016) model on a very large com-\nbined dataset of twelve language pairs. Results are somewhat worse than those for 12 separately\ntrained single-pair GNMT models. This is not surprising, given that the twelve models have 12\ntimes the capacity and twelve times the aggregate training of the one model. We repeat this ex-\nperiment with a single MoE-augmented model. See Appendix E for details on model architecture.\nWe train our model on the same dataset as (Johnson et al., 2016) and process the same number of\ntraining examples (about 3 billion sentence pairs). Our training time was shorter due to the lower\ncomputational budget of our model.\nResults: Results for the single-pair GNMT models, the multilingual GNMT model and the mul-\ntilingual MoE model are given in Table 5. The MoE model achieves 19% lower perplexity on the\ndev set than the multilingual GNMT model. On BLEU score, the MoE model signi\ufb01cantly beats\nthe multilingual GNMT model on 11 of the 12 language pairs (by as much as 5.84 points), and even\nbeats the monolingual GNMT models on 8 of 12 language pairs. The poor performance on English\n!Korean seems to be a result of severe overtraining, as for the rarer language pairs a small number\nof real examples were highly oversampled in the training corpus.\nTable 5: Multilingual Machine Translation (bold values represent best results).\nGNMT-Mono GNMT-Multi MoE-Multi MoE-Multi vs.\nGNMT-Multi\nParameters 278M / model 278M 8.7B\nops/timestep 212M 212M 102M\ntraining time, hardware various 21 days, 96 k20s 12 days, 64 k40s\nPerplexity (dev) 4.14 3.35 -19%\nFrench!English Test BLEU 36.47 34.40 37.46 +3.06\nGerman!English Test BLEU 31.77 31.17 34.80 +3.63\nJapanese!English Test BLEU 23.41 21.62 25.91 +4.29\nKorean!English Test BLEU 25.42 22.87 28.71 +5.84\nPortuguese!English Test BLEU 44.40 42.53 46.13 +3.60\nSpanish!English Test BLEU 38.00 36.04 39.39 +3.35\nEnglish!French Test BLEU 35.37 34.00 36.59 +2.59\nEnglish!German Test BLEU 26.43 23.15 24.53 +1.38\nEnglish!Japanese Test BLEU 23.66 21.10 22.78 +1.68\nEnglish!Korean Test BLEU 19.75 18.41 16.62 -1.79\nEnglish!Portuguese Test BLEU 38.40 37.35 37.90 +0.55\nEnglish!Spanish Test BLEU 34.50 34.25 36.21 +1.96",
                "subsection": []
            }
        ]
    },
    {
        "id": "5",
        "section": "Experiments",
        "text": "5.1 1 B ILLION WORD LANGUAGE MODELING BENCHMARK\nDataset: This dataset, introduced by (Chelba et al., 2013) consists of shuf\ufb02ed unique sentences\nfrom news articles, totaling approximately 829 million words, with a vocabulary of 793,471 words.\nPrevious State-of-the-Art: The best previously published results (Jozefowicz et al., 2016) use\nmodels consisting of one or more stacked Long Short-Term Memory (LSTM) layers (Hochreiter\n& Schmidhuber, 1997; Gers et al., 2000). The number of parameters in the LSTM layers of these\nmodels vary from 2 million to 151 million. Quality increases greatly with parameter count, as do\ncomputational costs. Results for these models form the top line of Figure 2-right.\nMoE Models: Our models consist of two stacked LSTM layers with a MoE layer between them\n(see Figure 1). We vary the sizes of the layers and the number of experts. For full details on model\narchitecture, training regimen, additional baselines and results, see Appendix C.\nLow Computation, Varied Capacity: To investigate the effects of adding capacity, we trained\na series of MoE models all with roughly equal computational costs: about 8 million multiply-and-\nadds per training example per timestep in the forwards pass, excluding the softmax layer. We call\nthis metric (ops/timestep). We trained models with \ufb02at MoEs containing 4, 32, and 256 experts, and\nmodels with hierarchical MoEs containing 256, 1024, and 4096 experts. Each expert had about 1\nmillion parameters. For all the MoE layers, 4 experts were active per input.\nThe results of these models are shown in Figure 2-left. The model with 4 always-active experts\nperformed (unsurprisingly) similarly to the computationally-matched baseline models, while the\nlargest of the models (4096 experts) achieved an impressive 24% lower perplexity on the test set.\nFigure 2: Model comparison on 1-Billion-Word Language-Modeling Benchmark. On the left, we\nplot test perplexity as a function of model capacity for models with similar computational budgets\nof approximately 8-million-ops-per-timestep. On the right, we plot test perplexity as a function of\ncomputational budget. The top line represents the LSTM models from (Jozefowicz et al., 2016).\nThe bottom line represents 4-billion parameter MoE models with different computational budgets.\nVaried Computation, High Capacity: In addition to the largest model from the previous section,\nwe trained two more MoE models with similarly high capacity (4 billion parameters), but higher\nUnder review as a conference paper at ICLR 2017\nTable 1: Summary of high-capacity MoE-augmented models with varying computational budgets,\nvs. best previously published results (Jozefowicz et al., 2016). Details in Appendix C.\nTest Test #Parameters ops/timestep Training TFLOPS\nPerplexity Perplexity excluding embedding Time /GPU\n10 epochs 100 epochs and softmax layers 10 epochs\nBest Published Results 34.7 30.6 151 million 151 million 59 hours, 32 k40s 1.09\nLow-Budget MoE Model 34.1 4303 million 8.9 million 15 hours, 16 k40s 0.74\nMedium-Budget MoE Model 31.3 4313 million 33.8 million 17 hours, 32 k40s 1.22\nHigh-Budget MoE Model 28.0 4371 million 142.7 million 47 hours, 32 k40s 1.56\ncan be found in Appendix C.2. Results of these three models form the bottom line of Figure 2-right.\nTable 1 compares the results of these models to the best previously-published result on this dataset .\nEven the fastest of these models beats the best published result (when controlling for the number of\ntraining epochs), despite requiring only 6% of the computation.\nComputational Ef\ufb01ciency: We trained our models using TensorFlow (Abadi et al., 2016) on clus-\nters containing 16-32 Tesla K40 GPUs. For each of our models, we determine computational ef\ufb01-\nciency in TFLOPS/GPU by dividing the number of \ufb02oating point operations required to process\none training batch by the observed step time and the number of GPUs in the cluster. The operation\ncounts used here are higher than the ones we report in our ops/timestep numbers in that we include\nthe backwards pass, we include the importance-sampling-based training of the softmax layer, and\nwe count a multiply-and-add as two separate operations. For all of our MoE models, the \ufb02oating\npoint operations involved in the experts represent between 37% and 46% of the total.\nFor our baseline models wtih no MoE, observed computational ef\ufb01ciency ranged from 1.07-1.29\nTFLOPS/GPU. For our low-computation MoE models, computation ef\ufb01ciency ranged from 0.74-",
        "subsection": [
            {
                "id": "0.90",
                "section": "Tflops",
                "text": "parallelism. Our highest-computation MoE model was more ef\ufb01cient at 1.56 TFLOPS/GPU, likely\ndue to the larger matrices. These numbers represent a signi\ufb01cant fraction of the theoretical maximum\nof 4.29 TFLOPS/GPU claimed by NVIDIA. Detailed results are in Appendix C, Table 7.\n5.2 100 B ILLION WORD GOOGLE NEWS CORPUS\nFigure 3: Language modeling on a 100 billion word corpus. Models have similar computational\nbudgets (8 million ops/timestep).\nOn the 1-billion-word corpus, adding additional capacity seems to produce diminishing returns as\nthe number of parameters in the MoE layer exceeds 1 billion, as can be seen in Figure 2-left. We\nhypothesized that for a larger training set, even higher capacities would produce signi\ufb01cant quality\nimprovements.\nWe constructed a similar training set consisting of shuf\ufb02ed unique sentences from Google\u2019s internal\nnews corpus, totalling roughly 100 billion words. Similarly to the previous section, we tested a\nseries of models with similar computational costs of about 8 million ops/timestep. In addition to a\nUnder review as a conference paper at ICLR 2017\n4096, 16384, 65536, and 131072 experts. This corresponds to up to 137 billion parameters in the\nMoE layer. Details on architecture, training, and results are given in Appendix D.\nResults: Figure 3 shows test perplexity as a function of capacity after training on 10 billion words\n(top line) and 100 billion words (bottom line). When training over the full 100 billion words, test\nperplexity improves signi\ufb01cantly up to 65536 experts (68 billion parameters), dropping 39% lower\nthan the computationally matched baseline, but degrades at 131072 experts, possibly a result of too\nmuch sparsity. The widening gap between the two lines demonstrates (unsurprisingly) that increased\nmodel capacity helps more on larger training sets.\nEven at 65536 experts (99.994% layer sparsity), computational ef\ufb01ciency for the model stays at a\nrespectable 0.72 TFLOPS/GPU.",
                "subsection": []
            }
        ]
    },
    {
        "id": "6",
        "section": "Conclusion",
        "text": "This work is the \ufb01rst to demonstrate major wins from conditional computation in deep networks.\nWe carefully identi\ufb01ed the design considerations and challenges of conditional computing and ad-\ndressed them with a combination of algorithmic and engineering solutions. While we focused on\ntext, conditional computation may help in other domains as well, provided suf\ufb01ciently large train-\ning sets. We look forward to seeing many novel implementations and applications of conditional\ncomputation in the years to come.\nACKNOWLEDGMENTS\nWe would like to thank all of the members of the Google Brain and Google Translate teams who\nhelped us with this project, in particular Zhifeng Chen, Yonghui Wu, and Melvin Johnson. Thanks\nalso to our anonymous ICLR reviewers for the helpful suggestions on making this paper better.\n2Under review as a conference paper at ICLR 2017\nREFERENCES\nMart\u00edn Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Gre-\ngory S. Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Ian J. Good-\nfellow, Andrew Harp, Geoffrey Irving, Michael Isard, Yangqing Jia, Rafal J\u00f3zefowicz, Lukasz\nKaiser, Manjunath Kudlur, Josh Levenberg, Dan Man\u00e9, Rajat Monga, Sherry Moore, Derek Gor-\ndon Murray, Chris Olah, Mike Schuster, Jonathon Shlens, Benoit Steiner, Ilya Sutskever, Kunal\nTalwar, Paul A. Tucker, Vincent Vanhoucke, Vijay Vasudevan, Fernanda B. Vi\u00e9gas, Oriol Vinyals,\nPete Warden, Martin Wattenberg, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng. Tensor\ufb02ow:\nLarge-scale machine learning on heterogeneous distributed systems. CoRR , abs/1603.04467,\n2016. URL http://arxiv.org/abs/1603.04467 .\nRahaf Aljundi, Punarjay Chakravarty, and Tinne Tuytelaars. Expert gate: Lifelong learning with a\nnetwork of experts. CoRR , abs/1611.06194, 2016. URL http://arxiv.org/abs/1611.\n06194 .\nA. Almahairi, N. Ballas, T. Cooijmans, Y . Zheng, H. Larochelle, and A. Courville. Dynamic Capac-\nity Networks. ArXiv e-prints , November 2015.\nDario Amodei, Rishita Anubhai, Eric Battenberg, Carl Case, Jared Casper, Bryan Catanzaro, Jing-\ndong Chen, Mike Chrzanowski, Adam Coates, Greg Diamos, Erich Elsen, Jesse Engel, Linxi\nFan, Christopher Fougner, Tony Han, Awni Y . Hannun, Billy Jun, Patrick LeGresley, Libby Lin,\nSharan Narang, Andrew Y . Ng, Sherjil Ozair, Ryan Prenger, Jonathan Raiman, Sanjeev Satheesh,\nDavid Seetapun, Shubho Sengupta, Yi Wang, Zhiqian Wang, Chong Wang, Bo Xiao, Dani Yo-\ngatama, Jun Zhan, and Zhenyao Zhu. Deep speech 2: End-to-end speech recognition in english\nand mandarin. arXiv preprint arXiv:1512.02595 , 2015.\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\nlearning to align and translate. arXiv preprint arXiv:1409.0473 , 2014.\nEmmanuel Bengio, Pierre-Luc Bacon, Joelle Pineau, and Doina Precup. Conditional computation\nin neural networks for faster models. arXiv preprint arXiv:1511.06297 , 2015.\nYoshua Bengio, Nicholas L\u00e9onard, and Aaron Courville. Estimating or propagating gradients\nthrough stochastic neurons for conditional computation. arXiv preprint arXiv:1308.3432 , 2013.\nCiprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge, Thorsten Brants, Phillipp Koehn, and Tony\nRobinson. One billion word benchmark for measuring progress in statistical language modeling.\narXiv preprint arXiv:1312.3005 , 2013.\nK. Cho and Y . Bengio. Exponentially Increasing the Capacity-to-Computation Ratio for Conditional\nComputation in Deep Learning. ArXiv e-prints , June 2014.\nRonan Collobert, Samy Bengio, and Yoshua Bengio. A parallel mixture of SVMs for very large\nscale problems. Neural Computing , 2002.\nAndrew Davis and Itamar Arel. Low-rank approximations for conditional feedforward computation\nin deep neural networks. arXiv preprint arXiv:1312.4461 , 2013.\nMarc Peter Deisenroth and Jun Wei Ng. Distributed Gaussian processes. In ICML , 2015.\nJohn Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and\nstochastic optimization, 2010.\nNadir Durrani, Barry Haddow, Philipp Koehn, and Kenneth Hea\ufb01eld. Edinburgh\u2019s phrase-based\nmachine translation systems for wmt-14. In Proceedings of the Ninth Workshop on Statistical\nMachine Translation , 2014.\nDavid Eigen, Marc\u2019Aurelio Ranzato, and Ilya Sutskever. Learning factored representations in a deep\nmixture of experts. arXiv preprint arXiv:1312.4314 , 2013.\nEkaterina Garmash and Christof Monz. Ensemble learning for multi-source neural machine transla-\ntion. InUnder review as a conference paper at ICLR 2017\nFelix A. Gers, J\u00fcrgen A. Schmidhuber, and Fred A. Cummins. Learning to forget: Continual pre-\ndiction with lstm. Neural Computation , 2000.\nAudrunas Gruslys, R\u00e9mi Munos, Ivo Danihelka, Marc Lanctot, and Alex Graves. Memory-ef\ufb01cient\nbackpropagation through time. CoRR , abs/1606.03401, 2016. URL http://arxiv.org/\nabs/1606.03401 .\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-\nnition. IEEE Conference on Computer Vision and Pattern Recognition , 2015.\nGeoffrey Hinton, Li Deng, Dong Yu, George E. Dahl, Abdel-rahman Mohamed, Navdeep Jaitly,\nAndrew Senior, Vincent Vanhoucke, Patrick Nguyen, Tara N. Sainath, et al. Deep neural networks\nfor acoustic modeling in speech recognition: The shared views of four research groups. IEEE\nSignal Processing Magazine , 2012.\nSepp Hochreiter and J\u00fcrgen Schmidhuber. Long short-term memory. Neural Computation , 1997.\nSergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by\nreducing internal covariate shift. arXiv preprint arXiv:1502.03167 , 2015.\nRobert A. Jacobs, Michael I. Jordan, Steven J. Nowlan, and Geoffrey E. Hinton. Adaptive mixtures\nof local experts. Neural Computing , 1991.\nMelvin Johnson, Mike Schuster, Quoc V . Le, Maxim Krikun, Yonghui Wu, Zhifeng Chen, Nikhil\nThorat, Fernanda B. Vi\u00e9gas, Martin Wattenberg, Greg Corrado, Macduff Hughes, and Jeffrey\nDean. Google\u2019s multilingual neural machine translation system: Enabling zero-shot translation.\nCoRR , abs/1611.04558, 2016. URL http://arxiv.org/abs/1611.04558 .\nMichael I. Jordan and Robert A. Jacobs. Hierarchical mixtures of experts and the EM algorithm.\nNeural Computing , 1994.\nRafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring the\nlimits of language modeling. arXiv preprint arXiv:1602.02410 , 2016.\nDiederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR , 2015.\nReinhard Kneser and Hermann. Ney. Improved backingoff for m-gram language modeling., 1995.\nAlex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. Imagenet classi\ufb01cation with deep convo-\nlutional neural networks. In NIPS , 2012.\nQuoc V . Le, Marc\u2019Aurelio Ranzato, Rajat Monga, Matthieu Devin, Kai Chen, Greg S. Corrado,\nJeffrey Dean, and Andrew Y . Ng. Building high-level features using large scale unsupervised\nlearning. In ICML , 2012.\nPatrick Gallinari Ludovic Denoyer. Deep sequential neural network. arXiv preprint\narXiv:1410.0510 , 2014.\nMinh-Thang Luong, Hieu Pham, and Christopher D. Manning. Effective approaches to attention-\nbased neural machine translation. EMNLP , 2015a.\nMinh-Thang Luong, Ilya Sutskever, Quoc V . Le, Oriol Vinyals, and Wojciech Zaremba. Addressing\nthe rare word problem in neural machine translation. ACL, 2015b.\nCarl Edward Rasmussen and Zoubin Ghahramani. In\ufb01nite mixtures of Gaussian process experts.\nNIPS , 2002.\nHasim Sak, Andrew W Senior, and Fran\u00e7oise Beaufays. Long short-term memory recurrent neural\nnetwork architectures for large scale acoustic modeling. In INTERSPEECH , pp. 338\u2013342, 2014.\nMike Schuster and Kaisuke Nakajima. Japanese and Korean voice search. ICASSP , 2012.\nBabak Shahbaba and Radford Neal. Nonlinear models using dirichlet process mixtures. JMLR ,\nUnder review as a conference paper at ICLR 2017\nIlya Sutskever, Oriol Vinyals, and Quoc V . Le. Sequence to sequence learning with neural networks.\nInNIPS , 2014.\nLucas Theis and Matthias Bethge. Generative image modeling using spatial LSTMs. In NIPS , 2015.\nV olker Tresp. Mixtures of Gaussian Processes. In NIPS , 2001.\nYonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V . Le, Mohammad Norouzi, Wolfgang Macherey,\nMaxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, Jeff Klingner, Apurva Shah, Melvin John-\nson, Xiaobing Liu, \u0141ukasz Kaiser, Stephan Gouws, Yoshikiyo Kato, Taku Kudo, Hideto Kazawa,\nKeith Stevens, George Kurian, Nishant Patil, Wei Wang, Cliff Young, Jason Smith, Jason Riesa,\nAlex Rudnick, Oriol Vinyals, Greg Corrado, Macduff Hughes, and Jeffrey Dean. Google\u2019s neural\nmachine translation system: Bridging the gap between human and machine translation. arXiv\npreprint arXiv:1609.08144 , 2016.\nBangpeng Yao, Dirk Walther, Diane Beck, and Li Fei-fei. Hierarchical mixture of classi\ufb01cation\nexperts uncovers interactions between brain regions. In NIPS . 2009.\nWojciech Zaremba, Ilya Sutskever, and Oriol Vinyals. Recurrent neural network regularization.\narXiv preprint arXiv:1409.2329 , 2014.\nJie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with fast-forward\nconnections for neural machine translation.Under review as a conference paper at ICLR 2017\nAPPENDICES\nA L OAD-BALANCING LOSS\nAs discussed in section 4, for load-balancing purposes, we want to de\ufb01ne an additional loss function\nto encourage experts to receive roughly equal numbers of training examples. Unfortunately, the\nnumber of examples received by an expert is a discrete quantity, so it can not be used in back-\npropagation. Instead, we de\ufb01ne a smooth estimator Load (X)of the number of examples assigned to\neach expert for a batch Xof inputs. The smoothness allows us to back-propagate gradients through\nthe estimator. This is the purpose of the noise term in the gating function. We de\ufb01ne P(x;i)as the\nprobability that G(x)iis nonzero, given a new random choice of noise on element i, but keeping\nthe already-sampled choices of noise on the other elements. To compute P(x;i), we note that the\nG(x)iis nonzero if and only if H(x)iis greater than the kth-greatest element of H(x)excluding\nitself. The probability works out to be:\nP(x;i) =Pr\u0010\n(x\u0001Wg)i+StandardNormal ()\u0001Softplus ((x\u0001Wnoise)i)\n>kth _excluding (H(x);k;i)\u0011 (8)\nWherekth_excluding (v;k;i )means the kth highest component of v, excluding component i. Sim-\nplifying, we get:\nP(x;i) = \b\u0010(x\u0001Wg)i\u0000kth_excluding (H(x);k;i)\nSoftplus ((x\u0001Wnoise)i)\u0011\n(9)\nWhere \bis the CDF of the standard normal distribution.\nLoad (X)i=X\nx2XP(x;i) (10)\nWe can now de\ufb01ne the load loss to be the square of the coef\ufb01cient of variation of the load vector,\nmultiplied by a hand-tuned scaling factor wload.\nLload(X) =wload\u0001CV(Load (X))2(11)\nInitial Load Imbalance: To avoid out-of-memory errors, we need to initialize the network in a\nstate of approximately equal expert load (since the soft constraints need some time to work). To\naccomplish this, we initialize the matrices WgandWnoise to all zeros, which yields no signal and\nsome noise.\nExperiments: We trained a set of models with identical architecture (the MoE-256 model de-\nscribed in Appendix C), using different values of wimportance andwload. We trained each model for\n10 epochs, then measured perplexity on the test set. We also measured the coef\ufb01cients of variation\ninImportance andLoad , as well as ratio of the load on the most overloaded expert to the average\nload. This last value is signi\ufb01cant for load balancing purposes on distributed hardware. All of these\nmetrics were averaged over several training batches.\nTable 6: Experiments with different combinations of losses.\nwimportancewload Test Perplexity CV(Importance (X))CV(Load (X))max (Load (X))\nmean (Load (X))\n0.0 0.0 39.8 3.04 3.01 17.80\n0.2 0.0 35.6 0.06 0.17 1.47\n0.0 0.2 35.7 0.22 0.04 1.15\n0.1 0.1 35.6 0.06 0.05 1.14\n0.01 0.01 35.7 0.48 0.11 1.37\n1.0 1.0 35.7 0.03 0.02 1.07Under review as a conference paper at ICLR 2017\nResults: Results are reported in Table 6. All the combinations containing at least one the two\nlosses led to very similar model quality, where having no loss was much worse. Models with higher\nvalues ofwloadhad lower loads on the most overloaded expert.\nB H IERACHICAL MIXTURE OF EXPERTS\nIf the number of experts is very large, we can reduce the branching factor by using a two-level\nhierarchical MoE. In a hierarchical MoE, a primary gating network chooses a sparse weighted com-\nbination of \u201cexperts\", each of which is itself a secondary mixture-of-experts with its own gating\nnetwork.3If the hierarchical MoE consists of agroups ofbexperts each, we denote the primary gat-\ning network by Gprimary , the secondary gating networks by (G1;G2::Ga), and the expert networks\nby(E0;0;E0;1::Ea;b). The output of the MoE is given by:\nyH=aX\ni=1bX\nj=1Gprimary (x)i\u0001Gi(x)j\u0001Ei;j(x) (12)\nOur metrics of expert utilization change to the following:\nImportance H(X)i;j=X\nx2XGprimary (x)i\u0001Gi(x)j (13)\nLoadH(X)i;j=Loadprimary (X)i\u0001Loadi(X(i))j\njX(i)j(14)\nLoadprimary andLoadideonte theLoad functions for the primary gating network and ithsec-\nondary gating network respectively. X(i)denotes the subset of Xfor whichGprimary (x)i>0.\nIt would seem simpler to let LoadH(X)i;j=Loadi(Xi)j, but this would not have a gradient with\nrespect to the primary gating network, so we use the formulation above.\nC 1 B ILLION WORD LANGUAGE MODELING BENCHMARK - EXPERIMENTAL DETAILS\nC.1 8-M ILLION -OPERATIONS -PER-TIMESTEP MODELS\nModel Architecture: Our model consists of \ufb01ve layers: a word embedding layer, a recurrent\nLong Short-Term Memory (LSTM) layer (Hochreiter & Schmidhuber, 1997; Gers et al., 2000), a\nMoE layer, a second LSTM layer, and a softmax layer. The dimensionality of the embedding layer,\nthe number of units in each LSTM layer, and the input and output dimensionality of the MoE layer\nare all equal to 512. For every layer other than the softmax, we apply drouput (Zaremba et al.,\n2014) to the layer output, dropping each activation with probability DropProb , otherwise dividing\nby(1\u0000DropProb ). After dropout, the output of the previous layer is added to the layer output.\nThis residual connection encourages gradient \ufb02ow (He et al., 2015).\nMoE Layer Architecture: Each expert in the MoE layer is a feed forward network with one\nReLU-activated hidden layer of size 1024 and an output layer of size 512. Thus, each expert contains\n[512\u00031024] + [1024\u0003512] = 1Mparameters. The output of the MoE layer is passed through a\nsigmoid function before dropout. We varied the number of experts between models, using ordinary\nMoE layers with 4, 32 and 256 experts and hierarchical MoE layers with 256, 1024 and 4096 experts.\nWe call the resulting models MoE-4, MoE-32, MoE-256, MoE-256-h, MoE-1024-h and MoE-4096-\nh. For the hierarchical MoE layers, the \ufb01rst level branching factor was 16, corresponding to the\nnumber of GPUs in our cluster. We use Noisy-Top-K Gating (see Section 2.1) with k= 4 for the\nordinary MoE layers and k= 2at each level of the hierarchical MoE layers. Thus, each example is\nprocessed by exactly 4 experts for a total of 4M ops/timestep. The two LSTM layers contribute 2M\nops/timestep each for the desired total of 8M.\n3Under review as a conference paper at ICLR 2017\nComputationally-Matched Baselines: The MoE-4 model does not employ sparsity, since all 4\nexperts are always used. In addition, we trained four more computationally-matched baseline models\nwith no sparsity:\n\u000fMoE-1-Wide: The MoE layer consists of a single \"expert\" containing one ReLU-activated\nhidden layer of size 4096.\n\u000fMoE-1-Deep: The MoE layer consists of a single \"expert\" containing four ReLU-activated\nhidden layers, each with size 1024 .\n\u000f4xLSTM-512: We replace the MoE layer with two additional 512-unit LSTM layers.\n\u000fLSTM-2048-512: The model contains one 2048-unit LSTM layer (and no MoE). The out-\nput of the LSTM is projected down to 512 dimensions (Sak et al., 2014). The next timestep\nof the LSTM receives the projected output. This is identical to one of the models published\nin (Jozefowicz et al., 2016). We re-ran it to account for differences in training regimen, and\nobtained results very similar to the published ones.\nTraining: The models were trained on a cluster of 16 K40 GPUs using the synchronous method\ndescribed in Section 3. Each batch consisted of a set of sentences totaling roughly 300,000 words. In\nthe interest of time, we limited training to 10 epochs, (27,000 steps). Training took 12-16 hours for\nall models, except for MoE-4, which took 18 hours (since all the expert computation was performed\non only 4 of 16 GPUs). We used the Adam optimizer (Kingma & Ba, 2015). The base learning\nrate was increased linearly for the \ufb01rst 1000 training steps, and decreased after that so as to be\nproportional to the inverse square root of the step number. The Softmax output layer was trained\nef\ufb01ciently using importance sampling similarly to the models in (Jozefowicz et al., 2016). For each\nmodel, we performed a hyper-parmeter search to \ufb01nd the best dropout probability, in increments of\n0.1.\nTo ensure balanced expert utilization we set wimportance = 0:1andwload= 0:1, as described in\nSection 4 and Appendix A.\nResults: We evaluate our model using perplexity on the holdout dataset, used by (Chelba et al.,\n2013; Jozefowicz et al., 2016). We follow the standard procedure and sum over all the words in-\ncluding the end of sentence symbol. Results are reported in Table 7. For each model, we report\nthe test perplexity, the computational budget, the parameter counts, the value of DropProb , and the\ncomputational ef\ufb01ciency.\nTable 7: Model comparison on 1 Billion Word Language Modeling Benchmark. Models marked\nwith * are from (Jozefowicz et al., 2016).\nModel Test Test ops/timestep #Params excluding TotalDrop -TFLOPS\nPerplexity Perplexity (millions) embed. & softmax #ParamsProb per GPU\n10 epochs (\ufb01nal) (millions) (billions) (observed)\nKneser-Ney 5-gram* 67.6 0.00001 1.8\nLSTM-512-512* 54.1 2.4 2.4 0.8 0.1\nLSTM-1024-512* 48.2 4.7 4.7 0.8 0.1\nLSTM-2048-512* 45.0 43.7 9.4 9.4 0.8 0.1 0.61\nLSTM-2048-512 44.7 9.4 9.4 0.8 0.1 1.21\n4xLSTM-512 46.0 8.4 8.4 0.8 0.1 1.07\nMoE-1-Wide 46.1 8.4 8.4 0.8 0.1 1.29\nMoE-1-Deep 45.7 8.4 8.4 0.8 0.1 1.29\nMoE-4 45.0 8.4 8.4 0.8 0.1 0.52\nMoE-32 39.7 8.4 37.8 0.9 0.1 0.87\nMoE-256 35.7 8.6 272.9 1.1 0.1 0.81\nMoE-256-h 36.0 8.4 272.9 1.1 0.1 0.89\nMoE-1024-h 34.6 8.5 1079.0 1.9 0.2 0.90\nMoE-4096-h 34.1 8.9 4303.4 5.1 0.2 0.74\n2xLSTM-8192-1024* 34.7 30.6 151.0 151.0 1.8 0.25 1.09\nMoE-34M 31.3 33.8 4313.9 6.0 0.3 1.22\nMoE-143M 28.0 142.7 4371.1 6.0 0.4 1.56Under review as a conference paper at ICLR 2017\nC.2 M ORE EXPENSIVE MODELS\nWe ran two additional models (MoE-34M and MoE-143M) to investigate the effects of adding more\ncomputation in the presence of a large MoE layer. These models have computation budgets of 34M\nand 143M ops/timestep. Similar to the models above, these models use a MoE layer between two\nLSTM layers. The dimensionality of the embedding layer, and the input and output dimensionality\nof the MoE layer are set to 1024 instead of 512. For MoE-34M, the LSTM layers have 1024 units.\nFor MoE-143M, the LSTM layers have 4096 units and an output projection of size 1024 (Sak et al.,\n2014). MoE-34M uses a hierarchical MoE layer with 1024 experts, each with a hidden layer of size\n2048. MoE-143M uses a hierarchical MoE layer with 256 experts, each with a hidden layer of size\n8192. Both models have 4B parameters in the MoE layers. We searched for the best DropProb for\neach model, and trained each model for 10 epochs.\nThe two models achieved test perplexity of 31:3and28:0respectively, showing that even in the\npresence of a large MoE, more computation is still useful. Results are reported at the bottom of\nTable 7. The larger of the two models has a similar computational budget to the best published\nmodel from the literature, and training times are similar. Comparing after 10 epochs, our model has\na lower test perplexity by 18%.\nD 100 B ILLION WORD GOOGLE NEWS CORPUS - EXPERIMENTAL DETAILS\nModel Architecture: The models are similar in structure to the 8-million-operations-per-timestep\nmodels described in the previous section. We vary the number of experts between models, using\nan ordinary MoE layer with 32 experts and hierarchical MoE layers with 256, 1024, 4096, 16384,\n65536 and 131072 experts. For the hierarchical MoE layers, the \ufb01rst level branching factors are 32,\n32, 64, 128, 256 and 256, respectively.\nTraining: Models are trained on a cluster of 32 Tesla K40 GPUs, except for the last two models,\nwhich are trained on clusters of 64 and 128 GPUs so as to have enough memory for all the param-\neters. For all models, training batch sizes are approximately 2.5 million words. Models are trained\nonce-through over about 100 billion words.\nWe implement several memory optimizations in order to \ufb01t up to 1 billion parameters per GPU.\nFirst, we do not store the activations of the hidden layers of the experts, but instead recompute them\non the backwards pass. Secondly, we modify the optimizer on the expert parameters to require less\nauxiliary storage:\nThe Adam optimizer (Kingma & Ba, 2015) keeps \ufb01rst and second moment estimates of the per-\nparameter gradients. This triples the required memory. To avoid keeping a \ufb01rst-moment estimator,\nwe set\f1= 0. To reduce the size of the second moment estimator, we replace it with a factored\napproximation. For a matrix of parameters, instead of maintaining a full matrix of second-moment\nestimators, we maintain vectors of row-wise and column-wise averages of that matrix. At each step,\nthe matrix of estimators is taken to be the outer product of those two vectors divided by the mean of\neither one. This technique could similarly be applied to Adagrad (Duchi et al., 2010).\nTable 8: Model comparison on 100 Billion Word Google News Dataset\nModel Test Test ops/timestep #Params excluding Total TFLOPS\nPerplexity Perplexity (millions) embed. & softmax #Params per GPU\n.1 epochs 1 epoch (millions) (billions) (observed)\nKneser-Ney 5-gram 67.1 45.3 0.00001 76.0\n4xLSTM-512 54.5 47.0 8.4 8.4 0.1 1.23\nMoE-32 48.5 40.4 8.4 37.8 0.1 0.83\nMoE-256-h 42.8 35.3 8.4 272.9 0.4 1.11\nMoE-1024-h 40.3 32.7 8.5 1079.0 1.2 1.14\nMoE-4096-h 38.9 30.9 8.6 4303.4 4.4 1.07\nMoE-16384-h 38.2 29.7 8.8 17201.0 17.3 0.96\nMoE-65536-h 38.2 28.9 9.2 68791.0 68.9 0.72\nMoE-131072-h 39.8 29.2 9.7 137577.6 137.7 0.30\nResults: We evaluate our model using perplexity on a holdout dataset. Results are reported in\nUnder review as a conference paper at ICLR 2017\nmodel than for the baseline model. It is notable that the measured computational ef\ufb01ciency of\nthe largest model (0.30 TFLOPS/GPU) is very low compared to the other models. This is likely\na result of the fact that, for purposes of comparison to the other models, we did not increase the\ntraining batch size proportionally to the number of GPUs. For comparison, we include results for\na computationally matched baseline model consisting of 4 LSTMs, and for an unpruned 5-gram\nmodel with Kneser-Ney smoothing (Kneser & Ney, 1995).4\nE M ACHINE TRANSLATION - EXPERIMENTAL DETAILS\nModel Architecture for Single Language Pair MoE Models: Our model is a modi\ufb01ed version\nof the GNMT model described in (Wu et al., 2016). To reduce computation, we decrease the number\nof LSTM layers in the encoder and decoder from 9 and 8 to 3 and 2 respectively. We insert MoE\nlayers in both the encoder (between layers 2 and 3) and the decoder (between layers 1 and 2). We use\nan attention mechanism between the encoder and decoder, with the \ufb01rst decoder LSTM receiving\noutput from and providing input for the attention5. All of the layers in our model have input and\noutput dimensionality of 512. Our LSTM layers have 2048 hidden units, with a 512-dimensional\noutput projection. We add residual connections around all LSTM and MoE layers to encourage\ngradient \ufb02ow (He et al., 2015). Similar to GNMT, to effectively deal with rare words, we used sub-\nword units (also known as \u201cwordpieces\") (Schuster & Nakajima, 2012) for inputs and outputs in our\nsystem.\nWe use a shared source and target vocabulary of 32K wordpieces. We also used the same beam\nsearch technique as proposed in (Wu et al., 2016).\nWe train models with different numbers of experts in the MoE layers. In addition to a baseline\nmodel with no MoE layers, we train models with \ufb02at MoE layers containing 32 experts, and models\nwith hierarchical MoE layers containing 512 and 2048 experts. The \ufb02at MoE layers use k= 4and\nthe hierarchical MoE models use k= 2 at each level of the gating network. Thus, each input is\nprocessed by exactly 4 experts in each MoE layer. Each expert in the MoE layer is a feed forward\nnetwork with one hidden layer of size 2048 and ReLU activation. Thus, each expert contains [512\u0003\n2048] + [2048\u0003512] = 2Mparameters. The output of the MoE layer is passed through a sigmoid\nfunction. We use the strictly-balanced gating function described in Appendix F.\nModel Architecture for Multilingual MoE Model: We used the same model architecture as\nfor the single-language-pair models, with the following exceptions: We used noisy-top-k gating as\ndescribed in Section 2.1, not the scheme from Appendix F. The MoE layers in the encoder and\ndecoder are non-hierarchical MoEs with n= 512 experts, and k= 2. Each expert has a larger\nhidden layer of size 8192 . This doubles the amount of computation in the MoE layers, raising the\ncomputational budget of the entire model from 85M to 102M ops/timestep.\nTraining: We trained our networks using the Adam optimizer (Kingma & Ba, 2015). The base\nlearning rate was increased linearly for the \ufb01rst 2000 training steps, held constant for an additional\n8000 steps, and decreased after that so as to be proportional to the inverse square root of the step\nnumber. For the single-language-pair models, similarly to (Wu et al., 2016), we applied dropout\n(Zaremba et al., 2014) to the output of all embedding, LSTM and MoE layers, using DropProb =\n0:4. Training was done synchronously on a cluster of up to 64 GPUs as described in section 3. Each\ntraining batch consisted of a set of sentence pairs containing roughly 16000 words per GPU.\nTo ensure balanced expert utilization we set wimportance = 0:01andwload= 0:01, as described in\nSection 4 and Appendix A.\nMetrics: We evaluated our models using the perplexity and the standard BLEU score metric. We\nreported tokenized BLEU score as computed by the multi-bleu.pl script, downloaded from the public\nimplementation of Moses (on Github), which was also used in (Luong et al., 2015a).\n4While the original size of the corpus was 130 billion words, the neural models were trained for a maximum\nof 100 billion words. The reported Kneser-Ney 5-gram models were trained over 13 billion and 130 billion\nwords respectively, giving them a slight advantage over the other reported results.\n5For performance reasons, we use a slightly different attention function from the one described in (Wu et al.,\nUnder review as a conference paper at ICLR 2017\nResults: Tables 2, 3 and 4 in Section 5.3 show comparisons of our results to other published\nmethods. Figure 4 shows test perplexity as a function of number of words in the (training data\u2019s)\nsource sentences processed for models with different numbers of experts. As can be seen from the\nFigure, as we increased the number of experts to approach 2048, the test perplexity of our model\ncontinued to improve.\n2.02.53.03.54.04.55.05.56.0Perplexity#Experts=0\n#Experts=32\n#Experts=512\n#Experts=20480 1 2 3 4 5 6 7 8 9\nNumber of source words processed1e92.02.53.03.54.04.55.05.56.0Perplexity#Experts=0\n#Experts=32\n#Experts=512\n#Experts=2048\n2345678Perplexity#Experts=0\n#Experts=32\n#Experts=512\n#Experts=20480.0 0.5 1.0 1.5 2.0\nNumber of source words processed1e102345678Perplexity#Experts=0\n#Experts=32\n#Experts=512\n#Experts=2048\nFigure 4: Perplexity on WMT\u201914 En !Fr (left) and Google Production En !Fr (right) datasets as\na function of number of words processed. The large differences between models at the beginning\nof training are due to different batch sizes. All models incur the same computational budget (85M\nops/timestep) except the one with no experts.\nWe found that the experts indeed become highly specialized by syntax and/or semantics, as can be\nseen in Table 9. For example, one expert is used when the inde\ufb01nite article \u201ca\" introduces the direct\nobject in a verb phrase indicating importance or leadership.\nTable 9: Contexts corresponding to a few of the 2048 experts in the MoE layer in the encoder portion\nof the WMT\u201914 En !Fr translation model. For each expert i, we sort the inputs in a training batch\nin decreasing order of G(x)i, and show the words surrounding the corresponding positions in the\ninput sentences.\nExpert 381 Expert 752 Expert 2004\n... with researchers , ... ... plays acore ... ... with rapidly growing ...\n... to innovation . ... plays acritical ... ... under static conditions ...\n... tics researchers . ... provides alegislative ... ... to swift ly ...\n... the generation of ... ... play aleading ... ... to dras tically ...\n... technology innovations is ... ... assume aleadership ... ... the rapid and ...\n... technological innovations , ... ... plays acentral ... ... the fastest ...\n... support innovation throughout ... ... taken aleading ... ... the Quick Method ...\n... role innovation will ... ... established areconciliation ... ... rec urrent ) ...\n... research scienti st ... ... played avital ... ... provides quick access ...\n... promoting innovation where ... ... have acentral ... ... of volatile organic ...\n... ... ...\nF S TRICTLY BALANCED GATING\nDue to some peculiarities in our infrastructure which have since been \ufb01xed, at the time we ran some\nof the machine translation experiments, our models ran faster if every expert received exactly the\nsame batch size. To accommodate this, we used a different gating function which we describe below.\nRecall that we de\ufb01ne the softmax gating function to be:\nG\u001b(x) =Softmax (x\u0001WgUnder review as a conference paper at ICLR 2017\nSparse Gating (alternate formulation): To obtain a sparse gating vector, we multiply G\u001b(x)\ncomponent-wise with a sparse mask M(G\u001b(x))and normalize the output. The mask itself is a\nfunction ofG\u001b(x)and speci\ufb01es which experts are assigned to each input example:\nG(x)i=G\u001b(x)iM(G\u001b(x))iPn\nj=1G\u001b(x)jM(G\u001b(x))j(16)\nTop-K Mask: To implement top-k gating in this formulation, we would let M(v) =TopK (v;k),\nwhere:\nTopK (v;k)i=\u001a1ifviis in the top kelements of v.\n0otherwise.(17)\nBatchwise Mask: To force each expert to receive the exact same number of examples, we intro-\nduce an alternative mask function, Mbatchwise (X;m ), which operates over batches of input vectors.\nInstead of keeping the top kvalues per example, we keep the top mvalues per expert across the\ntraining batch, where m=kjXj\nn, so that each example is sent to an average of kexperts.\nMbatchwise (X;m )j;i=\u001a1ifXj;iis in the top mvalues for to expert i\n0otherwise(18)\nAs our experiments suggest and also observed in (Ioffe & Szegedy, 2015), using a batchwise func-\ntion during training (such as Mbatchwise ) requires modi\ufb01cations to the inference when we may not\nhave a large batch of examples. Our solution to this is to train a vector Tof per-expert threshold\nvalues to approximate the effects of the batchwise mask. We use the following mask at inference\ntime:\nMthreshold (x;T)i=\u001a1ifxi>Ti\n0otherwise(19)\nTo learn the threshold values, we apply an additional loss at training time which is minimized when\nthe batchwise mask and the threshold mask are identical.\nLbatchwise (X;T;m ) =jXjX\nj=1nX\ni=1(Mthreshold (x;T)i\u0000Mbatchwise (X;m )j;i)(Xj;i\u0000Ti)(20)\nG A TTENTION FUNCTION\nThe attention mechanism described in GNMT (Wu et al., 2016) involves a learned \u201cAttention Func-\ntion\"A(xi;yj)which takes a \u201csource vector\" xiand a \u201ctarget vector\" yj, and must be computed for\nevery source time step iand target time step j. In GNMT, the attention function is implemented as\na feed forward neural network with a hidden layer of size n. It can be expressed as:\nAGNMT (xi;yj) =nX\nd=1Vdtanh((xiU)d+ (yjW)d) (21)\nWhereUandWare trainable weight matrices and Vis a trainable weight vector.\nFor performance reasons, in our models, we used a slightly different attention function:\nA(xi;yj) =nX\nd=1Vdtanh((xiU)d)tanh((yjW)dUnder review as a conference paper at ICLR 2017\nWith our attention function, we can simultaneously compute the attention function on multiple\nsource time steps and multiple target time steps using optimized matrix multiplications. We found",
        "subsection": []
    },
    {
        "missing": []
    },
    {
        "references": []
    },
    {
        "title": "Outrageously Large Neural Networks: The Sparsely-Gated\n  Mixture-of-Experts Layer",
        "arxiv_id": "1701.06538"
    }
]